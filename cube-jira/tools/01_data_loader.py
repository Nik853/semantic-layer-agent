"""
=================================================================
–ó–ê–ì–†–£–ó–ß–ò–ö –î–ê–ù–ù–´–• –í CUBE
=================================================================
–°–∫—Ä–∏–ø—Ç —á–∏—Ç–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è
–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —á–µ—Ä–µ–∑ GigaChat –∏ —Å–æ–∑–¥–∞—ë—Ç YAML-–º–æ–¥–µ–ª–∏ –¥–ª—è Cube.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–µ–∂–∏–º—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö (database.driver –≤ config.yml):
  - postgresql              ‚Äî –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ (psycopg2)
  - greenplum               ‚Äî Greenplum —á–µ—Ä–µ–∑ SQLAlchemy (Kerberos –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
  - hive                    ‚Äî Hive —á–µ—Ä–µ–∑ SQLAlchemy/PyHive (Kerberos)
  - duckdb                  ‚Äî –ª–æ–∫–∞–ª—å–Ω—ã–π DuckDB-—Ñ–∞–π–ª
  - cube                    ‚Äî —á—Ç–µ–Ω–∏–µ –∏–∑ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ Cube API (–±–µ–∑ –ë–î)

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:
  --jira-plan <file.xlsx>   ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑ JIRA execution plan —Ñ–∞–π–ª–∞

–ó–∞–ø—É—Å–∫: python 01_data_loader.py
        python 01_data_loader.py --source cube
        python 01_data_loader.py --source duckdb
        python 01_data_loader.py --jira-plan sample_execution.xlsx
=================================================================
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path
from datetime import datetime

# ============================================================
# –ê–≤—Ç–æ—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
# ============================================================

def _ensure_packages():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã (–±–∞–∑–æ–≤—ã–µ)"""
    required = {
        "yaml": "pyyaml",
        "langchain_gigachat": "langchain-gigachat",
    }
    missing = []
    for module, pip_name in required.items():
        try:
            __import__(module)
        except ImportError:
            missing.append(pip_name)
    if missing:
        print(f"üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–∞–∫–µ—Ç–æ–≤: {', '.join(missing)}")
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "--quiet"] + missing
        )
        print("‚úÖ –ü–∞–∫–µ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

_ensure_packages()

import yaml

# psycopg2 / duckdb –ø–æ–¥–≥—Ä—É–∂–∞—é—Ç—Å—è –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ create_data_source()
try:
    import psycopg2
except ImportError:
    psycopg2 = None  # –ù–µ –Ω—É–∂–µ–Ω –¥–ª—è duckdb/cube —Ä–µ–∂–∏–º–æ–≤

# ============================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
# ============================================================

def load_config(config_path="config.yml"):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML-—Ñ–∞–π–ª–∞"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


# ============================================================
# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat
# ============================================================

def create_gigachat(config):
    """–°–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç GigaChat. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 2 —Ä–µ–∂–∏–º–∞:
       1. credentials ‚Äî –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø (SberCloud)
       2. base_url + access_token ‚Äî —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ (–∑–∞–∫—Ä—ã—Ç—ã–π –∫–æ–Ω—Ç—É—Ä)
    """
    from langchain_gigachat import GigaChat
    
    gc = config["gigachat"]
    model = gc.get("model", "GigaChat")
    
    # –†–µ–∂–∏–º 2: —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ (base_url + access_token –∏–∑ env)
    if gc.get("base_url"):
        token_env = gc.get("access_token_env", "JPY_API_TOKEN")
        token = os.getenv(token_env, "")
        return GigaChat(
            base_url=gc["base_url"],
            access_token=token,
            model=model,
            timeout=gc.get("timeout", 60)
        )
    
    # –†–µ–∂–∏–º 1: —á–µ—Ä–µ–∑ credentials
    if gc.get("credentials"):
        return GigaChat(
            credentials=gc["credentials"],
            model=model,
            verify_ssl_certs=gc.get("verify_ssl", False),
            timeout=gc.get("timeout", 60)
        )
    
    print("‚ùå –û—à–∏–±–∫–∞: –ó–∞–ø–æ–ª–Ω–∏—Ç–µ gigachat.credentials –∏–ª–∏ gigachat.base_url –≤ config.yml")
    sys.exit(1)


# ============================================================
# –ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ë–î
# ============================================================

def get_db_connection(config):
    """–ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ PostgreSQL / GreenPlum"""
    if psycopg2 is None:
        print("‚ùå psycopg2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install psycopg2-binary")
        print("   –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --source duckdb / --source cube")
        sys.exit(1)
    db = config["database"]
    return psycopg2.connect(
        host=db["host"],
        port=db["port"],
        dbname=db["name"],
        user=db["user"],
        password=db["password"]
    )


def get_schema(config):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è —Å—Ö–µ–º—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
    return config.get("database", {}).get("schema", "public")


def get_tables(conn, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü"""
    cur = conn.cursor()
    cur.execute("""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = %s 
          AND table_type = 'BASE TABLE'
        ORDER BY table_name
    """, (schema,))
    tables = [row[0] for row in cur.fetchall()]
    cur.close()
    return tables


def get_columns(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ —Ç–∞–±–ª–∏—Ü—ã —Å —Ç–∏–ø–∞–º–∏"""
    cur = conn.cursor()
    cur.execute("""
        SELECT column_name, data_type, is_nullable, column_default,
               character_maximum_length
        FROM information_schema.columns
        WHERE table_schema = %s AND table_name = %s
        ORDER BY ordinal_position
    """, (schema, table_name))
    columns = []
    for row in cur.fetchall():
        columns.append({
            "name": row[0],
            "data_type": row[1],
            "nullable": row[2] == "YES",
            "default": row[3],
            "max_length": row[4]
        })
    cur.close()
    return columns


def get_foreign_keys(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏ —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    cur.execute("""
        SELECT
            kcu.column_name,
            ccu.table_name AS foreign_table,
            ccu.column_name AS foreign_column
        FROM information_schema.table_constraints AS tc
        JOIN information_schema.key_column_usage AS kcu
            ON tc.constraint_name = kcu.constraint_name
        JOIN information_schema.constraint_column_usage AS ccu
            ON ccu.constraint_name = tc.constraint_name
        WHERE tc.constraint_type = 'FOREIGN KEY'
          AND tc.table_name = %s
          AND tc.table_schema = %s
    """, (table_name, schema))
    fks = []
    for row in cur.fetchall():
        fks.append({
            "column": row[0],
            "foreign_table": row[1],
            "foreign_column": row[2]
        })
    cur.close()
    return fks


def get_primary_key(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å primary key —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    cur.execute("""
        SELECT kcu.column_name
        FROM information_schema.table_constraints tc
        JOIN information_schema.key_column_usage kcu
            ON tc.constraint_name = kcu.constraint_name
        WHERE tc.constraint_type = 'PRIMARY KEY'
          AND tc.table_name = %s
          AND tc.table_schema = %s
    """, (table_name, schema))
    result = cur.fetchone()
    cur.close()
    return result[0] if result else "id"


def get_sample_data(conn, table_name, schema="public", limit=5):
    """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    try:
        cur.execute(f'SELECT * FROM {schema}."{table_name}" LIMIT %s', (limit,))
        columns = [desc[0] for desc in cur.description]
        rows = cur.fetchall()
        cur.close()
        return columns, rows
    except Exception:
        cur.close()
        return [], []


def get_row_count(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫"""
    cur = conn.cursor()
    cur.execute(f'SELECT COUNT(*) FROM {schema}."{table_name}"')
    count = cur.fetchone()[0]
    cur.close()
    return count


# ============================================================
# –ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑ DuckDB
# ============================================================

class DuckDBSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ‚Äî –ª–æ–∫–∞–ª—å–Ω—ã–π DuckDB-—Ñ–∞–π–ª.
    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ç–æ—Ç –∂–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á—Ç–æ –∏ psycopg2-—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã—à–µ.
    """

    def __init__(self, config):
        import duckdb
        db_path = config["database"].get("path", "./data.duckdb")
        self.schema = config["database"].get("schema", "main")
        self.conn = duckdb.connect(db_path, read_only=True)
        print(f"‚úÖ DuckDB: {db_path} (schema={self.schema})")

    def get_tables(self):
        rows = self.conn.execute(
            "SELECT table_name FROM information_schema.tables "
            "WHERE table_schema = ? AND table_type = 'BASE TABLE' "
            "ORDER BY table_name",
            [self.schema]
        ).fetchall()
        return [r[0] for r in rows]

    def get_columns(self, table_name):
        rows = self.conn.execute(
            "SELECT column_name, data_type, is_nullable, column_default, "
            "character_maximum_length "
            "FROM information_schema.columns "
            "WHERE table_schema = ? AND table_name = ? "
            "ORDER BY ordinal_position",
            [self.schema, table_name]
        ).fetchall()
        return [{
            "name": r[0],
            "data_type": r[1],
            "nullable": r[2] == "YES",
            "default": r[3],
            "max_length": r[4]
        } for r in rows]

    def get_foreign_keys(self, table_name):
        # DuckDB –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç FK, –Ω–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω—è–µ—Ç information_schema
        try:
            rows = self.conn.execute(
                "SELECT kcu.column_name, ccu.table_name, ccu.column_name "
                "FROM information_schema.table_constraints tc "
                "JOIN information_schema.key_column_usage kcu "
                "  ON tc.constraint_name = kcu.constraint_name "
                "JOIN information_schema.constraint_column_usage ccu "
                "  ON ccu.constraint_name = tc.constraint_name "
                "WHERE tc.constraint_type = 'FOREIGN KEY' "
                "  AND tc.table_name = ? AND tc.table_schema = ?",
                [table_name, self.schema]
            ).fetchall()
            return [{"column": r[0], "foreign_table": r[1], "foreign_column": r[2]}
                    for r in rows]
        except Exception:
            return []

    def get_primary_key(self, table_name):
        try:
            rows = self.conn.execute(
                "SELECT kcu.column_name "
                "FROM information_schema.table_constraints tc "
                "JOIN information_schema.key_column_usage kcu "
                "  ON tc.constraint_name = kcu.constraint_name "
                "WHERE tc.constraint_type = 'PRIMARY KEY' "
                "  AND tc.table_name = ? AND tc.table_schema = ?",
                [table_name, self.schema]
            ).fetchall()
            return rows[0][0] if rows else "id"
        except Exception:
            return "id"

    def get_sample_data(self, table_name, limit=5):
        try:
            result = self.conn.execute(
                f'SELECT * FROM {self.schema}."{table_name}" LIMIT ?', [limit]
            )
            columns = [desc[0] for desc in result.description]
            rows = result.fetchall()
            return columns, rows
        except Exception:
            return [], []

    def get_row_count(self, table_name):
        result = self.conn.execute(
            f'SELECT COUNT(*) FROM {self.schema}."{table_name}"'
        )
        return result.fetchone()[0]

    def close(self):
        self.conn.close()


# ============================================================
# –ß—Ç–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ Cube API
# ============================================================

class CubeAPISource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ‚Äî –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ Cube REST API /meta.
    –ù–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –≤–æ–æ–±—â–µ.
    Cube —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç ‚Üí —á–∏—Ç–∞–µ–º –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
    """

    def __init__(self, config):
        import httpx
        self.cube_url = config["cube"]["api_url"]
        self.schema = config["database"].get("schema", "public")
        headers = {}
        token = config["cube"].get("api_token", "")
        if token:
            headers["Authorization"] = f"Bearer {token}"

        resp = httpx.get(f"{self.cube_url}/meta", headers=headers, timeout=15.0)
        resp.raise_for_status()
        self.meta = resp.json()
        self.cubes = {c["name"]: c for c in self.meta.get("cubes", [])}
        print(f"‚úÖ Cube API: {len(self.cubes)} –∫—É–±–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {self.cube_url}")

    def get_tables(self):
        return sorted(self.cubes.keys())

    def get_columns(self, table_name):
        cube = self.cubes.get(table_name, {})
        columns = []
        for dim in cube.get("dimensions", []):
            short_name = dim["name"].split(".")[-1]
            columns.append({
                "name": short_name,
                "data_type": dim.get("type", "string"),
                "nullable": True,
                "default": None,
                "max_length": None
            })
        return columns

    def get_foreign_keys(self, table_name):
        # Cube API –Ω–µ –æ—Ç–¥–∞—ë—Ç FK ‚Äî joins –æ–±–Ω–∞—Ä—É–∂–∏–º –ø–æ –∏–º–µ–Ω–∞–º –∫–æ–ª–æ–Ω–æ–∫
        return []

    def get_primary_key(self, table_name):
        cube = self.cubes.get(table_name, {})
        for dim in cube.get("dimensions", []):
            if dim.get("primaryKey"):
                return dim["name"].split(".")[-1]
        return "id"

    def get_sample_data(self, table_name, limit=5):
        # –ù–µ –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å sample data —á–µ—Ä–µ–∑ Cube API
        return [], []

    def get_row_count(self, table_name):
        # –ü—Ä–æ–±—É–µ–º count –∏–∑ Cube
        import httpx
        cube = self.cubes.get(table_name, {})
        count_measure = None
        for m in cube.get("measures", []):
            if m.get("type") == "count":
                count_measure = m["name"]
                break
        if not count_measure:
            return 0
        try:
            headers = {"Content-Type": "application/json"}
            resp = httpx.post(
                f"{self.cube_url}/load",
                json={"query": {"measures": [count_measure], "limit": 1}},
                headers=headers,
                timeout=15.0
            )
            data = resp.json().get("data", [])
            if data:
                return list(data[0].values())[0]
        except Exception:
            pass
        return 0

    def close(self):
        pass


# ============================================================
# –§–∞–±—Ä–∏–∫–∞: –≤—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ============================================================

def create_data_source(config, override_source=None):
    """–°–æ–∑–¥–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ config.yml –∏–ª–∏ --source –∞—Ä–≥—É–º–µ–Ω—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –º–µ—Ç–æ–¥–∞–º–∏: get_tables, get_columns, get_foreign_keys,
    get_primary_key, get_sample_data, get_row_count, close.
    """
    driver = override_source or config.get("database", {}).get("driver", "postgresql")
    driver = driver.lower().strip()

    if driver == "duckdb":
        return DuckDBSource(config), driver
    elif driver == "cube":
        return CubeAPISource(config), driver
    elif driver in ("postgresql", "postgres"):
        conn = get_db_connection(config)
        schema = get_schema(config)
        return _PsycopgSource(conn, schema), driver
    elif driver == "greenplum":
        from db_sources import GreenplumSource
        return GreenplumSource(config), driver
    elif driver == "hive":
        from db_sources import HiveSource
        return HiveSource(config), driver
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π driver: {driver}")
        print("   –î–æ–ø—É—Å—Ç–∏–º—ã–µ: postgresql, greenplum, hive, duckdb, cube")
        sys.exit(1)


class _PsycopgSource:
    """–û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ psycopg2 –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."""
    def __init__(self, conn, schema):
        self.conn = conn
        self.schema = schema

    def get_tables(self):
        return get_tables(self.conn, self.schema)

    def get_columns(self, table_name):
        return get_columns(self.conn, table_name, self.schema)

    def get_foreign_keys(self, table_name):
        return get_foreign_keys(self.conn, table_name, self.schema)

    def get_primary_key(self, table_name):
        return get_primary_key(self.conn, table_name, self.schema)

    def get_sample_data(self, table_name, limit=5):
        return get_sample_data(self.conn, table_name, self.schema, limit)

    def get_row_count(self, table_name):
        return get_row_count(self.conn, table_name, self.schema)

    def close(self):
        self.conn.close()


# ============================================================
# JIRA Knowledge Base ‚Äî —É–ª—É—á—à–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è JIRA-—Ç–∞–±–ª–∏—Ü
# ============================================================

JIRA_TABLE_HINTS = {
    "jiraissue": {
        "title": "–ó–∞–¥–∞—á–∏ JIRA",
        "description": "–û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∑–∞–¥–∞—á JIRA. –°–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ issues —Å –∫–ª—é—á–∞–º–∏, –æ–ø–∏—Å–∞–Ω–∏—è–º–∏, —Å—Ç–∞—Ç—É—Å–∞–º–∏ –∏ –¥–∞—Ç–∞–º–∏.",
        "suggested_measures": [
            {"name": "open_count", "sql": "CASE WHEN {CUBE}.resolution IS NULL THEN 1 END",
             "type": "count", "title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –∑–∞–¥–∞—á–∏", "description": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—Ä–µ—à—ë–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"},
            {"name": "resolved_count", "sql": "CASE WHEN {CUBE}.resolution IS NOT NULL THEN 1 END",
             "type": "count", "title": "–ó–∞–∫—Ä—ã—Ç—ã–µ –∑–∞–¥–∞—á–∏", "description": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—à—ë–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"},
        ],
        "column_hints": {
            "pkey": "–ö–ª—é—á –∑–∞–¥–∞—á–∏ (PROJECT-123)",
            "summary": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–¥–∞—á–∏",
            "description": "–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ",
            "created": "–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á–∏",
            "updated": "–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è",
            "resolutiondate": "–î–∞—Ç–∞ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏",
            "duedate": "–ü–ª–∞–Ω–æ–≤—ã–π —Å—Ä–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è",
            "reporter": "–ê–≤—Ç–æ—Ä –∑–∞–¥–∞—á–∏ (ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)",
            "assignee": "–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –∑–∞–¥–∞—á–∏ (ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)",
            "priority": "–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (ID)",
            "issuestatus": "–°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ (ID)",
            "issuetype": "–¢–∏–ø –∑–∞–¥–∞—á–∏ (ID)",
            "project": "–ü—Ä–æ–µ–∫—Ç (ID)",
            "resolution": "–†–µ–∑–æ–ª—é—Ü–∏—è (ID). NULL = –∑–∞–¥–∞—á–∞ –æ—Ç–∫—Ä—ã—Ç–∞",
            "story_points": "–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ Story Points",
            "timeoriginalestimate": "–ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ (—Å–µ–∫—É–Ω–¥—ã)",
            "timeestimate": "–¢–µ–∫—É—â–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ (—Å–µ–∫—É–Ω–¥—ã)",
            "timespent": "–ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)",
        },
    },
    "issuestatus": {
        "title": "–°—Ç–∞—Ç—É—Å—ã –∑–∞–¥–∞—á",
        "description": "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å—Ç–∞—Ç—É—Å–æ–≤ JIRA (Open, In Progress, Done –∏ —Ç.–¥.)",
        "column_hints": {
            "pname": "–ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞",
            "statuscategory": "–ö–∞—Ç–µ–≥–æ—Ä–∏—è (todo/in_progress/done)",
        },
    },
    "issuelink": {
        "title": "–°–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏",
        "description": "–°–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ JIRA (blocks, is blocked by, duplicates, relates to –∏ –¥—Ä.)",
        "suggested_measures": [
            {"name": "link_count", "type": "count", "title": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π",
             "description": "–û–±—â–µ–µ —á–∏—Å–ª–æ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏"},
        ],
        "column_hints": {
            "source": "ID –∑–∞–¥–∞—á–∏-–∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å–≤—è–∑–∏",
            "destination": "ID –∑–∞–¥–∞—á–∏-–Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å–≤—è–∑–∏",
            "linktype": "–¢–∏–ø —Å–≤—è–∑–∏ (ID –∏–∑ issuelinktype)",
        },
    },
    "nodeassociation": {
        "title": "–ü—Ä–∏–≤—è–∑–∫–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π",
        "description": "–°–≤—è–∑–∏ –º–µ–∂–¥—É —É–∑–ª–∞–º–∏ JIRA: –∑–∞–¥–∞—á–∞‚Üî–∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –∑–∞–¥–∞—á–∞‚Üî–≤–µ—Ä—Å–∏—è, –ø—Ä–æ–µ–∫—Ç‚Üî–∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏ –¥—Ä.",
        "column_hints": {
            "source_node_id": "ID –∏—Å—Ö–æ–¥–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (–æ–±—ã—á–Ω–æ –∑–∞–¥–∞—á–∏)",
            "source_node_entity": "–¢–∏–ø –∏—Å—Ö–æ–¥–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (Issue)",
            "sink_node_id": "ID —Ü–µ–ª–µ–≤–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (–∫–æ–º–ø–æ–Ω–µ–Ω—Ç, –≤–µ—Ä—Å–∏—è)",
            "sink_node_entity": "–¢–∏–ø —Ü–µ–ª–µ–≤–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (Component, Version)",
            "association_type": "–¢–∏–ø –ø—Ä–∏–≤—è–∑–∫–∏",
        },
    },
    "project": {
        "title": "–ü—Ä–æ–µ–∫—Ç—ã JIRA",
        "description": "–°–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å –∫–ª—é—á–∞–º–∏, –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è–º–∏.",
        "column_hints": {
            "pkey": "–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –ø—Ä–æ–µ–∫—Ç–∞ (AUTH, PAY, CRM)",
            "pname": "–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞",
            "lead": "–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞ (ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞",
            "projecttype": "–¢–∏–ø –ø—Ä–æ–µ–∫—Ç–∞ (software, business)",
        },
    },
    "issuetype": {
        "title": "–¢–∏–ø—ã –∑–∞–¥–∞—á",
        "description": "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á (Bug, Story, Task, Epic –∏ —Ç.–¥.)",
        "column_hints": {
            "pname": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ —Ç–∏–ø–∞",
        },
    },
    "priority": {
        "title": "–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∑–∞–¥–∞—á",
        "description": "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ (Blocker, Critical, High, Medium, Low, Trivial).",
        "column_hints": {
            "pname": "–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞",
            "sequence": "–ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ)",
        },
    },
    "resolution": {
        "title": "–†–µ–∑–æ–ª—é—Ü–∏–∏ –∑–∞–¥–∞—á",
        "description": "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ä–µ–∑–æ–ª—é—Ü–∏–π (Fixed, Won't Fix, Duplicate, Cannot Reproduce –∏ –¥—Ä.)",
        "column_hints": {
            "pname": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–∑–æ–ª—é—Ü–∏–∏",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–∑–æ–ª—é—Ü–∏–∏",
        },
    },
    "cwd_user": {
        "title": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ JIRA",
        "description": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–∏—Å—Ç–µ–º—ã JIRA —Å –ª–æ–≥–∏–Ω–∞–º–∏ –∏ –∏–º–µ–Ω–∞–º–∏.",
        "column_hints": {
            "user_name": "–õ–æ–≥–∏–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
            "display_name": "–û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è (–§–ò–û)",
            "email_address": "E-mail –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
            "active": "–ê–∫—Ç–∏–≤–µ–Ω –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
        },
    },
    "component": {
        "title": "–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–æ–≤",
        "description": "–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–º–æ–¥—É–ª–∏) –ø—Ä–æ–µ–∫—Ç–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á.",
        "column_hints": {
            "cname": "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞",
            "project": "ID –ø—Ä–æ–µ–∫—Ç–∞",
            "lead": "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –∑–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)",
        },
    },
    "projectversion": {
        "title": "–í–µ—Ä—Å–∏–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤",
        "description": "–í–µ—Ä—Å–∏–∏ (—Ä–µ–ª–∏–∑—ã) –ø—Ä–æ–µ–∫—Ç–æ–≤ ‚Äî –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è.",
        "column_hints": {
            "vname": "–ù–∞–∑–≤–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ (1.0, 2.0-RC1)",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–∏",
            "startdate": "–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –≤–µ—Ä—Å–∏–∏",
            "releasedate": "–î–∞—Ç–∞ —Ä–µ–ª–∏–∑–∞",
            "released": "–§–ª–∞–≥: –≤–µ—Ä—Å–∏—è –≤—ã–ø—É—â–µ–Ω–∞",
            "archived": "–§–ª–∞–≥: –≤–µ—Ä—Å–∏—è –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∞",
            "project": "ID –ø—Ä–æ–µ–∫—Ç–∞",
        },
    },
    "worklog": {
        "title": "–£—á—ë—Ç —Ä–∞–±–æ—á–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏",
        "description": "–ó–∞–ø–∏—Å–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—á–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ –∑–∞–¥–∞—á–∞–º.",
        "suggested_measures": [
            {"name": "total_time_spent", "sql": "timeworked", "type": "sum",
             "title": "–°—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)", "description": "–û–±—â–µ–µ –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è"},
        ],
        "column_hints": {
            "issueid": "ID –∑–∞–¥–∞—á–∏",
            "author": "–ê–≤—Ç–æ—Ä –∑–∞–ø–∏—Å–∏ (ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)",
            "timeworked": "–ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)",
            "startdate": "–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã",
            "created": "–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø–∏—Å–∏",
        },
    },
    "changegroup": {
        "title": "–ì—Ä—É–ø–ø—ã –∏–∑–º–µ–Ω–µ–Ω–∏–π",
        "description": "–ì—Ä—É–ø–ø—ã –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø–æ–ª–µ–π –∑–∞–¥–∞—á (–∏—Å—Ç–æ—Ä–∏—è). –ö–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ = –æ–¥–∏–Ω –∞–∫—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.",
        "column_hints": {
            "issueid": "ID –∑–∞–¥–∞—á–∏",
            "author": "–ê–≤—Ç–æ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏—è (ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)",
            "created": "–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è",
        },
    },
    "changeitem": {
        "title": "–≠–ª–µ–º–µ–Ω—Ç—ã –∏–∑–º–µ–Ω–µ–Ω–∏–π",
        "description": "–î–µ—Ç–∞–ª–∏ –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è: –∫–∞–∫–æ–µ –ø–æ–ª–µ, —Å—Ç–∞—Ä–æ–µ –∏ –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.",
        "column_hints": {
            "groupid": "ID –≥—Ä—É–ø–ø—ã –∏–∑–º–µ–Ω–µ–Ω–∏–π (changegroup)",
            "field": "–ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω—ë–Ω–Ω–æ–≥–æ –ø–æ–ª—è (status, assignee, priority...)",
            "oldvalue": "–°—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (ID)",
            "oldstring": "–°—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (—Ç–µ–∫—Å—Ç)",
            "newvalue": "–ù–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (ID)",
            "newstring": "–ù–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (—Ç–µ–∫—Å—Ç)",
            "fieldtype": "–¢–∏–ø –ø–æ–ª—è (jira, custom)",
        },
    },
    "customfield": {
        "title": "–ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø–æ–ª—è",
        "description": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø–æ–ª–µ–π JIRA.",
        "column_hints": {
            "cfname": "–ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è",
            "customfieldtypekey": "–¢–∏–ø –ø–æ–ª—è (—Å—Ç—Ä–æ–∫–∞, —á–∏—Å–ª–æ, –¥–∞—Ç–∞...)",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—è",
        },
    },
    "customfieldvalue": {
        "title": "–ó–Ω–∞—á–µ–Ω–∏—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø–æ–ª–µ–π",
        "description": "–ó–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø–æ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á.",
        "column_hints": {
            "issue": "ID –∑–∞–¥–∞—á–∏",
            "customfield": "ID –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –ø–æ–ª—è",
            "stringvalue": "–°—Ç—Ä–æ–∫–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
            "numbervalue": "–ß–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
            "textvalue": "–¢–µ–∫—Å—Ç–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç)",
            "datevalue": "–ó–Ω–∞—á–µ–Ω–∏–µ –¥–∞—Ç—ã",
        },
    },
    "issuelinktype": {
        "title": "–¢–∏–ø—ã —Å–≤—è–∑–µ–π –∑–∞–¥–∞—á",
        "description": "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ç–∏–ø–æ–≤ —Å–≤—è–∑–µ–π (Blocks, Is blocked by, Duplicates –∏ —Ç.–¥.)",
        "column_hints": {
            "linkname": "–ù–∞–∑–≤–∞–Ω–∏–µ —Å–≤—è–∑–∏ (–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–ø–µ—Ä—ë–¥)",
            "inward": "–û–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–∏ (–æ–±—Ä–∞—Ç–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)",
            "outward": "–û–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–∏ (–ø—Ä—è–º–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)",
        },
    },
    "label": {
        "title": "–ú–µ—Ç–∫–∏ –∑–∞–¥–∞—á",
        "description": "–ú–µ—Ç–∫–∏ (label) –∑–∞–¥–∞—á –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏.",
        "column_hints": {
            "issue": "ID –∑–∞–¥–∞—á–∏",
            "label": "–¢–µ–∫—Å—Ç –º–µ—Ç–∫–∏",
        },
    },
    "sprint": {
        "title": "–°–ø—Ä–∏–Ω—Ç—ã",
        "description": "–°–ø—Ä–∏–Ω—Ç—ã Agile-–¥–æ—Å–æ–∫ ‚Äî –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.",
        "column_hints": {
            "name": "–ù–∞–∑–≤–∞–Ω–∏–µ —Å–ø—Ä–∏–Ω—Ç–∞",
            "start_date": "–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ —Å–ø—Ä–∏–Ω—Ç–∞",
            "end_date": "–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å–ø—Ä–∏–Ω—Ç–∞",
            "complete_date": "–î–∞—Ç–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è",
            "state": "–°–æ—Å—Ç–æ—è–Ω–∏–µ (active, closed, future)",
        },
    },
    "issue_history": {
        "title": "–ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–∞–¥–∞—á",
        "description": "–ñ—É—Ä–Ω–∞–ª –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø–æ–ª–µ–π –∑–∞–¥–∞—á: —Å–º–µ–Ω—ã —Å—Ç–∞—Ç—É—Å–∞, –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –∏ –¥—Ä.",
        "suggested_measures": [
            {"name": "status_change_count",
             "sql": "CASE WHEN {CUBE}.field = 'status' THEN 1 END",
             "type": "count", "title": "–°–º–µ–Ω—ã —Å—Ç–∞—Ç—É—Å–∞",
             "description": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–º–µ–Ω —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á"},
        ],
        "column_hints": {
            "issue_id": "ID –∑–∞–¥–∞—á–∏",
            "author_id": "–ê–≤—Ç–æ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏—è (ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)",
            "field": "–ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω—ë–Ω–Ω–æ–≥–æ –ø–æ–ª—è (status, assignee, priority...)",
            "old_value": "–°—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
            "new_value": "–ù–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
            "created_at": "–î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è",
        },
    },
    "issue_comment": {
        "title": "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –∑–∞–¥–∞—á–∞–º",
        "description": "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫ –∑–∞–¥–∞—á–∞–º JIRA.",
        "column_hints": {
            "issue_id": "ID –∑–∞–¥–∞—á–∏",
            "author_id": "–ê–≤—Ç–æ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è (ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)",
            "body": "–¢–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è",
            "created_at": "–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è",
            "updated_at": "–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è",
        },
    },
    "user": {
        "title": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ JIRA",
        "description": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–∏—Å—Ç–µ–º—ã JIRA —Å –ª–æ–≥–∏–Ω–∞–º–∏, –∏–º–µ–Ω–∞–º–∏ –∏ —Ä–æ–ª—è–º–∏.",
        "column_hints": {
            "username": "–õ–æ–≥–∏–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
            "display_name": "–û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è (–§–ò–û)",
            "email": "E-mail –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
            "is_active": "–ê–∫—Ç–∏–≤–µ–Ω –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
        },
    },
}


def load_jira_plan(plan_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å JIRA execution plan —Ñ–∞–π–ª (xlsx/csv).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: source_table_name ‚Üí {columns from plan}.
    """
    info = {}
    plan_path_lower = plan_path.lower()

    try:
        if plan_path_lower.endswith(".xlsx") or plan_path_lower.endswith(".xls"):
            import openpyxl
            wb = openpyxl.load_workbook(plan_path, data_only=True)
            ws = wb.active
            headers = [str(c.value or "").strip().lower() for c in ws[1]]
            for row in ws.iter_rows(min_row=2, values_only=True):
                rec = dict(zip(headers, row))
                src_table = str(rec.get("source_table", "") or "").strip()
                if src_table:
                    info[src_table] = {
                        "source_schema": str(rec.get("source_schema", "") or ""),
                        "source_cluster": str(rec.get("source_cluster", "") or ""),
                        "target_table": str(rec.get("table_step2", "") or ""),
                        "process_description": str(rec.get("process_description", "") or "")[:500],
                        "last_updated": str(rec.get("last_updated_time", "") or ""),
                    }
        elif plan_path_lower.endswith(".csv"):
            import csv
            with open(plan_path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for rec in reader:
                    src_table = rec.get("source_table", "").strip()
                    if src_table:
                        info[src_table] = {
                            "source_schema": rec.get("source_schema", ""),
                            "source_cluster": rec.get("source_cluster", ""),
                            "target_table": rec.get("table_step2", ""),
                            "process_description": rec.get("process_description", "")[:500],
                            "last_updated": rec.get("last_updated_time", ""),
                        }
        else:
            print(f"‚ö†Ô∏è  –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {plan_path}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ .xlsx –∏–ª–∏ .csv")
            return {}

        print(f"‚úÖ JIRA plan –∑–∞–≥—Ä—É–∂–µ–Ω: {len(info)} source-—Ç–∞–±–ª–∏—Ü –∏–∑ {plan_path}")
        for t in info:
            print(f"   - {t}")
        return info

    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ JIRA plan: {e}")
        return {}


def _singularize(word: str) -> set:
    """–í–µ—Ä–Ω—É—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–æ—Ä–º –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞."""
    forms = {word}
    if word.endswith("ies") and len(word) > 4:
        forms.add(word[:-3] + "y")          # priorities ‚Üí priority
    if word.endswith("ses") or word.endswith("xes"):
        forms.add(word[:-2])                # statuses ‚Üí status
    if word.endswith("es") and len(word) > 3:
        forms.add(word[:-2])                # statuses ‚Üí status
    if word.endswith("s") and not word.endswith("ss"):
        forms.add(word[:-1])                # issues ‚Üí issue
    return forms


def match_jira_hints(table_name: str, jira_plan: dict = None) -> dict:
    """–ù–∞–π—Ç–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∏–∑ JIRA Knowledge Base –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã.
    –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è (issue_links ‚Üî issuelink),
    –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (priorities ‚Üî priority), –ø—Ä–µ—Ñ–∏–∫—Å—ã (jiraissue ‚Üî issues).
    """
    tl = table_name.lower()
    tl_no_sep = tl.replace("_", "").replace("-", "")
    tl_singulars = _singularize(tl_no_sep)

    for pattern, hints in JIRA_TABLE_HINTS.items():
        pat_no_sep = pattern.replace("_", "")
        pat_singulars = _singularize(pat_no_sep)

        # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π + —á–∏—Å–ª–∞)
        if pat_no_sep == tl_no_sep:
            return hints
        if tl_singulars & pat_singulars:
            return hints

        # 2. –ü—Ä–µ—Ñ–∏–∫—Å "jira" –≤ KB: jiraissue ‚Üí issue ‚Üî issues
        if pat_no_sep.startswith("jira"):
            core = pat_no_sep[4:]
            core_singulars = _singularize(core)
            if tl_singulars & core_singulars:
                return hints

        # 3. –ü—Ä–µ—Ñ–∏–∫—Å "project" –≤ KB: projectversion ‚Üí version ‚Üî versions
        if pat_no_sep.startswith("project"):
            core = pat_no_sep[7:]
            core_singulars = _singularize(core)
            if tl_singulars & core_singulars:
                return hints

        # 4. –°—É—Ñ—Ñ–∏–∫—Å–Ω—ã–π –º–∞—Ç—á: dm_jira_components ‚Üí component
        for pf in pat_singulars:
            if len(pf) >= 5 and any(tf.endswith(pf) for tf in tl_singulars):
                return hints

        # 5. –ü–æ–¥—Å—Ç—Ä–æ–∫–∞ >= 6 —Å–∏–º–≤–æ–ª–æ–≤ (–∏–∑–±–µ–≥–∞–µ—Ç –ª–æ–∂–Ω—ã—Ö –º–∞—Ç—á–µ–π)
        for pf in pat_singulars:
            if len(pf) >= 6 and pf in tl_no_sep:
                return hints

    if jira_plan:
        for src_table, plan_info in jira_plan.items():
            src_no_sep = src_table.lower().replace("_", "")
            src_singulars = _singularize(src_no_sep)
            if tl_singulars & src_singulars:
                return {
                    "title": f"–¢–∞–±–ª–∏—Ü–∞ –∏–∑ JIRA ({src_table})",
                    "description": f"–ò—Å—Ç–æ—á–Ω–∏–∫: {plan_info.get('source_schema', '')}.{src_table}. "
                                   f"–¶–µ–ª–µ–≤–∞—è: {plan_info.get('target_table', '')}.",
                }

    return {}


def enrich_descriptions_with_jira(descriptions: dict, table_name: str,
                                   columns: list, jira_plan: dict = None) -> dict:
    """–î–æ–ø–æ–ª–Ω–∏—Ç—å GigaChat-–æ–ø–∏—Å–∞–Ω–∏—è –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –∏–∑ JIRA Knowledge Base."""
    hints = match_jira_hints(table_name, jira_plan)
    if not hints:
        return descriptions

    col_descs = descriptions.get("columns", {})
    col_hints = hints.get("column_hints", {})

    for col_name, hint_text in col_hints.items():
        if col_name in col_descs:
            existing = col_descs[col_name]
            if not existing.get("description") or len(existing["description"]) < 10:
                existing["description"] = hint_text
        else:
            for c in columns:
                if c["name"].lower() == col_name or col_name in c["name"].lower():
                    col_descs[c["name"]] = col_descs.get(c["name"], {})
                    if not col_descs[c["name"]].get("description"):
                        col_descs[c["name"]]["description"] = hint_text
                    break

    descriptions["columns"] = col_descs
    return descriptions


def get_jira_suggested_measures(table_name: str) -> list:
    """–ü–æ–ª—É—á–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ä—ã –∏–∑ JIRA Knowledge Base."""
    hints = match_jira_hints(table_name)
    return hints.get("suggested_measures", [])


# ============================================================
# –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏
# ============================================================

def detect_implicit_relationships(table_name, columns, all_tables, explicit_fks):
    """
    –ù–∞–π—Ç–∏ –Ω–µ—è–≤–Ω—ã–µ —Å–≤—è–∑–∏ –ø–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏—é –æ–± –∏–º–µ–Ω–∞—Ö (*_id ‚Üí —Ç–∞–±–ª–∏—Ü–∞).
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
      - –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: project_id ‚Üí projects
      - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ: priority_id ‚Üí priorities, status_id ‚Üí statuses
      - –ü—Ä–µ—Ñ–∏–∫—Å—ã –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü: status_id ‚Üí issue_statuses, type_id ‚Üí issue_types
      - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–∞–ø–ø–∏–Ω–≥–∏: assignee_id ‚Üí users, reporter_id ‚Üí users, author_id ‚Üí users
    """
    explicit_cols = {fk["column"] for fk in explicit_fks}
    implicit = []

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–∞–ø–ø–∏–Ω–≥–∏: –∫–æ–ª–æ–Ω–∫–∞ ‚Üí —Ü–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
    SEMANTIC_MAP = {
        "assignee": "users",
        "reporter": "users",
        "author": "users",
        "creator": "users",
        "owner": "users",
        "updated_by": "users",
        "created_by": "users",
        "lead": "users",
        "manager": "users",
        "parent": None,  # self-join –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
    }

    for col in columns:
        col_name = col["name"]
        if not col_name.endswith("_id") or col_name in explicit_cols:
            continue
        if col_name == "id":
            continue

        base = col_name[:-3]  # "project_id" ‚Üí "project"

        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥
        if base in SEMANTIC_MAP:
            target = SEMANTIC_MAP[base]
            if target and target in all_tables:
                implicit.append({
                    "column": col_name,
                    "foreign_table": target,
                    "foreign_column": "id",
                    "source": "implicit"
                })
                continue
            elif target is None and table_name in all_tables:
                # self-join (parent_id ‚Üí —Ç–∞ –∂–µ —Ç–∞–±–ª–∏—Ü–∞)
                implicit.append({
                    "column": col_name,
                    "foreign_table": table_name,
                    "foreign_column": "id",
                    "source": "implicit"
                })
                continue

        # 2. –ü—Ä—è–º—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –ø–æ –∏–º–µ–Ω–∏
        candidates = [
            base + "s",       # project ‚Üí projects
            base + "es",      # status ‚Üí statuses
            base,             # sprint ‚Üí sprint
        ]
        if base.endswith("y"):
            candidates.append(base[:-1] + "ies")  # priority ‚Üí priorities, category ‚Üí categories
        if base.endswith("s"):
            candidates.append(base)

        matched_table = None
        for candidate in candidates:
            if candidate in all_tables and candidate != table_name:
                matched_table = candidate
                break

        # 3. –ï—Å–ª–∏ –ø—Ä—è–º–æ–µ –Ω–µ –Ω–∞—à–ª–æ ‚Äî –ø—Ä–æ–±—É–µ–º —Å –¥–æ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏
        if not matched_table:
            # –ü–æ–ª—É—á–∞–µ–º ¬´–¥–æ–º–µ–Ω¬ª –∏–∑ –∏–º–µ–Ω–∏ —Ç–µ–∫—É—â–µ–π —Ç–∞–±–ª–∏—Ü—ã (issue_comments ‚Üí issue)
            # –∏ –ø—Ä–æ–±—É–µ–º prefix_base (issue_status, issue_priority, etc.)
            prefixes_to_try = set()
            # –ò–∑ –∏–º–µ–Ω–∏ —Ç–∞–±–ª–∏—Ü—ã: issues ‚Üí issue, issue_comments ‚Üí issue
            parts = table_name.split("_")
            if parts:
                singular = parts[0].rstrip("s")
                prefixes_to_try.add(singular)       # "issue"
                prefixes_to_try.add(parts[0])       # "issues"

            # –¢–∞–∫–∂–µ –æ–±—â–∏–µ –¥–æ–º–µ–Ω–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
            prefixes_to_try.update(["issue", "project", "workflow", "notification", "permission"])

            for prefix in prefixes_to_try:
                prefix_candidates = [
                    f"{prefix}_{base}s",       # issue_statuses
                    f"{prefix}_{base}es",      # issue_statuses
                    f"{prefix}_{base}",        # issue_type
                ]
                if base.endswith("y"):
                    prefix_candidates.append(f"{prefix}_{base[:-1]}ies")  # issue_priorities
                
                for candidate in prefix_candidates:
                    if candidate in all_tables and candidate != table_name:
                        matched_table = candidate
                        break
                if matched_table:
                    break

        # 4. –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äî –ø–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö base –≤ –∏–º–µ–Ω–∏
        if not matched_table:
            for t in all_tables:
                if t != table_name and base in t and t.endswith("s"):
                    matched_table = t
                    break

        if matched_table:
            implicit.append({
                "column": col_name,
                "foreign_table": matched_table,
                "foreign_column": "id",
                "source": "implicit"
            })

    return implicit


def build_all_relationships(table_name, columns, all_tables, explicit_fks):
    """
    –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —è–≤–Ω—ã–µ FK –∏ –Ω–µ—è–≤–Ω—ã–µ —Å–≤—è–∑–∏.
    –ü—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–∫–∞—Ö –Ω–∞ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–ª–∏–∞—Å—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ join-–∑–∞–ø–∏—Å–µ–π:
      [{"column": "assignee_id", "foreign_table": "users", "alias": "users_assignee",
        "foreign_column": "id", "relationship": "many_to_one"}]
    """
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏
    all_rels = []
    for fk in explicit_fks:
        all_rels.append({**fk, "source": "explicit"})

    implicit = detect_implicit_relationships(table_name, columns, all_tables, explicit_fks)
    all_rels.extend(implicit)

    if not all_rels:
        return []

    # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –∫–∞–∂–¥–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç –∫–∞–∫ —Ü–µ–ª—å
    target_count = {}
    for rel in all_rels:
        t = rel["foreign_table"]
        target_count[t] = target_count.get(t, 0) + 1

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å –∞–ª–∏–∞—Å–∞–º–∏
    joins = []
    for rel in all_rels:
        target = rel["foreign_table"]
        col = rel["column"]
        base = col[:-3] if col.endswith("_id") else col  # assignee_id ‚Üí assignee

        # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è >1 —Ä–∞–∑ ‚Äî –∞–ª–∏–∞—Å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω
        if target_count[target] > 1:
            alias = f"{target}_{base}"  # users_assignee, users_reporter
        else:
            alias = target

        joins.append({
            "column": col,
            "foreign_table": target,
            "alias": alias,
            "foreign_column": rel.get("foreign_column", "id"),
            "relationship": "many_to_one",
            "source": rel.get("source", "explicit")
        })

    return joins


def _parse_json_safe(text):
    """–†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
    import json as _json
    import re

    text = text.strip()
    # –£–±–∏—Ä–∞–µ–º markdown
    if text.startswith("```"):
        lines = text.split("\n")
        text = "\n".join(lines[1:])
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()
    # –¢–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏–µ –∫–∞–≤—ã—á–∫–∏
    for old, new in [('\u201c', '"'), ('\u201d', '"'), ('\u00ab', '"'), ('\u00bb', '"'),
                     ('\u2018', "'"), ('\u2019', "'")]:
        text = text.replace(old, new)
    # –ò—â–µ–º JSON-–±–ª–æ–∫
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        cleaned = match.group()
        cleaned = re.sub(r',\s*}', '}', cleaned)
        cleaned = re.sub(r',\s*]', ']', cleaned)
        return _json.loads(cleaned)
    return _json.loads(text)


def suggest_joins_via_llm(llm, table_name, columns, detected_joins, all_tables):
    """
    –ü–æ–ø—Ä–æ—Å–∏—Ç—å GigaChat –¥–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –¥–∂–æ–π–Ω–æ–≤
    –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤—è–∑–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
    """
    if not detected_joins and not columns:
        return {}

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π
    join_lines = []
    for j in detected_joins:
        join_lines.append(f"{j['column']} ‚Üí {j['foreign_table']} (–∞–ª–∏–∞—Å: {j['alias']})")
    detected_text = "–°–≤—è–∑–∏:\n" + "\n".join(f"  - {l}" for l in join_lines)

    # –ö–æ–ª–æ–Ω–∫–∏ —Å _id –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–ª–∏ –≤ detected
    detected_cols = {j["column"] for j in detected_joins}
    unmatched_id_cols = [
        c["name"] for c in columns
        if c["name"].endswith("_id") and c["name"] != "id" and c["name"] not in detected_cols
    ]

    unmatched_text = ""
    if unmatched_id_cols:
        unmatched_text = f"\n–ù–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(unmatched_id_cols)}"

    prompt = f"""–î–ª—è –∫–∞–∂–¥–æ–π —Å–≤—è–∑–∏ —Ç–∞–±–ª–∏—Ü—ã {table_name} –¥–∞–π title (1-2 —Å–ª–æ–≤–∞) –∏ description (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ) –Ω–∞ —Ä—É—Å—Å–∫–æ–º.

{detected_text}{unmatched_text}

–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{"joins": [{{"column": "col_id", "title": "–ù–∞–∑–≤–∞–Ω–∏–µ", "description": "–û–ø–∏—Å–∞–Ω–∏–µ"}}], "extra_joins": []}}"""

    try:
        response = llm.invoke(prompt)
        return _parse_json_safe(response.content)
    except Exception as e:
        print(f"  ‚ö†Ô∏è GigaChat –Ω–µ —Å–º–æ–≥ –æ–ø–∏—Å–∞—Ç—å —Å–≤—è–∑–∏ {table_name}: {e}")
        return {}


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ GigaChat
# ============================================================

def generate_descriptions(llm, table_name, columns, fks, sample_columns, sample_rows, row_count):
    """
    –ü–æ–ø—Ä–æ—Å–∏—Ç—å GigaChat —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã –∏ –∫–æ–ª–æ–Ω–æ–∫.
    –ü—Ä–∏ –Ω–µ—É–¥–∞—á–µ —Å –±–æ–ª—å—à–æ–π —Ç–∞–±–ª–∏—Ü–µ–π ‚Äî —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ 2 –∑–∞–ø—Ä–æ—Å–∞.
    """
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö (–Ω–µ –±–æ–ª–µ–µ 3 —Å—Ç—Ä–æ–∫, —É–∫–æ—Ä–æ—á–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
    sample_text = ""
    if sample_rows:
        sample_text = "\n–ü—Ä–∏–º–µ—Ä—ã:\n"
        for row in sample_rows[:2]:
            parts = []
            for col, val in zip(sample_columns, row):
                val_str = str(val)[:50] if val is not None else "NULL"
                parts.append(f"{col}={val_str}")
            sample_text += "  " + ", ".join(parts[:8]) + "\n"

    columns_text = "\n".join(
        f"  - {c['name']} ({c['data_type']})"
        for c in columns
    )

    prompt = f"""–û–ø–∏—à–∏ —Ç–∞–±–ª–∏—Ü—É {table_name} ({row_count} —Å—Ç—Ä–æ–∫) –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –ö–æ–ª–æ–Ω–∫–∏:
{columns_text}
{sample_text}
–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ JSON:
{{"table_title": "–†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (2-3 —Å–ª–æ–≤–∞)", "table_description": "–û–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)", "columns": {{"col_name": {{"title": "–ù–∞–∑–≤–∞–Ω–∏–µ", "description": "–ß—Ç–æ —Ö—Ä–∞–Ω–∏—Ç"}}}}}}"""

    # –ü–æ–ø—ã—Ç–∫–∞ 1
    try:
        response = llm.invoke(prompt)
        return _parse_json_safe(response.content)
    except Exception:
        pass

    # –ü–æ–ø—ã—Ç–∫–∞ 2: —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü)
    short_cols = ", ".join(c["name"] for c in columns)
    prompt2 = f"""–¢–∞–±–ª–∏—Ü–∞ {table_name}. –ö–æ–ª–æ–Ω–∫–∏: {short_cols}.
–î–∞–π table_title (2 —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º) –∏ table_description (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ).
–î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏ –¥–∞–π title (1-2 —Å–ª–æ–≤–∞) –∏ description (–∫—Ä–∞—Ç–∫–æ).
–û—Ç–≤–µ—Ç: JSON {{"table_title":"...", "table_description":"...", "columns":{{"col":{{"title":"...", "description":"..."}}}}}}"""

    try:
        response = llm.invoke(prompt2)
        return _parse_json_safe(response.content)
    except Exception as e:
        print(f"  ‚ö†Ô∏è GigaChat –Ω–µ —Å–º–æ–≥ –æ–ø–∏—Å–∞—Ç—å {table_name}: {e}")

    # Fallback ‚Äî –±–∞–∑–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
    result = {
        "table_title": table_name.replace("_", " ").title(),
        "table_description": f"–¢–∞–±–ª–∏—Ü–∞ {table_name}",
        "columns": {}
    }
    for c in columns:
        result["columns"][c["name"]] = {
            "title": c["name"].replace("_", " ").replace("id", "").strip().title() or c["name"],
            "description": f"{c['name']} ({c['data_type']})"
        }
    return result


# ============================================================
# –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ PostgreSQL ‚Üí Cube.js
# ============================================================

def pg_type_to_cube(pg_type, column_name):
    """–°–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–∏–ø PostgreSQL —Å —Ç–∏–ø–æ–º Cube.js"""
    pg_type = pg_type.lower()
    
    # –í—Ä–µ–º—è
    if any(t in pg_type for t in ["timestamp", "date", "time"]):
        return "time"
    
    # –ß–∏—Å–ª–∞
    if any(t in pg_type for t in ["integer", "int", "bigint", "smallint", "serial",
                                    "numeric", "decimal", "real", "double", "float"]):
        return "number"
    
    # –ë—É–ª–µ–≤—ã
    if "boolean" in pg_type:
        return "boolean"
    
    # –°—Ç—Ä–æ–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    return "string"


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Cube YAML
# ============================================================

def generate_cube_yaml(table_name, columns, enriched_joins, pk, descriptions,
                       schema="public", jira_plan=None):
    """
    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å YAML-–º–æ–¥–µ–ª—å Cube –¥–ª—è –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã.
    enriched_joins ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç build_all_relationships + –æ–ø–∏—Å–∞–Ω–∏—è –æ—Ç LLM.
    jira_plan ‚Äî –¥–∞–Ω–Ω—ã–µ –∏–∑ JIRA execution plan (–¥–ª—è –¥–æ–ø. –º–µ—Ä).
    """
    
    desc = descriptions
    cube_name = table_name
    
    cube = {
        "name": cube_name,
        "sql_table": f"{schema}.{table_name}",
        "title": desc.get("table_title", table_name),
        "description": desc.get("table_description", ""),
    }
    
    # --- Joins (–æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–µ: FK + implicit + LLM) ---
    joins = []
    join_columns = set()  # –∫–æ–ª–æ–Ω–∫–∏, –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ –≤ join
    
    for j in enriched_joins:
        alias = j["alias"]
        col = j["column"]
        foreign_col = j.get("foreign_column", "id")
        join_columns.add(col)
        
        join_entry = {
            "name": alias,
            "sql": "{CUBE}." + col + " = {" + alias + "}." + foreign_col,
            "relationship": j.get("relationship", "many_to_one"),
        }
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if j.get("title"):
            join_entry["title"] = j["title"]
        if j.get("description"):
            join_entry["description"] = j["description"]
        
        joins.append(join_entry)
    
    if joins:
        cube["joins"] = joins
    
    # --- Dimensions ---
    dimensions = []
    col_descs = desc.get("columns", {})
    
    for c in columns:
        col_name = c["name"]
        cube_type = pg_type_to_cube(c["data_type"], col_name)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º FK-–∫–æ–ª–æ–Ω–∫–∏ (–æ–Ω–∏ —É—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ join)
        if col_name in join_columns and col_name != pk:
            continue
        
        dim = {
            "name": col_name,
            "sql": col_name,
            "type": cube_type,
        }
        
        if col_name == pk:
            dim["primary_key"] = True
        
        col_desc = col_descs.get(col_name, {})
        if col_desc.get("title"):
            dim["title"] = col_desc["title"]
        if col_desc.get("description"):
            dim["description"] = col_desc["description"]
        
        dimensions.append(dim)
    
    cube["dimensions"] = dimensions
    
    # --- Measures ---
    measures = [
        {
            "name": "count",
            "type": "count",
            "title": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            "description": f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ {desc.get('table_title', table_name)}"
        }
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º sum/avg –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–Ω–µ ID –∏ –Ω–µ FK)
    for c in columns:
        col_name = c["name"]
        if col_name.endswith("_id") or col_name == pk:
            continue
        if pg_type_to_cube(c["data_type"], col_name) == "number":
            col_title = col_descs.get(col_name, {}).get("title", col_name)
            measures.append({
                "name": f"total_{col_name}",
                "sql": col_name,
                "type": "sum",
                "title": f"–°—É–º–º–∞ {col_title}",
                "description": f"–°—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ª—è {col_name}"
            })
            measures.append({
                "name": f"avg_{col_name}",
                "sql": col_name,
                "type": "avg",
                "title": f"–°—Ä–µ–¥–Ω–µ–µ {col_title}",
                "description": f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è {col_name}"
            })
    
    # JIRA-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ä—ã –∏–∑ Knowledge Base
    jira_measures = get_jira_suggested_measures(table_name)
    existing_names = {m["name"] for m in measures}
    for jm in jira_measures:
        if jm["name"] not in existing_names:
            measures.append(jm)

    cube["measures"] = measures
    
    return {"cubes": [cube]}


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è semantic-–∫–æ–Ω—Ñ–∏–≥–æ–≤ (glossary, examples)
# ============================================================

def generate_glossary(all_tables_info):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–π glossary.yml"""
    glossary = {}
    
    for info in all_tables_info:
        table = info["table_name"]
        desc = info["descriptions"]
        cube_name = table
        
        # –¢–µ—Ä–º–∏–Ω –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        title = desc.get("table_title", table)
        glossary[table] = {
            "aliases": [title.lower(), table, table.replace("_", " ")],
            "semantic_type": "entity",
            "fields": [f"{cube_name}.id"],
            "filter_operator": "equals",
            "description": desc.get("table_description", "")
        }
        
        # –¢–µ—Ä–º–∏–Ω count –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        glossary[f"{table}_count"] = {
            "aliases": [
                f"–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ {title.lower()}",
                f"—Å–∫–æ–ª—å–∫–æ {title.lower()}",
            ],
            "semantic_type": "metric",
            "measures": [f"{cube_name}.count"],
            "description": f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π {title}"
        }
    
    return glossary


def generate_examples(all_tables_info):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–π examples.yml"""
    examples = []
    
    for info in all_tables_info:
        table = info["table_name"]
        desc = info["descriptions"]
        cube_name = table
        title = desc.get("table_title", table)
        
        # –ü—Ä–∏–º–µ—Ä: "—Å–∫–æ–ª—å–∫–æ <—Å—É—â–Ω–æ—Å—Ç–µ–π>"
        examples.append({
            "question": f"—Å–∫–æ–ª—å–∫–æ {title.lower()}",
            "intent": "analytics",
            "query": {
                "measures": [f"{cube_name}.count"],
                "limit": 100
            },
            "tags": ["count", table]
        })
        
        # –ü—Ä–∏–º–µ—Ä: "—Å–ø–∏—Å–æ–∫ <—Å—É—â–Ω–æ—Å—Ç–µ–π>"
        dims = []
        for c in info["columns"][:5]:
            if c["name"] != "id" and not c["name"].endswith("_id"):
                dims.append(f"{cube_name}.{c['name']}")
        
        if dims:
            examples.append({
                "question": f"—Å–ø–∏—Å–æ–∫ {title.lower()}",
                "intent": "analytics",
                "query": {
                    "measures": [f"{cube_name}.count"],
                    "dimensions": dims[:4],
                    "limit": 100
                },
                "tags": ["list", table]
            })
    
    return examples


# ============================================================
# MAIN
# ============================================================

def main():
    # –†–∞–∑–±–æ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = argparse.ArgumentParser(description="–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –≤ Cube")
    parser.add_argument("--source", choices=["postgresql", "greenplum", "hive", "duckdb", "cube"],
                        help="–ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å database.driver –∏–∑ config.yml")
    parser.add_argument("--jira-plan", metavar="FILE",
                        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É JIRA execution plan (xlsx/csv) –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
    args = parser.parse_args()

    print("=" * 60)
    print("  –ó–ê–ì–†–£–ó–ß–ò–ö –î–ê–ù–ù–´–• –í CUBE")
    print("  –ò—Å—Ç–æ—á–Ω–∏–∫ ‚Üí –û–ø–∏—Å–∞–Ω–∏—è GigaChat ‚Üí YAML-–º–æ–¥–µ–ª–∏ Cube")
    print("=" * 60)
    print()
    
    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥
    config = load_config()
    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # 2. –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É –¥–∞–Ω–Ω—ã—Ö
    source, driver_name = create_data_source(config, args.source)
    schema = config.get("database", {}).get("schema", "public")
    print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {driver_name}, –°—Ö–µ–º–∞: {schema}")
    
    # 3. –ó–∞–≥—Ä—É–∑–∏—Ç—å JIRA plan (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
    jira_plan = {}
    plan_file = args.jira_plan or config.get("jira_plan_path")
    if plan_file and Path(plan_file).exists():
        jira_plan = load_jira_plan(plan_file)
    elif plan_file:
        print(f"‚ö†Ô∏è  JIRA plan —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {plan_file}")

    # 4. –ü–æ–¥–∫–ª—é—á–∏—Ç—å GigaChat
    print("üîÑ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat...")
    llm = create_gigachat(config)
    print("‚úÖ GigaChat –≥–æ—Ç–æ–≤")
    
    # 4. –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü
    tables = source.get_tables()
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
    for t in tables:
        print(f"   - {t}")
    print()
    
    # 5. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—É—é —Ç–∞–±–ª–∏—Ü—É
    model_path = Path(config["cube"]["model_path"])
    model_path.mkdir(parents=True, exist_ok=True)
    
    all_tables_set = set(tables)
    all_tables_info = []
    
    for i, table in enumerate(tables, 1):
        print(f"[{i}/{len(tables)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: {table}")
        
        # –ß–∏—Ç–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —á–µ—Ä–µ–∑ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        columns = source.get_columns(table)
        fks = source.get_foreign_keys(table)
        pk = source.get_primary_key(table)
        row_count = source.get_row_count(table)
        sample_cols, sample_rows = source.get_sample_data(table, 5)
        
        print(f"   –ö–æ–ª–æ–Ω–æ–∫: {len(columns)}, FK: {len(fks)}, –°—Ç—Ä–æ–∫: {row_count}")
        
        # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏ (FK + implicit –ø–æ –∏–º–µ–Ω–∞–º)
        enriched_joins = build_all_relationships(table, columns, all_tables_set, fks)
        implicit_count = sum(1 for j in enriched_joins if j.get("source") == "implicit")
        if enriched_joins:
            print(f"   üîó –°–≤—è–∑–µ–π: {len(enriched_joins)} (FK: {len(fks)}, –ø–æ –∏–º–µ–Ω–∞–º: {implicit_count})")
            for j in enriched_joins:
                src = "FK" if j["source"] == "explicit" else "‚Üí"
                print(f"      {src} {j['column']} ‚Üí {j['foreign_table']} (as {j['alias']})")
        
        # –ü—Ä–æ—Å–∏–º GigaChat –æ–ø–∏—Å–∞—Ç—å —Å–≤—è–∑–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
        if enriched_joins:
            print(f"   ü§ñ GigaChat: –æ–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–µ–π...")
            join_suggestions = suggest_joins_via_llm(llm, table, columns, enriched_joins, all_tables_set)
            
            # –û–±–æ–≥–∞—â–∞–µ–º joins –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –æ—Ç LLM
            llm_joins_map = {}
            for lj in join_suggestions.get("joins", []):
                key = lj.get("column", "")
                llm_joins_map[key] = lj
            
            for j in enriched_joins:
                llm_info = llm_joins_map.get(j["column"], {})
                if llm_info.get("title"):
                    j["title"] = llm_info["title"]
                if llm_info.get("description"):
                    j["description"] = llm_info["description"]
                if llm_info.get("alias") and llm_info["alias"] != j["alias"]:
                    j["alias"] = llm_info["alias"]
            
            # –î–æ–±–∞–≤–ª—è–µ–º extra_joins –æ—Ç LLM
            for extra in join_suggestions.get("extra_joins", []):
                extra_table = extra.get("foreign_table", "")
                if extra_table in all_tables_set and extra_table != table:
                    col_names = {c["name"] for c in columns}
                    if extra.get("column") in col_names:
                        enriched_joins.append({
                            "column": extra["column"],
                            "foreign_table": extra_table,
                            "alias": extra.get("alias", extra_table),
                            "foreign_column": extra.get("foreign_column", "id"),
                            "relationship": "many_to_one",
                            "title": extra.get("title", ""),
                            "description": extra.get("description", ""),
                            "source": "llm"
                        })
                        print(f"      ‚ú® LLM –ø—Ä–µ–¥–ª–æ–∂–∏–ª: {extra['column']} ‚Üí {extra_table}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ GigaChat
        print(f"   ü§ñ GigaChat: –æ–ø–∏—Å–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –∏ –∫–æ–ª–æ–Ω–æ–∫...")
        descriptions = generate_descriptions(
            llm, table, columns, fks, sample_cols, sample_rows, row_count
        )

        # –û–±–æ–≥–∞—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ JIRA Knowledge Base
        jira_hints = match_jira_hints(table, jira_plan)
        if jira_hints:
            print(f"   üìö JIRA KB: {jira_hints.get('title', 'match found')}")
            descriptions = enrich_descriptions_with_jira(descriptions, table, columns, jira_plan)
            if not descriptions.get("table_description") or len(descriptions["table_description"]) < 10:
                descriptions["table_description"] = jira_hints.get("description", descriptions.get("table_description", ""))
            if not descriptions.get("table_title") or descriptions["table_title"] == table:
                descriptions["table_title"] = jira_hints.get("title", descriptions.get("table_title", table))

        print(f"   ‚úÖ –û–ø–∏—Å–∞–Ω–∏—è: {descriptions.get('table_title', '?')}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Cube YAML
        cube_schema = schema if driver_name != "duckdb" else "main"
        cube_yaml = generate_cube_yaml(table, columns, enriched_joins, pk, descriptions,
                                        cube_schema, jira_plan)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        yaml_path = model_path / f"{table}.yml"
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(cube_yaml, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {yaml_path}")
        
        all_tables_info.append({
            "table_name": table,
            "columns": columns,
            "fks": fks,
            "enriched_joins": enriched_joins,
            "descriptions": descriptions
        })
        print()
    
    # 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º semantic-–∫–æ–Ω—Ñ–∏–≥–∏
    config_path = Path("config")
    config_path.mkdir(exist_ok=True)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è glossary.yml...")
    glossary = generate_glossary(all_tables_info)
    with open(config_path / "glossary.yml", 'w', encoding='utf-8') as f:
        yaml.dump(glossary, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è examples.yml...")
    examples = generate_examples(all_tables_info)
    with open(config_path / "examples.yml", 'w', encoding='utf-8') as f:
        yaml.dump(examples, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è semantic_layer.yml...")
    layer_config = {
        "cube": {
            "base_url": config["cube"]["api_url"],
            "enabled": True,
            "preferred_cubes": [t["table_name"] for t in all_tables_info]
        },
        "intents": {
            "analytics": {
                "description": "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ Cube",
                "keywords": ["—Å–∫–æ–ª—å–∫–æ", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–ø–æ–∫–∞–∂–∏", "—Å–ø–∏—Å–æ–∫",
                             "—Å—Ä–µ–¥–Ω–∏–π", "—Å—É–º–º–∞", "—Ç–æ–ø", "–≤—Å–µ–≥–æ", "–ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º"],
                "priority": 1
            }
        },
        "query_generation": {
            "default_limit": 100,
            "max_limit": 10000
        }
    }
    with open(config_path / "semantic_layer.yml", 'w', encoding='utf-8') as f:
        yaml.dump(layer_config, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    source.close()
    
    # –°–≤–æ–¥–∫–∞ –ø–æ —Å–≤—è–∑—è–º
    total_joins = sum(len(info.get("enriched_joins", [])) for info in all_tables_info)
    fk_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "explicit")
        for info in all_tables_info
    )
    implicit_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "implicit")
        for info in all_tables_info
    )
    llm_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "llm")
        for info in all_tables_info
    )
    
    print()
    print("=" * 60)
    print("  ‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 60)
    print(f"""
–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: {driver_name}
–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:
  üìÅ {model_path}/          ‚Äî YAML-–º–æ–¥–µ–ª–∏ Cube ({len(tables)} —Ñ–∞–π–ª–æ–≤)
  üìÅ config/glossary.yml    ‚Äî –ë–∏–∑–Ω–µ—Å-–≥–ª–æ—Å—Å–∞—Ä–∏–π
  üìÅ config/examples.yml    ‚Äî –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
  üìÅ config/semantic_layer.yml ‚Äî –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è

–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ (joins):
  üîó –í—Å–µ–≥–æ: {total_joins}
     - –ò–∑ FK constraints: {fk_joins}
     - –ü–æ –∏–º–µ–Ω–∞–º –∫–æ–ª–æ–Ω–æ–∫: {implicit_joins}
     - –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–æ GigaChat: {llm_joins}

–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
  1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –∏–∑ {model_path}/ –≤ –ø–∞–ø–∫—É model/cubes/ –≤–∞—à–µ–≥–æ Cube-–ø—Ä–æ–µ–∫—Ç–∞
  2. –ü–†–û–í–ï–†–¨–¢–ï –°–í–Ø–ó–ò: –æ—Ç–∫—Ä–æ–π—Ç–µ YAML-—Ñ–∞–π–ª—ã –∏ —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ joins –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
  3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Cube: npx cubejs-server
  4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: curl http://localhost:4000/cubejs-api/v1/meta
  5. –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ glossary.yml –∏ examples.yml –ø–æ–¥ –≤–∞—à–∏ –∑–∞–¥–∞—á–∏
  6. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 02_build_faiss.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
  7. –û—Ç–∫—Ä–æ–π—Ç–µ 03_agent.ipynb –≤ JupyterLab –∏ –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã
""")


if __name__ == "__main__":
    main()
