"""
=================================================================
–ó–ê–ì–†–£–ó–ß–ò–ö –î–ê–ù–ù–´–• –í CUBE
=================================================================
–°–∫—Ä–∏–ø—Ç —á–∏—Ç–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è
–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —á–µ—Ä–µ–∑ GigaChat –∏ —Å–æ–∑–¥–∞—ë—Ç YAML-–º–æ–¥–µ–ª–∏ –¥–ª—è Cube.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–µ–∂–∏–º—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö (database.driver –≤ config.yml):
  - postgresql              ‚Äî –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ (psycopg2)
  - greenplum               ‚Äî Greenplum —á–µ—Ä–µ–∑ SQLAlchemy (Kerberos –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
  - hive                    ‚Äî Hive —á–µ—Ä–µ–∑ SQLAlchemy/PyHive (Kerberos)
  - duckdb                  ‚Äî –ª–æ–∫–∞–ª—å–Ω—ã–π DuckDB-—Ñ–∞–π–ª
  - cube                    ‚Äî —á—Ç–µ–Ω–∏–µ –∏–∑ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ Cube API (–±–µ–∑ –ë–î)

Knowledge Base (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
  config.yml ‚Üí knowledge_base_path: "./kb/jira_kb.yml"
  –í–Ω–µ—à–Ω–∏–π YAML-—Ñ–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ —Ç–∞–±–ª–∏—Ü, –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
  –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–º–∏ –º–µ—Ä–∞–º–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞.

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:
  --kb <file.yml>           ‚Äî –ø—É—Ç—å –∫ Knowledge Base (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config.yml)
  --etl-plan <file.xlsx>    ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑ ETL execution plan —Ñ–∞–π–ª–∞
  --enrich-etl              ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å –£–ñ–ï –°–£–©–ï–°–¢–í–£–Æ–©–ò–ï –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ ETL plan
  --enrich-with-llm         ‚Äî –ø—Ä–∏ --enrich-etl –ø–µ—Ä–µ–æ–ø–∏—Å–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ —á–µ—Ä–µ–∑ GigaChat
  --model-dir <dir>         ‚Äî –ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—è–º–∏ (–¥–ª—è --enrich-etl)

–ó–∞–ø—É—Å–∫:
  –ü–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è:
    python 01_data_loader.py
    python 01_data_loader.py --source cube
    python 01_data_loader.py --kb ./kb/jira_kb.yml --etl-plan plan.xlsx

  –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π (–±–µ–∑ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏):
    python 01_data_loader.py --enrich-etl --etl-plan plan.xlsx
    python 01_data_loader.py --enrich-etl --etl-plan plan.xlsx --enrich-with-llm
    python 01_data_loader.py --enrich-etl --etl-plan plan.xlsx --model-dir ../model/cubes
=================================================================
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path
from datetime import datetime

# ============================================================
# –ê–≤—Ç–æ—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
# ============================================================

def _ensure_packages():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã (–±–∞–∑–æ–≤—ã–µ)"""
    required = {
        "yaml": "pyyaml",
        "langchain_gigachat": "langchain-gigachat",
    }
    missing = []
    for module, pip_name in required.items():
        try:
            __import__(module)
        except ImportError:
            missing.append(pip_name)
    if missing:
        print(f"üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–∞–∫–µ—Ç–æ–≤: {', '.join(missing)}")
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "--quiet"] + missing
        )
        print("‚úÖ –ü–∞–∫–µ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

_ensure_packages()

import yaml

# psycopg2 / duckdb –ø–æ–¥–≥—Ä—É–∂–∞—é—Ç—Å—è –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ create_data_source()
try:
    import psycopg2
except ImportError:
    psycopg2 = None  # –ù–µ –Ω—É–∂–µ–Ω –¥–ª—è duckdb/cube —Ä–µ–∂–∏–º–æ–≤

# ============================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
# ============================================================

def load_config(config_path="config.yml"):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML-—Ñ–∞–π–ª–∞"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


# ============================================================
# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat
# ============================================================

def create_gigachat(config):
    """–°–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç GigaChat. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 2 —Ä–µ–∂–∏–º–∞:
       1. credentials ‚Äî –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø (SberCloud)
       2. base_url + access_token ‚Äî —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ (–∑–∞–∫—Ä—ã—Ç—ã–π –∫–æ–Ω—Ç—É—Ä)
    """
    from langchain_gigachat import GigaChat
    
    gc = config["gigachat"]
    model = gc.get("model", "GigaChat")
    
    # –†–µ–∂–∏–º 2: —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ (base_url + access_token –∏–∑ env)
    if gc.get("base_url"):
        token_env = gc.get("access_token_env", "JPY_API_TOKEN")
        token = os.getenv(token_env, "")
        return GigaChat(
            base_url=gc["base_url"],
            access_token=token,
            model=model,
            timeout=gc.get("timeout", 60)
        )
    
    # –†–µ–∂–∏–º 1: —á–µ—Ä–µ–∑ credentials
    if gc.get("credentials"):
        return GigaChat(
            credentials=gc["credentials"],
            model=model,
            verify_ssl_certs=gc.get("verify_ssl", False),
            timeout=gc.get("timeout", 60)
        )
    
    print("‚ùå –û—à–∏–±–∫–∞: –ó–∞–ø–æ–ª–Ω–∏—Ç–µ gigachat.credentials –∏–ª–∏ gigachat.base_url –≤ config.yml")
    sys.exit(1)


# ============================================================
# –ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ë–î
# ============================================================

def get_db_connection(config):
    """–ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ PostgreSQL / GreenPlum"""
    if psycopg2 is None:
        print("‚ùå psycopg2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install psycopg2-binary")
        print("   –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --source duckdb / --source cube")
        sys.exit(1)
    db = config["database"]
    return psycopg2.connect(
        host=db["host"],
        port=db["port"],
        dbname=db["name"],
        user=db["user"],
        password=db["password"]
    )


def get_schema(config):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è —Å—Ö–µ–º—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
    return config.get("database", {}).get("schema", "public")


def get_tables(conn, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü"""
    cur = conn.cursor()
    cur.execute("""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = %s 
          AND table_type = 'BASE TABLE'
        ORDER BY table_name
    """, (schema,))
    tables = [row[0] for row in cur.fetchall()]
    cur.close()
    return tables


def get_columns(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ —Ç–∞–±–ª–∏—Ü—ã —Å —Ç–∏–ø–∞–º–∏"""
    cur = conn.cursor()
    cur.execute("""
        SELECT column_name, data_type, is_nullable, column_default,
               character_maximum_length
        FROM information_schema.columns
        WHERE table_schema = %s AND table_name = %s
        ORDER BY ordinal_position
    """, (schema, table_name))
    columns = []
    for row in cur.fetchall():
        columns.append({
            "name": row[0],
            "data_type": row[1],
            "nullable": row[2] == "YES",
            "default": row[3],
            "max_length": row[4]
        })
    cur.close()
    return columns


def get_foreign_keys(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏ —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    cur.execute("""
        SELECT
            kcu.column_name,
            ccu.table_name AS foreign_table,
            ccu.column_name AS foreign_column
        FROM information_schema.table_constraints AS tc
        JOIN information_schema.key_column_usage AS kcu
            ON tc.constraint_name = kcu.constraint_name
        JOIN information_schema.constraint_column_usage AS ccu
            ON ccu.constraint_name = tc.constraint_name
        WHERE tc.constraint_type = 'FOREIGN KEY'
          AND tc.table_name = %s
          AND tc.table_schema = %s
    """, (table_name, schema))
    fks = []
    for row in cur.fetchall():
        fks.append({
            "column": row[0],
            "foreign_table": row[1],
            "foreign_column": row[2]
        })
    cur.close()
    return fks


def get_primary_key(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å primary key —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    cur.execute("""
        SELECT kcu.column_name
        FROM information_schema.table_constraints tc
        JOIN information_schema.key_column_usage kcu
            ON tc.constraint_name = kcu.constraint_name
        WHERE tc.constraint_type = 'PRIMARY KEY'
          AND tc.table_name = %s
          AND tc.table_schema = %s
    """, (table_name, schema))
    result = cur.fetchone()
    cur.close()
    return result[0] if result else "id"


def get_sample_data(conn, table_name, schema="public", limit=5):
    """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    try:
        cur.execute(f'SELECT * FROM {schema}."{table_name}" LIMIT %s', (limit,))
        columns = [desc[0] for desc in cur.description]
        rows = cur.fetchall()
        cur.close()
        return columns, rows
    except Exception:
        cur.close()
        return [], []


def get_row_count(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫"""
    cur = conn.cursor()
    cur.execute(f'SELECT COUNT(*) FROM {schema}."{table_name}"')
    count = cur.fetchone()[0]
    cur.close()
    return count


# ============================================================
# –ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑ DuckDB
# ============================================================

class DuckDBSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ‚Äî –ª–æ–∫–∞–ª—å–Ω—ã–π DuckDB-—Ñ–∞–π–ª.
    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ç–æ—Ç –∂–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á—Ç–æ –∏ psycopg2-—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã—à–µ.
    """

    def __init__(self, config):
        import duckdb
        db_path = config["database"].get("path", "./data.duckdb")
        self.schema = config["database"].get("schema", "main")
        self.conn = duckdb.connect(db_path, read_only=True)
        print(f"‚úÖ DuckDB: {db_path} (schema={self.schema})")

    def get_tables(self):
        rows = self.conn.execute(
            "SELECT table_name FROM information_schema.tables "
            "WHERE table_schema = ? AND table_type = 'BASE TABLE' "
            "ORDER BY table_name",
            [self.schema]
        ).fetchall()
        return [r[0] for r in rows]

    def get_columns(self, table_name):
        rows = self.conn.execute(
            "SELECT column_name, data_type, is_nullable, column_default, "
            "character_maximum_length "
            "FROM information_schema.columns "
            "WHERE table_schema = ? AND table_name = ? "
            "ORDER BY ordinal_position",
            [self.schema, table_name]
        ).fetchall()
        return [{
            "name": r[0],
            "data_type": r[1],
            "nullable": r[2] == "YES",
            "default": r[3],
            "max_length": r[4]
        } for r in rows]

    def get_foreign_keys(self, table_name):
        # DuckDB –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç FK, –Ω–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω—è–µ—Ç information_schema
        try:
            rows = self.conn.execute(
                "SELECT kcu.column_name, ccu.table_name, ccu.column_name "
                "FROM information_schema.table_constraints tc "
                "JOIN information_schema.key_column_usage kcu "
                "  ON tc.constraint_name = kcu.constraint_name "
                "JOIN information_schema.constraint_column_usage ccu "
                "  ON ccu.constraint_name = tc.constraint_name "
                "WHERE tc.constraint_type = 'FOREIGN KEY' "
                "  AND tc.table_name = ? AND tc.table_schema = ?",
                [table_name, self.schema]
            ).fetchall()
            return [{"column": r[0], "foreign_table": r[1], "foreign_column": r[2]}
                    for r in rows]
        except Exception:
            return []

    def get_primary_key(self, table_name):
        try:
            rows = self.conn.execute(
                "SELECT kcu.column_name "
                "FROM information_schema.table_constraints tc "
                "JOIN information_schema.key_column_usage kcu "
                "  ON tc.constraint_name = kcu.constraint_name "
                "WHERE tc.constraint_type = 'PRIMARY KEY' "
                "  AND tc.table_name = ? AND tc.table_schema = ?",
                [table_name, self.schema]
            ).fetchall()
            return rows[0][0] if rows else "id"
        except Exception:
            return "id"

    def get_sample_data(self, table_name, limit=5):
        try:
            result = self.conn.execute(
                f'SELECT * FROM {self.schema}."{table_name}" LIMIT ?', [limit]
            )
            columns = [desc[0] for desc in result.description]
            rows = result.fetchall()
            return columns, rows
        except Exception:
            return [], []

    def get_row_count(self, table_name):
        result = self.conn.execute(
            f'SELECT COUNT(*) FROM {self.schema}."{table_name}"'
        )
        return result.fetchone()[0]

    def close(self):
        self.conn.close()


# ============================================================
# –ß—Ç–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ Cube API
# ============================================================

class CubeAPISource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ‚Äî –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ Cube REST API /meta.
    –ù–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –≤–æ–æ–±—â–µ.
    Cube —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç ‚Üí —á–∏—Ç–∞–µ–º –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
    """

    def __init__(self, config):
        import httpx
        self.cube_url = config["cube"]["api_url"]
        self.schema = config["database"].get("schema", "public")
        headers = {}
        token = config["cube"].get("api_token", "")
        if token:
            headers["Authorization"] = f"Bearer {token}"

        resp = httpx.get(f"{self.cube_url}/meta", headers=headers, timeout=15.0)
        resp.raise_for_status()
        self.meta = resp.json()
        self.cubes = {c["name"]: c for c in self.meta.get("cubes", [])}
        print(f"‚úÖ Cube API: {len(self.cubes)} –∫—É–±–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {self.cube_url}")

    def get_tables(self):
        return sorted(self.cubes.keys())

    def get_columns(self, table_name):
        cube = self.cubes.get(table_name, {})
        columns = []
        for dim in cube.get("dimensions", []):
            short_name = dim["name"].split(".")[-1]
            columns.append({
                "name": short_name,
                "data_type": dim.get("type", "string"),
                "nullable": True,
                "default": None,
                "max_length": None
            })
        return columns

    def get_foreign_keys(self, table_name):
        # Cube API –Ω–µ –æ—Ç–¥–∞—ë—Ç FK ‚Äî joins –æ–±–Ω–∞—Ä—É–∂–∏–º –ø–æ –∏–º–µ–Ω–∞–º –∫–æ–ª–æ–Ω–æ–∫
        return []

    def get_primary_key(self, table_name):
        cube = self.cubes.get(table_name, {})
        for dim in cube.get("dimensions", []):
            if dim.get("primaryKey"):
                return dim["name"].split(".")[-1]
        return "id"

    def get_sample_data(self, table_name, limit=5):
        # –ù–µ –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å sample data —á–µ—Ä–µ–∑ Cube API
        return [], []

    def get_row_count(self, table_name):
        # –ü—Ä–æ–±—É–µ–º count –∏–∑ Cube
        import httpx
        cube = self.cubes.get(table_name, {})
        count_measure = None
        for m in cube.get("measures", []):
            if m.get("type") == "count":
                count_measure = m["name"]
                break
        if not count_measure:
            return 0
        try:
            headers = {"Content-Type": "application/json"}
            resp = httpx.post(
                f"{self.cube_url}/load",
                json={"query": {"measures": [count_measure], "limit": 1}},
                headers=headers,
                timeout=15.0
            )
            data = resp.json().get("data", [])
            if data:
                return list(data[0].values())[0]
        except Exception:
            pass
        return 0

    def close(self):
        pass


# ============================================================
# –§–∞–±—Ä–∏–∫–∞: –≤—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ============================================================

def create_data_source(config, override_source=None):
    """–°–æ–∑–¥–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ config.yml –∏–ª–∏ --source –∞—Ä–≥—É–º–µ–Ω—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –º–µ—Ç–æ–¥–∞–º–∏: get_tables, get_columns, get_foreign_keys,
    get_primary_key, get_sample_data, get_row_count, close.
    """
    driver = override_source or config.get("database", {}).get("driver", "postgresql")
    driver = driver.lower().strip()

    if driver == "duckdb":
        return DuckDBSource(config), driver
    elif driver == "cube":
        return CubeAPISource(config), driver
    elif driver in ("postgresql", "postgres"):
        conn = get_db_connection(config)
        schema = get_schema(config)
        return _PsycopgSource(conn, schema), driver
    elif driver == "greenplum":
        from db_sources import GreenplumSource
        return GreenplumSource(config), driver
    elif driver == "hive":
        from db_sources import HiveSource
        return HiveSource(config), driver
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π driver: {driver}")
        print("   –î–æ–ø—É—Å—Ç–∏–º—ã–µ: postgresql, greenplum, hive, duckdb, cube")
        sys.exit(1)


class _PsycopgSource:
    """–û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ psycopg2 –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."""
    def __init__(self, conn, schema):
        self.conn = conn
        self.schema = schema

    def get_tables(self):
        return get_tables(self.conn, self.schema)

    def get_columns(self, table_name):
        return get_columns(self.conn, table_name, self.schema)

    def get_foreign_keys(self, table_name):
        return get_foreign_keys(self.conn, table_name, self.schema)

    def get_primary_key(self, table_name):
        return get_primary_key(self.conn, table_name, self.schema)

    def get_sample_data(self, table_name, limit=5):
        return get_sample_data(self.conn, table_name, self.schema, limit)

    def get_row_count(self, table_name):
        return get_row_count(self.conn, table_name, self.schema)

    def close(self):
        self.conn.close()


# ============================================================
# Knowledge Base ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è —Ç–∞–±–ª–∏—Ü
# ============================================================

_KNOWLEDGE_BASE = {}  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ YAML-—Ñ–∞–π–ª–∞


def load_knowledge_base(kb_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å Knowledge Base –∏–∑ YAML-—Ñ–∞–π–ª–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: pattern_name ‚Üí {title, description, column_hints, suggested_measures}.
    """
    global _KNOWLEDGE_BASE
    try:
        with open(kb_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f) or {}
        _KNOWLEDGE_BASE = data
        print(f"‚úÖ Knowledge Base –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(data)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ {kb_path}")
        return data
    except FileNotFoundError:
        print(f"‚ö†Ô∏è  KB —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {kb_path}")
        return {}
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ KB: {e}")
        return {}


def load_etl_plan(plan_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å ETL execution plan —Ñ–∞–π–ª (xlsx/csv).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: source_table_name ‚Üí {columns from plan}.
    """
    info = {}
    plan_path_lower = plan_path.lower()

    try:
        if plan_path_lower.endswith(".xlsx") or plan_path_lower.endswith(".xls"):
            import openpyxl
            wb = openpyxl.load_workbook(plan_path, data_only=True)
            ws = wb.active
            headers = [str(c.value or "").strip().lower() for c in ws[1]]
            for row in ws.iter_rows(min_row=2, values_only=True):
                rec = dict(zip(headers, row))
                src_table = str(rec.get("source_table", "") or "").strip()
                if src_table:
                    info[src_table] = {
                        "source_schema": str(rec.get("source_schema", "") or ""),
                        "source_cluster": str(rec.get("source_cluster", "") or ""),
                        "target_table": str(rec.get("table_step2", "") or ""),
                        "process_description": str(rec.get("process_description", "") or "")[:500],
                        "last_updated": str(rec.get("last_updated_time", "") or ""),
                    }
        elif plan_path_lower.endswith(".csv"):
            import csv
            with open(plan_path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for rec in reader:
                    src_table = rec.get("source_table", "").strip()
                    if src_table:
                        info[src_table] = {
                            "source_schema": rec.get("source_schema", ""),
                            "source_cluster": rec.get("source_cluster", ""),
                            "target_table": rec.get("table_step2", ""),
                            "process_description": rec.get("process_description", "")[:500],
                            "last_updated": rec.get("last_updated_time", ""),
                        }
        else:
            print(f"‚ö†Ô∏è  –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç ETL plan: {plan_path}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ .xlsx –∏–ª–∏ .csv")
            return {}

        print(f"‚úÖ ETL plan –∑–∞–≥—Ä—É–∂–µ–Ω: {len(info)} source-—Ç–∞–±–ª–∏—Ü –∏–∑ {plan_path}")
        for t in info:
            print(f"   - {t}")
        return info

    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ETL plan: {e}")
        return {}


def _singularize(word: str) -> set:
    """–í–µ—Ä–Ω—É—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–æ—Ä–º –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞."""
    forms = {word}
    if word.endswith("ies") and len(word) > 4:
        forms.add(word[:-3] + "y")          # priorities ‚Üí priority
    if word.endswith("ses") or word.endswith("xes"):
        forms.add(word[:-2])                # statuses ‚Üí status
    if word.endswith("es") and len(word) > 3:
        forms.add(word[:-2])                # statuses ‚Üí status
    if word.endswith("s") and not word.endswith("ss"):
        forms.add(word[:-1])                # issues ‚Üí issue
    return forms


def match_kb_hints(table_name: str, etl_plan: dict = None) -> dict:
    """–ù–∞–π—Ç–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∏–∑ Knowledge Base –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã.
    –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è (issue_links ‚Üî issuelink),
    –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (priorities ‚Üî priority), –ø—Ä–µ—Ñ–∏–∫—Å—ã (jiraissue ‚Üî issues).
    """
    tl = table_name.lower()
    tl_no_sep = tl.replace("_", "").replace("-", "")
    tl_singulars = _singularize(tl_no_sep)

    for pattern, hints in _KNOWLEDGE_BASE.items():
        pat_no_sep = pattern.replace("_", "")
        pat_singulars = _singularize(pat_no_sep)

        # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π + —á–∏—Å–ª–∞)
        if pat_no_sep == tl_no_sep:
            return hints
        if tl_singulars & pat_singulars:
            return hints

        # 2. –ü—Ä–µ—Ñ–∏–∫—Å "jira" –≤ KB: jiraissue ‚Üí issue ‚Üî issues
        if pat_no_sep.startswith("jira"):
            core = pat_no_sep[4:]
            core_singulars = _singularize(core)
            if tl_singulars & core_singulars:
                return hints

        # 3. –ü—Ä–µ—Ñ–∏–∫—Å "project" –≤ KB: projectversion ‚Üí version ‚Üî versions
        if pat_no_sep.startswith("project"):
            core = pat_no_sep[7:]
            core_singulars = _singularize(core)
            if tl_singulars & core_singulars:
                return hints

        # 4. –°—É—Ñ—Ñ–∏–∫—Å–Ω—ã–π –º–∞—Ç—á: dm_jira_components ‚Üí component
        for pf in pat_singulars:
            if len(pf) >= 5 and any(tf.endswith(pf) for tf in tl_singulars):
                return hints

        # 5. –ü–æ–¥—Å—Ç—Ä–æ–∫–∞ >= 6 —Å–∏–º–≤–æ–ª–æ–≤ (–∏–∑–±–µ–≥–∞–µ—Ç –ª–æ–∂–Ω—ã—Ö –º–∞—Ç—á–µ–π)
        for pf in pat_singulars:
            if len(pf) >= 6 and pf in tl_no_sep:
                return hints

    if etl_plan:
        for src_table, plan_info in etl_plan.items():
            src_no_sep = src_table.lower().replace("_", "")
            src_singulars = _singularize(src_no_sep)
            if tl_singulars & src_singulars:
                return {
                    "title": f"–¢–∞–±–ª–∏—Ü–∞ –∏–∑ ETL ({src_table})",
                    "description": f"–ò—Å—Ç–æ—á–Ω–∏–∫: {plan_info.get('source_schema', '')}.{src_table}. "
                                   f"–¶–µ–ª–µ–≤–∞—è: {plan_info.get('target_table', '')}.",
                }

    return {}


def enrich_descriptions_with_kb(descriptions: dict, table_name: str,
                                columns: list, etl_plan: dict = None) -> dict:
    """–î–æ–ø–æ–ª–Ω–∏—Ç—å GigaChat-–æ–ø–∏—Å–∞–Ω–∏—è –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –∏–∑ Knowledge Base."""
    hints = match_kb_hints(table_name, etl_plan)
    if not hints:
        return descriptions

    col_descs = descriptions.get("columns", {})
    col_hints = hints.get("column_hints", {})

    for col_name, hint_text in col_hints.items():
        if col_name in col_descs:
            existing = col_descs[col_name]
            if not existing.get("description") or len(existing["description"]) < 10:
                existing["description"] = hint_text
        else:
            for c in columns:
                if c["name"].lower() == col_name or col_name in c["name"].lower():
                    col_descs[c["name"]] = col_descs.get(c["name"], {})
                    if not col_descs[c["name"]].get("description"):
                        col_descs[c["name"]]["description"] = hint_text
                    break

    descriptions["columns"] = col_descs
    return descriptions


def get_kb_suggested_measures(table_name: str) -> list:
    """–ü–æ–ª—É—á–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ä—ã –∏–∑ Knowledge Base."""
    hints = match_kb_hints(table_name)
    return hints.get("suggested_measures", [])


# ============================================================
# –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏
# ============================================================

def detect_implicit_relationships(table_name, columns, all_tables, explicit_fks):
    """
    –ù–∞–π—Ç–∏ –Ω–µ—è–≤–Ω—ã–µ —Å–≤—è–∑–∏ –ø–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏—é –æ–± –∏–º–µ–Ω–∞—Ö (*_id ‚Üí —Ç–∞–±–ª–∏—Ü–∞).
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
      - –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: project_id ‚Üí projects
      - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ: priority_id ‚Üí priorities, status_id ‚Üí statuses
      - –ü—Ä–µ—Ñ–∏–∫—Å—ã –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü: status_id ‚Üí issue_statuses, type_id ‚Üí issue_types
      - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–∞–ø–ø–∏–Ω–≥–∏: assignee_id ‚Üí users, reporter_id ‚Üí users, author_id ‚Üí users
    """
    explicit_cols = {fk["column"] for fk in explicit_fks}
    implicit = []

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–∞–ø–ø–∏–Ω–≥–∏: –∫–æ–ª–æ–Ω–∫–∞ ‚Üí —Ü–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
    SEMANTIC_MAP = {
        "assignee": "users",
        "reporter": "users",
        "author": "users",
        "creator": "users",
        "owner": "users",
        "updated_by": "users",
        "created_by": "users",
        "lead": "users",
        "manager": "users",
        "parent": None,  # self-join –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
    }

    for col in columns:
        col_name = col["name"]
        if not col_name.endswith("_id") or col_name in explicit_cols:
            continue
        if col_name == "id":
            continue

        base = col_name[:-3]  # "project_id" ‚Üí "project"

        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥
        if base in SEMANTIC_MAP:
            target = SEMANTIC_MAP[base]
            if target and target in all_tables:
                implicit.append({
                    "column": col_name,
                    "foreign_table": target,
                    "foreign_column": "id",
                    "source": "implicit"
                })
                continue
            elif target is None and table_name in all_tables:
                # self-join (parent_id ‚Üí —Ç–∞ –∂–µ —Ç–∞–±–ª–∏—Ü–∞)
                implicit.append({
                    "column": col_name,
                    "foreign_table": table_name,
                    "foreign_column": "id",
                    "source": "implicit"
                })
                continue

        # 2. –ü—Ä—è–º—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –ø–æ –∏–º–µ–Ω–∏
        candidates = [
            base + "s",       # project ‚Üí projects
            base + "es",      # status ‚Üí statuses
            base,             # sprint ‚Üí sprint
        ]
        if base.endswith("y"):
            candidates.append(base[:-1] + "ies")  # priority ‚Üí priorities, category ‚Üí categories
        if base.endswith("s"):
            candidates.append(base)

        matched_table = None
        for candidate in candidates:
            if candidate in all_tables and candidate != table_name:
                matched_table = candidate
                break

        # 3. –ï—Å–ª–∏ –ø—Ä—è–º–æ–µ –Ω–µ –Ω–∞—à–ª–æ ‚Äî –ø—Ä–æ–±—É–µ–º —Å –¥–æ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏
        if not matched_table:
            # –ü–æ–ª—É—á–∞–µ–º ¬´–¥–æ–º–µ–Ω¬ª –∏–∑ –∏–º–µ–Ω–∏ —Ç–µ–∫—É—â–µ–π —Ç–∞–±–ª–∏—Ü—ã (issue_comments ‚Üí issue)
            # –∏ –ø—Ä–æ–±—É–µ–º prefix_base (issue_status, issue_priority, etc.)
            prefixes_to_try = set()
            # –ò–∑ –∏–º–µ–Ω–∏ —Ç–∞–±–ª–∏—Ü—ã: issues ‚Üí issue, issue_comments ‚Üí issue
            parts = table_name.split("_")
            if parts:
                singular = parts[0].rstrip("s")
                prefixes_to_try.add(singular)       # "issue"
                prefixes_to_try.add(parts[0])       # "issues"

            # –¢–∞–∫–∂–µ –æ–±—â–∏–µ –¥–æ–º–µ–Ω–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
            prefixes_to_try.update(["issue", "project", "workflow", "notification", "permission"])

            for prefix in prefixes_to_try:
                prefix_candidates = [
                    f"{prefix}_{base}s",       # issue_statuses
                    f"{prefix}_{base}es",      # issue_statuses
                    f"{prefix}_{base}",        # issue_type
                ]
                if base.endswith("y"):
                    prefix_candidates.append(f"{prefix}_{base[:-1]}ies")  # issue_priorities
                
                for candidate in prefix_candidates:
                    if candidate in all_tables and candidate != table_name:
                        matched_table = candidate
                        break
                if matched_table:
                    break

        # 4. –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äî –ø–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö base –≤ –∏–º–µ–Ω–∏
        if not matched_table:
            for t in all_tables:
                if t != table_name and base in t and t.endswith("s"):
                    matched_table = t
                    break

        if matched_table:
            implicit.append({
                "column": col_name,
                "foreign_table": matched_table,
                "foreign_column": "id",
                "source": "implicit"
            })

    return implicit


def build_all_relationships(table_name, columns, all_tables, explicit_fks):
    """
    –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —è–≤–Ω—ã–µ FK –∏ –Ω–µ—è–≤–Ω—ã–µ —Å–≤—è–∑–∏.
    –ü—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–∫–∞—Ö –Ω–∞ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–ª–∏–∞—Å—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ join-–∑–∞–ø–∏—Å–µ–π:
      [{"column": "assignee_id", "foreign_table": "users", "alias": "users_assignee",
        "foreign_column": "id", "relationship": "many_to_one"}]
    """
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏
    all_rels = []
    for fk in explicit_fks:
        all_rels.append({**fk, "source": "explicit"})

    implicit = detect_implicit_relationships(table_name, columns, all_tables, explicit_fks)
    all_rels.extend(implicit)

    if not all_rels:
        return []

    # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –∫–∞–∂–¥–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç –∫–∞–∫ —Ü–µ–ª—å
    target_count = {}
    for rel in all_rels:
        t = rel["foreign_table"]
        target_count[t] = target_count.get(t, 0) + 1

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å –∞–ª–∏–∞—Å–∞–º–∏
    joins = []
    for rel in all_rels:
        target = rel["foreign_table"]
        col = rel["column"]
        base = col[:-3] if col.endswith("_id") else col  # assignee_id ‚Üí assignee

        # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è >1 —Ä–∞–∑ ‚Äî –∞–ª–∏–∞—Å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω
        if target_count[target] > 1:
            alias = f"{target}_{base}"  # users_assignee, users_reporter
        else:
            alias = target

        joins.append({
            "column": col,
            "foreign_table": target,
            "alias": alias,
            "foreign_column": rel.get("foreign_column", "id"),
            "relationship": "many_to_one",
            "source": rel.get("source", "explicit")
        })

    return joins


def _fix_missing_commas(text):
    """–í—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ –≤ JSON –æ—Ç GigaChat.
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∫ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π, —Ç–∞–∫ –∏ –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–π JSON.
    """
    import re
    # --- –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π: "value"\n  "key" ‚Üí "value",\n  "key" ---
    text = re.sub(r'(")\s*\n(\s*")', r'\1,\n\2', text)
    text = re.sub(r'(})\s*\n(\s*")', r'\1,\n\2', text)
    text = re.sub(r'(\])\s*\n(\s*")', r'\1,\n\2', text)
    text = re.sub(r'(true|false|null|\d)\s*\n(\s*")', r'\1,\n\2', text)
    text = re.sub(r'(})\s*\n(\s*\{)', r'\1,\n\2', text)

    # --- –û–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–π: "value" "key" ‚Üí "value", "key" ---
    # "..." "..."  (–¥–≤–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–¥—Ä—è–¥ –±–µ–∑ –∑–∞–ø—è—Ç–æ–π)
    text = re.sub(r'(") (")', r'\1, \2', text)
    # } "key"  ‚Üí  }, "key"
    text = re.sub(r'(}) (")', r'\1, \2', text)
    # ] "key"  ‚Üí  ], "key"
    text = re.sub(r'(\]) (")', r'\1, \2', text)
    # true/false/null/number  "key"
    text = re.sub(r'(true|false|null)(\s+)(")', r'\1,\2\3', text)
    text = re.sub(r'(\d)(\s+)(")', r'\1,\2\3', text)
    # } {  ‚Üí  }, {  (–º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤)
    text = re.sub(r'(})\s*(\{)', r'\1, \2', text)

    # --- Trailing commas ---
    text = re.sub(r',\s*}', '}', text)
    text = re.sub(r',\s*]', ']', text)
    return text


def _balance_brackets(text):
    """–î–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏ –≤ JSON.
    GigaChat —á–∞—Å—Ç–æ –∑–∞–±—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ } –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞.
    """
    in_string = False
    escape = False
    opens = []
    match = {'{': '}', '[': ']'}
    for ch in text:
        if escape:
            escape = False
            continue
        if ch == '\\' and in_string:
            escape = True
            continue
        if ch == '"' and not escape:
            in_string = not in_string
            continue
        if in_string:
            continue
        if ch in ('{', '['):
            opens.append(ch)
        elif ch in ('}', ']'):
            if opens:
                opens.pop()
    closing = ''.join(match[o] for o in reversed(opens))
    if closing:
        text = text.rstrip()
        if text.endswith(','):
            text = text[:-1]
        text += closing
    return text


def _parse_json_safe(text):
    """–†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM.
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç: markdown-–æ–±—ë—Ä—Ç–∫–∏, —Ç–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏–µ –∫–∞–≤—ã—á–∫–∏,
    –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ, –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ–±–∫–∏.
    """
    import json as _json
    import re

    text = text.strip()

    # –£–±–∏—Ä–∞–µ–º markdown
    if "```" in text:
        parts = text.split("```")
        for part in parts:
            part = part.strip()
            if part.startswith("json"):
                part = part[4:].strip()
            if part.startswith("{"):
                text = part
                break

    # –¢–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏–µ –∫–∞–≤—ã—á–∫–∏
    for old, new in [('\u201c', '"'), ('\u201d', '"'), ('\u00ab', '"'), ('\u00bb', '"'),
                     ('\u2018', "'"), ('\u2019', "'")]:
        text = text.replace(old, new)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON-–±–ª–æ–∫
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        text = match.group()

    # –ü–æ–ø—ã—Ç–∫–∞ 1: –∫–∞–∫ –µ—Å—Ç—å
    try:
        return _json.loads(text)
    except _json.JSONDecodeError:
        pass

    # –ü–æ–ø—ã—Ç–∫–∞ 2: —á–∏–Ω–∏–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ
    fixed = _fix_missing_commas(text)
    try:
        return _json.loads(fixed)
    except _json.JSONDecodeError:
        pass

    # –ü–æ–ø—ã—Ç–∫–∞ 3: –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å–∫–æ–±–æ–∫ (GigaChat —á–∞—Å—Ç–æ –∑–∞–±—ã–≤–∞–µ—Ç –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ })
    balanced = _balance_brackets(fixed)
    try:
        return _json.loads(balanced)
    except _json.JSONDecodeError:
        pass

    # –ü–æ–ø—ã—Ç–∫–∞ 4: –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —á–∏—Å—Ç–∫–∞ ‚Äî —É–±–∏—Ä–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    cleaned = re.sub(r'[\x00-\x1f]+', ' ', balanced)
    for old, new in [('\u2014', '-'), ('\u2013', '-'), ('\u2026', '...')]:
        cleaned = cleaned.replace(old, new)
    try:
        return _json.loads(cleaned)
    except _json.JSONDecodeError as e:
        raise e


def suggest_joins_via_llm(llm, table_name, columns, detected_joins, all_tables):
    """
    –ü–æ–ø—Ä–æ—Å–∏—Ç—å GigaChat –¥–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –¥–∂–æ–π–Ω–æ–≤
    –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤—è–∑–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
    """
    if not detected_joins and not columns:
        return {}

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π
    join_lines = []
    for j in detected_joins:
        join_lines.append(f"{j['column']} ‚Üí {j['foreign_table']} (–∞–ª–∏–∞—Å: {j['alias']})")
    detected_text = "–°–≤—è–∑–∏:\n" + "\n".join(f"  - {l}" for l in join_lines)

    # –ö–æ–ª–æ–Ω–∫–∏ —Å _id –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–ª–∏ –≤ detected
    detected_cols = {j["column"] for j in detected_joins}
    unmatched_id_cols = [
        c["name"] for c in columns
        if c["name"].endswith("_id") and c["name"] != "id" and c["name"] not in detected_cols
    ]

    unmatched_text = ""
    if unmatched_id_cols:
        unmatched_text = f"\n–ù–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(unmatched_id_cols)}"

    prompt = f"""–î–ª—è –∫–∞–∂–¥–æ–π —Å–≤—è–∑–∏ —Ç–∞–±–ª–∏—Ü—ã {table_name} –¥–∞–π title (1-2 —Å–ª–æ–≤–∞) –∏ description (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ) –Ω–∞ —Ä—É—Å—Å–∫–æ–º.

{detected_text}{unmatched_text}

–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{"joins": [{{"column": "col_id", "title": "–ù–∞–∑–≤–∞–Ω–∏–µ", "description": "–û–ø–∏—Å–∞–Ω–∏–µ"}}], "extra_joins": []}}"""

    try:
        response = _llm_invoke_with_retry(llm, prompt)
        return _parse_json_safe(response.content)
    except Exception as e:
        print(f"  ‚ö†Ô∏è GigaChat –Ω–µ —Å–º–æ–≥ –æ–ø–∏—Å–∞—Ç—å —Å–≤—è–∑–∏ {table_name}: {e}")
        return {}


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ GigaChat
# ============================================================

def _llm_invoke_with_retry(llm, prompt, max_retries=3):
    """–í—ã–∑–æ–≤ LLM —Å retry –ø—Ä–∏ rate-limit (429) –∏ —Ç–∞–π–º–∞—É—Ç–∞—Ö."""
    import time as _time
    for attempt in range(max_retries):
        try:
            return llm.invoke(prompt)
        except Exception as e:
            err_str = str(e)
            if "429" in err_str or "Too Many Requests" in err_str or "timeout" in err_str.lower():
                wait = 5 * (attempt + 1)
                print(f"  ‚è≥ Rate limit / timeout, –∂–¥—É {wait}—Å (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries})...")
                _time.sleep(wait)
            else:
                raise
    raise RuntimeError(f"LLM –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")


def _analyze_sample_data(sample_columns, sample_rows, columns):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ sample-–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: col_name ‚Üí {unique_values, null_count, value_type_hint}.
    """
    if not sample_rows or not sample_columns:
        return {}

    analysis = {}
    col_types = {c["name"]: c["data_type"] for c in columns}

    for idx, col_name in enumerate(sample_columns):
        values = [row[idx] for row in sample_rows]
        non_null = [v for v in values if v is not None]
        null_count = len(values) - len(non_null)

        info = {"null_count": null_count, "total": len(values)}

        if not non_null:
            info["hint"] = "–≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è NULL"
            analysis[col_name] = info
            continue

        unique = set()
        for v in non_null:
            s = str(v).strip()
            if len(s) <= 100:
                unique.add(s)

        if len(unique) <= 10 and col_types.get(col_name, "") in (
            "character varying", "text", "varchar", "USER-DEFINED"
        ):
            info["unique_values"] = sorted(unique)
            info["hint"] = f"–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ: {', '.join(sorted(unique))}"
        elif len(unique) <= 5:
            info["unique_values"] = sorted(unique)
            info["hint"] = f"–ø—Ä–∏–º–µ—Ä—ã: {', '.join(sorted(unique)[:5])}"
        else:
            samples = [str(v)[:60] for v in non_null[:3]]
            info["hint"] = f"–ø—Ä–∏–º–µ—Ä—ã: {', '.join(samples)}"

        if null_count > 0:
            pct = int(null_count / len(values) * 100)
            info["hint"] += f" ({pct}% NULL)"

        analysis[col_name] = info

    return analysis


def generate_descriptions(llm, table_name, columns, fks, sample_columns,
                          sample_rows, row_count, etl_context=None):
    """
    GigaChat: –æ–ø–∏—Å–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –∏ –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ ETL-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    # –ê–Ω–∞–ª–∏–∑ sample data
    data_analysis = _analyze_sample_data(sample_columns, sample_rows, columns)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º
    col_lines = []
    for c in columns:
        line = f"  - {c['name']} ({c['data_type']})"
        hint = data_analysis.get(c["name"], {}).get("hint")
        if hint:
            line += f"  // {hint}"
        col_lines.append(line)
    columns_text = "\n".join(col_lines)

    # –ü—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–æ–∫ (–∫–æ–º–ø–∞–∫—Ç–Ω–æ)
    sample_text = ""
    if sample_rows:
        sample_text = "\n–ü—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–æ–∫:\n"
        for row in sample_rows[:3]:
            parts = []
            for col, val in zip(sample_columns, row):
                val_str = str(val)[:50] if val is not None else "NULL"
                parts.append(f"{col}={val_str}")
            sample_text += "  " + ", ".join(parts[:10]) + "\n"

    # FK-–∫–æ–Ω—Ç–µ–∫—Å—Ç
    fk_text = ""
    if fks:
        fk_lines = [f"  - {f['column']} ‚Üí {f['foreign_table']}.{f['foreign_column']}" for f in fks]
        fk_text = "\n–í–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏:\n" + "\n".join(fk_lines)

    # ETL-–∫–æ–Ω—Ç–µ–∫—Å—Ç
    etl_text = ""
    if etl_context:
        parts = []
        if etl_context.get("process_description"):
            parts.append(f"–ü—Ä–æ—Ü–µ—Å—Å: {etl_context['process_description']}")
        if etl_context.get("source_schema"):
            parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {etl_context['source_schema']}")
        if etl_context.get("target_table"):
            parts.append(f"–¶–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ ETL: {etl_context['target_table']}")
        if parts:
            etl_text = "\nETL-–∫–æ–Ω—Ç–µ–∫—Å—Ç: " + "; ".join(parts)

    prompt = f"""–û–ø–∏—à–∏ —Ç–∞–±–ª–∏—Ü—É {table_name} ({row_count} —Å—Ç—Ä–æ–∫) –Ω–∞ —Ä—É—Å—Å–∫–æ–º.

–ö–æ–ª–æ–Ω–∫–∏ (–ø–æ—Å–ª–µ // ‚Äî —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö):
{columns_text}
{sample_text}{fk_text}{etl_text}
–í–ê–ñ–ù–û: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–æ–∫ –∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ //).
–î–ª—è –∫–æ–ª–æ–Ω–æ–∫-–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π (status, type, priority –∏ –ø—Ä.) –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—á–∏—Å–ª–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ description.
–î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ —É–∫–∞–∂–∏ –µ–¥–∏–Ω–∏—Ü—É –∏–∑–º–µ—Ä–µ–Ω–∏—è –µ—Å–ª–∏ –º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑ –¥–∞–Ω–Ω—ã—Ö.

–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ JSON:
{{"table_title": "–†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (2-3 —Å–ª–æ–≤–∞)", "table_description": "–û–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)", "columns": {{"col_name": {{"title": "–ù–∞–∑–≤–∞–Ω–∏–µ", "description": "–ß—Ç–æ —Ö—Ä–∞–Ω–∏—Ç (—Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∑–Ω–∞—á–µ–Ω–∏–π)"}}}}}}"""

    # –ü–æ–ø—ã—Ç–∫–∞ 1
    try:
        response = _llm_invoke_with_retry(llm, prompt)
        return _parse_json_safe(response.content)
    except Exception:
        pass

    # –ü–æ–ø—ã—Ç–∫–∞ 2: —Å–æ–∫—Ä–∞—â—ë–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º
    col_hints = []
    for c in columns:
        hint = data_analysis.get(c["name"], {}).get("hint", "")
        col_hints.append(f"{c['name']}({c['data_type']}){': '+hint if hint else ''}")
    short_cols = "; ".join(col_hints[:20])

    prompt2 = f"""–¢–∞–±–ª–∏—Ü–∞ {table_name} ({row_count} —Å—Ç—Ä–æ–∫). –ö–æ–ª–æ–Ω–∫–∏ –∏ –¥–∞–Ω–Ω—ã–µ: {short_cols}.
{etl_text}
–î–∞–π table_title (2 —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º) –∏ table_description (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ).
–î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏ –¥–∞–π title (1-2 —Å–ª–æ–≤–∞) –∏ description (–∫—Ä–∞—Ç–∫–æ, –≤–∫–ª—é—á–∞—è –ø—Ä–∏–º–µ—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π).
–û—Ç–≤–µ—Ç: JSON {{"table_title":"...", "table_description":"...", "columns":{{"col":{{"title":"...", "description":"..."}}}}}}"""

    try:
        response = _llm_invoke_with_retry(llm, prompt2)
        return _parse_json_safe(response.content)
    except Exception as e:
        print(f"  ‚ö†Ô∏è GigaChat –Ω–µ —Å–º–æ–≥ –æ–ø–∏—Å–∞—Ç—å {table_name}: {e}")

    # Fallback ‚Äî –æ–ø–∏—Å–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ LLM)
    result = {
        "table_title": table_name.replace("_", " ").title(),
        "table_description": f"–¢–∞–±–ª–∏—Ü–∞ {table_name}",
        "columns": {}
    }
    for c in columns:
        col_name = c["name"]
        hint = data_analysis.get(col_name, {}).get("hint", "")
        desc_text = hint if hint else f"{col_name} ({c['data_type']})"
        result["columns"][col_name] = {
            "title": col_name.replace("_", " ").replace("id", "").strip().title() or col_name,
            "description": desc_text
        }
    return result


# ============================================================
# –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ PostgreSQL ‚Üí Cube.js
# ============================================================

def pg_type_to_cube(pg_type, column_name):
    """–°–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–∏–ø PostgreSQL —Å —Ç–∏–ø–æ–º Cube.js"""
    pg_type = pg_type.lower()
    
    # –í—Ä–µ–º—è
    if any(t in pg_type for t in ["timestamp", "date", "time"]):
        return "time"
    
    # –ß–∏—Å–ª–∞
    if any(t in pg_type for t in ["integer", "int", "bigint", "smallint", "serial",
                                    "numeric", "decimal", "real", "double", "float"]):
        return "number"
    
    # –ë—É–ª–µ–≤—ã
    if "boolean" in pg_type:
        return "boolean"
    
    # –°—Ç—Ä–æ–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    return "string"


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Cube YAML
# ============================================================

def generate_cube_yaml(table_name, columns, enriched_joins, pk, descriptions,
                       schema="public", etl_context=None):
    """
    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å YAML-–º–æ–¥–µ–ª—å Cube –¥–ª—è –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã.
    enriched_joins ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç build_all_relationships + –æ–ø–∏—Å–∞–Ω–∏—è –æ—Ç LLM.
    etl_context ‚Äî –¥–∞–Ω–Ω—ã–µ –∏–∑ ETL execution plan –¥–ª—è —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü—ã.
    """
    
    desc = descriptions
    cube_name = table_name
    
    table_desc = desc.get("table_description", "")
    if etl_context:
        etl_parts = []
        if etl_context.get("process_description"):
            etl_parts.append(etl_context["process_description"])
        if etl_context.get("source_schema"):
            etl_parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {etl_context['source_schema']}")
        if etl_context.get("target_table"):
            etl_parts.append(f"ETL —Ü–µ–ª–µ–≤–∞—è: {etl_context['target_table']}")
        if etl_parts:
            etl_suffix = " | " + "; ".join(etl_parts)
            table_desc = table_desc.rstrip(".") + etl_suffix

    cube = {
        "name": cube_name,
        "sql_table": f"{schema}.{table_name}",
        "title": desc.get("table_title", table_name),
        "description": table_desc,
    }
    
    # --- Joins (–æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–µ: FK + implicit + LLM) ---
    joins = []
    join_columns = set()  # –∫–æ–ª–æ–Ω–∫–∏, –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ –≤ join
    
    for j in enriched_joins:
        alias = j["alias"]
        col = j["column"]
        foreign_col = j.get("foreign_column", "id")
        join_columns.add(col)
        
        join_entry = {
            "name": alias,
            "sql": "{CUBE}." + col + " = {" + alias + "}." + foreign_col,
            "relationship": j.get("relationship", "many_to_one"),
        }
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if j.get("title"):
            join_entry["title"] = j["title"]
        if j.get("description"):
            join_entry["description"] = j["description"]
        
        joins.append(join_entry)
    
    if joins:
        cube["joins"] = joins
    
    # --- Dimensions ---
    dimensions = []
    col_descs = desc.get("columns", {})
    
    for c in columns:
        col_name = c["name"]
        cube_type = pg_type_to_cube(c["data_type"], col_name)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º FK-–∫–æ–ª–æ–Ω–∫–∏ (–æ–Ω–∏ —É—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ join)
        if col_name in join_columns and col_name != pk:
            continue
        
        dim = {
            "name": col_name,
            "sql": col_name,
            "type": cube_type,
        }
        
        if col_name == pk:
            dim["primary_key"] = True
        
        col_desc = col_descs.get(col_name, {})
        if col_desc.get("title"):
            dim["title"] = col_desc["title"]
        if col_desc.get("description"):
            dim["description"] = col_desc["description"]
        
        dimensions.append(dim)
    
    cube["dimensions"] = dimensions
    
    # --- Measures ---
    measures = [
        {
            "name": "count",
            "type": "count",
            "title": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            "description": f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ {desc.get('table_title', table_name)}"
        }
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º sum/avg –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–Ω–µ ID –∏ –Ω–µ FK)
    for c in columns:
        col_name = c["name"]
        if col_name.endswith("_id") or col_name == pk:
            continue
        if pg_type_to_cube(c["data_type"], col_name) == "number":
            col_title = col_descs.get(col_name, {}).get("title", col_name)
            measures.append({
                "name": f"total_{col_name}",
                "sql": col_name,
                "type": "sum",
                "title": f"–°—É–º–º–∞ {col_title}",
                "description": f"–°—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ª—è {col_name}"
            })
            measures.append({
                "name": f"avg_{col_name}",
                "sql": col_name,
                "type": "avg",
                "title": f"–°—Ä–µ–¥–Ω–µ–µ {col_title}",
                "description": f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è {col_name}"
            })
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ä—ã –∏–∑ Knowledge Base
    jira_measures = get_kb_suggested_measures(table_name)
    existing_names = {m["name"] for m in measures}
    for jm in jira_measures:
        if jm["name"] not in existing_names:
            measures.append(jm)

    cube["measures"] = measures
    
    return {"cubes": [cube]}


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è semantic-–∫–æ–Ω—Ñ–∏–≥–æ–≤ (glossary, examples)
# ============================================================

def generate_glossary(all_tables_info):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–π glossary.yml"""
    glossary = {}
    
    for info in all_tables_info:
        table = info["table_name"]
        desc = info["descriptions"]
        cube_name = table
        
        # –¢–µ—Ä–º–∏–Ω –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        title = desc.get("table_title", table)
        glossary[table] = {
            "aliases": [title.lower(), table, table.replace("_", " ")],
            "semantic_type": "entity",
            "fields": [f"{cube_name}.id"],
            "filter_operator": "equals",
            "description": desc.get("table_description", "")
        }
        
        # –¢–µ—Ä–º–∏–Ω count –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        glossary[f"{table}_count"] = {
            "aliases": [
                f"–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ {title.lower()}",
                f"—Å–∫–æ–ª—å–∫–æ {title.lower()}",
            ],
            "semantic_type": "metric",
            "measures": [f"{cube_name}.count"],
            "description": f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π {title}"
        }
    
    return glossary


def generate_examples(all_tables_info):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–π examples.yml"""
    examples = []
    
    for info in all_tables_info:
        table = info["table_name"]
        desc = info["descriptions"]
        cube_name = table
        title = desc.get("table_title", table)
        
        # –ü—Ä–∏–º–µ—Ä: "—Å–∫–æ–ª—å–∫–æ <—Å—É—â–Ω–æ—Å—Ç–µ–π>"
        examples.append({
            "question": f"—Å–∫–æ–ª—å–∫–æ {title.lower()}",
            "intent": "analytics",
            "query": {
                "measures": [f"{cube_name}.count"],
                "limit": 100
            },
            "tags": ["count", table]
        })
        
        # –ü—Ä–∏–º–µ—Ä: "—Å–ø–∏—Å–æ–∫ <—Å—É—â–Ω–æ—Å—Ç–µ–π>"
        dims = []
        for c in info["columns"][:5]:
            if c["name"] != "id" and not c["name"].endswith("_id"):
                dims.append(f"{cube_name}.{c['name']}")
        
        if dims:
            examples.append({
                "question": f"—Å–ø–∏—Å–æ–∫ {title.lower()}",
                "intent": "analytics",
                "query": {
                    "measures": [f"{cube_name}.count"],
                    "dimensions": dims[:4],
                    "limit": 100
                },
                "tags": ["list", table]
            })
    
    return examples


# ============================================================
# –ü–∞—Ä—Å–∏–Ω–≥ Spark Execution Plan –∏–∑ ETL
# ============================================================

def _parse_spark_plan(process_description: str) -> dict:
    """–ò–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ Spark execution plan.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {source_tables, columns, joins, filters, target_table, target_columns}.
    """
    import re
    info = {
        "source_tables": [],
        "columns": [],
        "joins": [],
        "filters": [],
        "target_table": None,
        "target_columns": [],
    }
    if not process_description:
        return info

    text = process_description

    # –¶–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∏ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ INSERT
    m = re.search(r'InsertIntoHiveTable\s+`([^`]+)`\.`([^`]+)`.*?\[([^\]]*)\]', text)
    if m:
        info["target_table"] = f"{m.group(1)}.{m.group(2)}"
        cols_str = m.group(3)
        info["target_columns"] = [c.strip().split("=")[0].strip()
                                  for c in cols_str.split(",") if c.strip()]

    # CreateDataSourceTableAsSelectCommand
    m2 = re.search(r'CreateDataSourceTableAsSelectCommand\s+`([^`]+)`\.`([^`]+)`.*?\[([^\]]*)\]', text)
    if m2:
        info["target_table"] = f"{m2.group(1)}.{m2.group(2)}"
        info["target_columns"] = [c.strip() for c in m2.group(3).split(",") if c.strip()]

    # Scan hive ‚Üí –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
    for m in re.finditer(r'Scan hive\s+([^\s\[]+)\s*\[([^\]]*)\]', text):
        full_table = m.group(1)
        scan_cols = [c.strip().split("#")[0].strip() for c in m.group(2).split(",") if c.strip()]
        info["source_tables"].append({"table": full_table, "columns": scan_cols})
        info["columns"].extend(scan_cols)

    # Join-–ø–∞—Ç—Ç–µ—Ä–Ω—ã
    for m in re.finditer(r'(BroadcastHashJoin|SortMergeJoin)\s*\[([^\]]*)\],\s*\[([^\]]*)\]', text):
        left_col = m.group(2).strip().split("#")[0].strip()
        right_col = m.group(3).strip().split("#")[0].strip()
        info["joins"].append({"left": left_col, "right": right_col, "type": m.group(1)})

    # –§–∏–ª—å—Ç—Ä—ã
    for m in re.finditer(r'Filter\s*\((.+?)\)\s*$', text, re.MULTILINE):
        filt = m.group(1).strip()
        if len(filt) < 200:
            info["filters"].append(filt)

    info["columns"] = sorted(set(info["columns"]))
    return info


def _format_etl_summary(etl_entry: dict, parsed_plan: dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å ETL-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –º–æ–¥–µ–ª–∏."""
    parts = []

    if parsed_plan.get("target_table"):
        parts.append(f"ETL —Ü–µ–ª–µ–≤–∞—è: {parsed_plan['target_table']}")
    if parsed_plan.get("target_columns"):
        parts.append(f"–ö–æ–ª–æ–Ω–∫–∏: {', '.join(parsed_plan['target_columns'][:15])}")
    if parsed_plan.get("source_tables"):
        src_names = list({s["table"].split(".")[-1] for s in parsed_plan["source_tables"]})
        parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(src_names[:10])}")
    if parsed_plan.get("joins"):
        join_descs = [f"{j['left']}‚Üî{j['right']}" for j in parsed_plan["joins"][:5]]
        parts.append(f"–°–≤—è–∑–∏: {', '.join(join_descs)}")

    if etl_entry.get("source_schema"):
        parts.append(f"–°—Ö–µ–º–∞: {etl_entry['source_schema']}")
    if etl_entry.get("last_updated"):
        parts.append(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ: {etl_entry['last_updated']}")

    return " | ".join(parts)


# ============================================================
# –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ ETL plan
# ============================================================

def enrich_models_with_etl(model_dir: str, etl_plan: dict, llm=None,
                           data_source=None, kb_path: str = None) -> dict:
    """–û–±–æ–≥–∞—Ç–∏—Ç—å —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Cube YAML-–º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ ETL plan.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      model_dir   ‚Äî –ø–∞–ø–∫–∞ —Å .yml —Ñ–∞–π–ª–∞–º–∏ –º–æ–¥–µ–ª–µ–π Cube
      etl_plan    ‚Äî dict –∏–∑ load_etl_plan()
      llm         ‚Äî GigaChat (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –ø–µ—Ä–µ–æ–ø–∏—Å–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫)
      data_source ‚Äî –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è sample data)
      kb_path     ‚Äî –ø—É—Ç—å –∫ KB (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {updated: [...], skipped: [...], errors: [...]}
    """
    model_path = Path(model_dir)
    if not model_path.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_dir}")
        return {"updated": [], "skipped": [], "errors": []}

    if kb_path and Path(kb_path).exists():
        load_knowledge_base(kb_path)

    # –ü–∞—Ä—Å–∏–º –≤—Å–µ ETL-–∑–∞–ø–∏—Å–∏ –∑–∞—Ä–∞–Ω–µ–µ
    etl_parsed = {}
    for src_table, entry in etl_plan.items():
        parsed = _parse_spark_plan(entry.get("process_description", ""))
        etl_parsed[src_table] = {"entry": entry, "parsed": parsed}

    yml_files = sorted(model_path.glob("*.yml"))
    print(f"\nüìÇ –ú–æ–¥–µ–ª–µ–π –≤ {model_dir}: {len(yml_files)}")
    print(f"üìã –ó–∞–ø–∏—Å–µ–π –≤ ETL plan: {len(etl_plan)}")
    print()

    results = {"updated": [], "skipped": [], "errors": []}

    for yml_file in yml_files:
        cube_name = yml_file.stem
        try:
            with open(yml_file, 'r', encoding='utf-8') as f:
                model = yaml.safe_load(f)
        except Exception as e:
            results["errors"].append(f"{cube_name}: –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è ‚Äî {e}")
            continue

        if not model or "cubes" not in model or not model["cubes"]:
            results["skipped"].append(f"{cube_name}: –Ω–µ—Ç –±–ª–æ–∫–∞ cubes")
            continue

        cube = model["cubes"][0]

        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å ETL plan
        matched_key = None
        matched_data = None
        cube_norm = cube_name.lower().replace("_", "")
        cube_singulars = _singularize(cube_norm)

        for src_table, data in etl_parsed.items():
            src_norm = src_table.lower().replace("_", "")
            src_singulars = _singularize(src_norm)

            if cube_singulars & src_singulars:
                matched_key = src_table
                matched_data = data
                break

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
            target = data["parsed"].get("target_table", "")
            if target:
                target_short = target.split(".")[-1].lower().replace("_", "")
                target_singulars = _singularize(target_short)
                if cube_singulars & target_singulars:
                    matched_key = src_table
                    matched_data = data
                    break

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –ø–ª–∞–Ω–µ
            for st in data["parsed"].get("source_tables", []):
                st_short = st["table"].split(".")[-1].lower().replace("_", "")
                st_singulars = _singularize(st_short)
                if cube_singulars & st_singulars:
                    matched_key = src_table
                    matched_data = data
                    break
            if matched_key:
                break

        if not matched_data:
            results["skipped"].append(cube_name)
            continue

        print(f"üîó {cube_name} ‚Üê ETL: {matched_key}")

        entry = matched_data["entry"]
        parsed = matched_data["parsed"]

        etl_summary = _format_etl_summary(entry, parsed)
        changes = []

        # 1. –û–±–æ–≥–∞—â–∞–µ–º description –∫—É–±–∞
        old_desc = cube.get("description", "")
        if "ETL" not in old_desc and etl_summary:
            cube["description"] = (old_desc.rstrip(". ") + " | " + etl_summary) if old_desc else etl_summary
            changes.append("description")

        # 2. –û–±–æ–≥–∞—â–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ parsed plan
        if parsed.get("target_columns"):
            dim_names = {d["name"] for d in cube.get("dimensions", [])}
            etl_cols = set(parsed["target_columns"])
            new_cols_in_etl = etl_cols - dim_names
            if new_cols_in_etl:
                changes.append(f"ETL-–∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –≤ –º–æ–¥–µ–ª–∏: {', '.join(sorted(new_cols_in_etl))}")

        # 3. –ï—Å–ª–∏ –µ—Å—Ç—å LLM + data_source ‚Äî –ø–µ—Ä–µ–æ–ø–∏—Å—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å ETL-–∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        if llm and data_source:
            try:
                columns = data_source.get_columns(cube_name)
                fks = data_source.get_foreign_keys(cube_name)
                row_count = data_source.get_row_count(cube_name)
                scols, srows = data_source.get_sample_data(cube_name, 10)

                new_desc = generate_descriptions(
                    llm, cube_name, columns, fks, scols, srows, row_count,
                    etl_context=entry
                )

                # –û–±–Ω–æ–≤–ª—è–µ–º title/description –µ—Å–ª–∏ GigaChat –¥–∞–ª –ª—É—á—à–µ
                if new_desc.get("table_title") and len(new_desc["table_title"]) > len(cube.get("title", "")):
                    cube["title"] = new_desc["table_title"]
                    changes.append("title (GigaChat+ETL)")

                if new_desc.get("table_description") and len(new_desc["table_description"]) > 20:
                    cube["description"] = new_desc["table_description"]
                    if etl_summary and "ETL" not in cube["description"]:
                        cube["description"] += " | " + etl_summary
                    changes.append("description (GigaChat+ETL)")

                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏—è dimensions
                col_descs = new_desc.get("columns", {})
                for dim in cube.get("dimensions", []):
                    dname = dim["name"]
                    new_col = col_descs.get(dname, {})
                    if new_col.get("title") and dim.get("title", "") in (dname, dname.replace("_", " ").title(), ""):
                        dim["title"] = new_col["title"]
                    old_dim_desc = dim.get("description", "")
                    if new_col.get("description") and (
                        not old_dim_desc or old_dim_desc.endswith(")") or len(old_dim_desc) < 15
                    ):
                        dim["description"] = new_col["description"]

                changes.append("dimensions (GigaChat+ETL)")

            except Exception as e:
                changes.append(f"‚ö†Ô∏è GigaChat: {e}")

        # 4. –û–±–æ–≥–∞—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ Knowledge Base
        if _KNOWLEDGE_BASE:
            kb_hints = match_kb_hints(cube_name)
            if kb_hints:
                col_hints = kb_hints.get("column_hints", {})
                for dim in cube.get("dimensions", []):
                    dname = dim["name"]
                    if dname in col_hints:
                        old_dim_desc = dim.get("description", "")
                        if not old_dim_desc or old_dim_desc.endswith(")") or len(old_dim_desc) < 15:
                            dim["description"] = col_hints[dname]

                # –î–æ–±–∞–≤–ª—è–µ–º suggested measures
                suggested = kb_hints.get("suggested_measures", [])
                existing_measure_names = {m["name"] for m in cube.get("measures", [])}
                for sm in suggested:
                    if sm["name"] not in existing_measure_names:
                        cube.setdefault("measures", []).append(sm)
                        changes.append(f"measure: {sm['name']}")

        if changes:
            model["cubes"][0] = cube
            with open(yml_file, 'w', encoding='utf-8') as f:
                yaml.dump(model, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
            results["updated"].append(f"{cube_name}: {', '.join(changes)}")
            print(f"   ‚úÖ {', '.join(changes)}")
        else:
            results["skipped"].append(cube_name)
            print(f"   ‚è≠Ô∏è  –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π")

    print(f"\n{'='*60}")
    print(f"  –û–±–Ω–æ–≤–ª–µ–Ω–æ: {len(results['updated'])}")
    print(f"  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(results['skipped'])}")
    print(f"  –û—à–∏–±–æ–∫:    {len(results['errors'])}")
    print(f"{'='*60}\n")

    return results


# ============================================================
# MAIN
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –≤ Cube")
    parser.add_argument("--source", choices=["postgresql", "greenplum", "hive", "duckdb", "cube"],
                        help="–ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å database.driver –∏–∑ config.yml")
    parser.add_argument("--kb", metavar="FILE",
                        help="–ü—É—Ç—å –∫ Knowledge Base YAML (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config.yml)")
    parser.add_argument("--etl-plan", metavar="FILE",
                        help="–ü—É—Ç—å –∫ ETL execution plan (xlsx/csv) –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
    parser.add_argument("--enrich-etl", action="store_true",
                        help="–û–±–æ–≥–∞—Ç–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ ETL plan (–±–µ–∑ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)")
    parser.add_argument("--enrich-with-llm", action="store_true",
                        help="–ü—Ä–∏ --enrich-etl –ø–µ—Ä–µ–æ–ø–∏—Å—ã–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ —á–µ—Ä–µ–∑ GigaChat + sample data")
    parser.add_argument("--model-dir", metavar="DIR",
                        help="–ü–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—è–º–∏ (–¥–ª—è --enrich-etl, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config.yml)")
    args = parser.parse_args()

    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥
    config = load_config()

    # ‚îÄ‚îÄ –†–µ–∂–∏–º: –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ ETL plan ‚îÄ‚îÄ
    if args.enrich_etl:
        print("=" * 60)
        print("  –û–ë–û–ì–ê–©–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ß–ï–†–ï–ó ETL PLAN")
        print("=" * 60)
        print()

        plan_file = args.etl_plan or config.get("etl_plan_path")
        if not plan_file:
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ ETL plan: --etl-plan <file.xlsx> –∏–ª–∏ etl_plan_path –≤ config.yml")
            sys.exit(1)
        if not Path(plan_file).exists():
            print(f"‚ùå ETL plan —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {plan_file}")
            sys.exit(1)

        etl_plan = load_etl_plan(plan_file)
        if not etl_plan:
            print("‚ùå ETL plan –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            sys.exit(1)

        model_dir = args.model_dir or config.get("cube", {}).get("model_path", "./cube_models")
        kb_path = args.kb or config.get("knowledge_base_path")

        llm = None
        data_source = None
        if args.enrich_with_llm:
            print("üîÑ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat...")
            llm = create_gigachat(config)
            print("‚úÖ GigaChat –≥–æ—Ç–æ–≤")
            data_source, _ = create_data_source(config, args.source)
            print(f"‚úÖ –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–∫–ª—é—á—ë–Ω")

        results = enrich_models_with_etl(
            model_dir=model_dir,
            etl_plan=etl_plan,
            llm=llm,
            data_source=data_source,
            kb_path=kb_path
        )

        if data_source:
            data_source.close()

        print("–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –ø–∞–ø–∫–µ –º–æ–¥–µ–ª–µ–π")
        print("  2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Cube: npx cubejs-server")
        print("  3. –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ FAISS: python 02_build_faiss.py")
        return

    # ‚îÄ‚îÄ –†–µ–∂–∏–º: –ø–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π ‚îÄ‚îÄ
    print("=" * 60)
    print("  –ó–ê–ì–†–£–ó–ß–ò–ö –î–ê–ù–ù–´–• –í CUBE")
    print("  –ò—Å—Ç–æ—á–Ω–∏–∫ ‚Üí –û–ø–∏—Å–∞–Ω–∏—è GigaChat ‚Üí YAML-–º–æ–¥–µ–ª–∏ Cube")
    print("=" * 60)
    print()
    
    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # 2. –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É –¥–∞–Ω–Ω—ã—Ö
    source, driver_name = create_data_source(config, args.source)
    schema = config.get("database", {}).get("schema", "public")
    print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {driver_name}, –°—Ö–µ–º–∞: {schema}")
    
    # 3. –ó–∞–≥—Ä—É–∑–∏—Ç—å Knowledge Base (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
    kb_path = args.kb or config.get("knowledge_base_path")
    if kb_path and Path(kb_path).exists():
        load_knowledge_base(kb_path)
    elif kb_path:
        print(f"‚ö†Ô∏è  KB —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {kb_path}")
    else:
        print("‚ÑπÔ∏è  Knowledge Base –Ω–µ —É–∫–∞–∑–∞–Ω–∞ (--kb –∏–ª–∏ knowledge_base_path –≤ config.yml)")

    # 4. –ó–∞–≥—Ä—É–∑–∏—Ç—å ETL plan (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
    etl_plan = {}
    plan_file = args.etl_plan or config.get("etl_plan_path")
    if plan_file and Path(plan_file).exists():
        etl_plan = load_etl_plan(plan_file)
    elif plan_file:
        print(f"‚ö†Ô∏è  ETL plan —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {plan_file}")

    # 5. –ü–æ–¥–∫–ª—é—á–∏—Ç—å GigaChat
    print("üîÑ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat...")
    llm = create_gigachat(config)
    print("‚úÖ GigaChat –≥–æ—Ç–æ–≤")
    
    # 6. –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü
    tables = source.get_tables()
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
    for t in tables:
        print(f"   - {t}")
    print()
    
    # 5. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—É—é —Ç–∞–±–ª–∏—Ü—É
    model_path = Path(config["cube"]["model_path"])
    model_path.mkdir(parents=True, exist_ok=True)
    
    all_tables_set = set(tables)
    all_tables_info = []
    
    for i, table in enumerate(tables, 1):
        print(f"[{i}/{len(tables)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: {table}")
        
        # –ß–∏—Ç–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —á–µ—Ä–µ–∑ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        columns = source.get_columns(table)
        fks = source.get_foreign_keys(table)
        pk = source.get_primary_key(table)
        row_count = source.get_row_count(table)
        sample_cols, sample_rows = source.get_sample_data(table, 5)
        
        print(f"   –ö–æ–ª–æ–Ω–æ–∫: {len(columns)}, FK: {len(fks)}, –°—Ç—Ä–æ–∫: {row_count}")
        
        # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏ (FK + implicit –ø–æ –∏–º–µ–Ω–∞–º)
        enriched_joins = build_all_relationships(table, columns, all_tables_set, fks)
        implicit_count = sum(1 for j in enriched_joins if j.get("source") == "implicit")
        if enriched_joins:
            print(f"   üîó –°–≤—è–∑–µ–π: {len(enriched_joins)} (FK: {len(fks)}, –ø–æ –∏–º–µ–Ω–∞–º: {implicit_count})")
            for j in enriched_joins:
                src = "FK" if j["source"] == "explicit" else "‚Üí"
                print(f"      {src} {j['column']} ‚Üí {j['foreign_table']} (as {j['alias']})")
        
        # –ü—Ä–æ—Å–∏–º GigaChat –æ–ø–∏—Å–∞—Ç—å —Å–≤—è–∑–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
        if enriched_joins:
            print(f"   ü§ñ GigaChat: –æ–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–µ–π...")
            join_suggestions = suggest_joins_via_llm(llm, table, columns, enriched_joins, all_tables_set)
            
            # –û–±–æ–≥–∞—â–∞–µ–º joins –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –æ—Ç LLM
            llm_joins_map = {}
            for lj in join_suggestions.get("joins", []):
                key = lj.get("column", "")
                llm_joins_map[key] = lj
            
            for j in enriched_joins:
                llm_info = llm_joins_map.get(j["column"], {})
                if llm_info.get("title"):
                    j["title"] = llm_info["title"]
                if llm_info.get("description"):
                    j["description"] = llm_info["description"]
                if llm_info.get("alias") and llm_info["alias"] != j["alias"]:
                    j["alias"] = llm_info["alias"]
            
            # –î–æ–±–∞–≤–ª—è–µ–º extra_joins –æ—Ç LLM
            for extra in join_suggestions.get("extra_joins", []):
                extra_table = extra.get("foreign_table", "")
                if extra_table in all_tables_set and extra_table != table:
                    col_names = {c["name"] for c in columns}
                    if extra.get("column") in col_names:
                        enriched_joins.append({
                            "column": extra["column"],
                            "foreign_table": extra_table,
                            "alias": extra.get("alias", extra_table),
                            "foreign_column": extra.get("foreign_column", "id"),
                            "relationship": "many_to_one",
                            "title": extra.get("title", ""),
                            "description": extra.get("description", ""),
                            "source": "llm"
                        })
                        print(f"      ‚ú® LLM –ø—Ä–µ–¥–ª–æ–∂–∏–ª: {extra['column']} ‚Üí {extra_table}")
        
        # ETL-–∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–π —Ç–∞–±–ª–∏—Ü—ã
        etl_context = None
        if etl_plan:
            for src_name, plan_info in etl_plan.items():
                src_norm = src_name.lower().replace("_", "")
                table_norm = table.lower().replace("_", "")
                if src_norm == table_norm or table_norm in src_norm or src_norm in table_norm:
                    etl_context = plan_info
                    print(f"   üìã ETL plan: —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å {src_name}")
                    break

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ GigaChat
        print(f"   ü§ñ GigaChat: –æ–ø–∏—Å–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –∏ –∫–æ–ª–æ–Ω–æ–∫...")
        descriptions = generate_descriptions(
            llm, table, columns, fks, sample_cols, sample_rows, row_count,
            etl_context=etl_context
        )

        # –û–±–æ–≥–∞—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ Knowledge Base
        kb_hints = match_kb_hints(table, etl_plan)
        if kb_hints:
            print(f"   üìö KB: {kb_hints.get('title', 'match found')}")
            descriptions = enrich_descriptions_with_kb(descriptions, table, columns, etl_plan)
            if not descriptions.get("table_description") or len(descriptions["table_description"]) < 10:
                descriptions["table_description"] = kb_hints.get("description", descriptions.get("table_description", ""))
            if not descriptions.get("table_title") or descriptions["table_title"] == table:
                descriptions["table_title"] = kb_hints.get("title", descriptions.get("table_title", table))

        print(f"   ‚úÖ –û–ø–∏—Å–∞–Ω–∏—è: {descriptions.get('table_title', '?')}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Cube YAML
        cube_schema = schema if driver_name != "duckdb" else "main"
        cube_yaml = generate_cube_yaml(table, columns, enriched_joins, pk, descriptions,
                                        cube_schema, etl_context)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        yaml_path = model_path / f"{table}.yml"
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(cube_yaml, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {yaml_path}")
        
        all_tables_info.append({
            "table_name": table,
            "columns": columns,
            "fks": fks,
            "enriched_joins": enriched_joins,
            "descriptions": descriptions
        })
        print()
    
    # 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º semantic-–∫–æ–Ω—Ñ–∏–≥–∏
    config_path = Path("config")
    config_path.mkdir(exist_ok=True)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è glossary.yml...")
    glossary = generate_glossary(all_tables_info)
    with open(config_path / "glossary.yml", 'w', encoding='utf-8') as f:
        yaml.dump(glossary, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è examples.yml...")
    examples = generate_examples(all_tables_info)
    with open(config_path / "examples.yml", 'w', encoding='utf-8') as f:
        yaml.dump(examples, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è semantic_layer.yml...")
    layer_config = {
        "cube": {
            "base_url": config["cube"]["api_url"],
            "enabled": True,
            "preferred_cubes": [t["table_name"] for t in all_tables_info]
        },
        "intents": {
            "analytics": {
                "description": "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ Cube",
                "keywords": ["—Å–∫–æ–ª—å–∫–æ", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–ø–æ–∫–∞–∂–∏", "—Å–ø–∏—Å–æ–∫",
                             "—Å—Ä–µ–¥–Ω–∏–π", "—Å—É–º–º–∞", "—Ç–æ–ø", "–≤—Å–µ–≥–æ", "–ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º"],
                "priority": 1
            }
        },
        "query_generation": {
            "default_limit": 100,
            "max_limit": 10000
        }
    }
    with open(config_path / "semantic_layer.yml", 'w', encoding='utf-8') as f:
        yaml.dump(layer_config, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    source.close()
    
    # –°–≤–æ–¥–∫–∞ –ø–æ —Å–≤—è–∑—è–º
    total_joins = sum(len(info.get("enriched_joins", [])) for info in all_tables_info)
    fk_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "explicit")
        for info in all_tables_info
    )
    implicit_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "implicit")
        for info in all_tables_info
    )
    llm_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "llm")
        for info in all_tables_info
    )
    
    print()
    print("=" * 60)
    print("  ‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 60)
    print(f"""
–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: {driver_name}
–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:
  üìÅ {model_path}/          ‚Äî YAML-–º–æ–¥–µ–ª–∏ Cube ({len(tables)} —Ñ–∞–π–ª–æ–≤)
  üìÅ config/glossary.yml    ‚Äî –ë–∏–∑–Ω–µ—Å-–≥–ª–æ—Å—Å–∞—Ä–∏–π
  üìÅ config/examples.yml    ‚Äî –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
  üìÅ config/semantic_layer.yml ‚Äî –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è

–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ (joins):
  üîó –í—Å–µ–≥–æ: {total_joins}
     - –ò–∑ FK constraints: {fk_joins}
     - –ü–æ –∏–º–µ–Ω–∞–º –∫–æ–ª–æ–Ω–æ–∫: {implicit_joins}
     - –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–æ GigaChat: {llm_joins}

–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
  1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –∏–∑ {model_path}/ –≤ –ø–∞–ø–∫—É model/cubes/ –≤–∞—à–µ–≥–æ Cube-–ø—Ä–æ–µ–∫—Ç–∞
  2. –ü–†–û–í–ï–†–¨–¢–ï –°–í–Ø–ó–ò: –æ—Ç–∫—Ä–æ–π—Ç–µ YAML-—Ñ–∞–π–ª—ã –∏ —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ joins –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
  3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Cube: npx cubejs-server
  4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: curl http://localhost:4000/cubejs-api/v1/meta
  5. –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ glossary.yml –∏ examples.yml –ø–æ–¥ –≤–∞—à–∏ –∑–∞–¥–∞—á–∏
  6. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 02_build_faiss.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
  7. –û—Ç–∫—Ä–æ–π—Ç–µ 03_agent.ipynb –≤ JupyterLab –∏ –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã
""")


if __name__ == "__main__":
    main()
