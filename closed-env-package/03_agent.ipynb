{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ê–≥–µ–Ω—Ç ‚Äî –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ Cube\n",
        "\n",
        "–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –∏ –ø–æ–ª—É—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Cube.\n",
        "\n",
        "**–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:**\n",
        "1. –ó–∞–ø–æ–ª–Ω–µ–Ω `config.yml`\n",
        "2. –ó–∞–ø—É—â–µ–Ω Cube (`npx cubejs-server`)\n",
        "3. –í—ã–ø–æ–ª–Ω–µ–Ω `python 01_data_loader.py` (–º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Cube)\n",
        "4. –í—ã–ø–æ–ª–Ω–µ–Ω `python 02_build_faiss.py` (FAISS-–∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω)\n",
        "\n",
        "> –ü–µ—Ä–≤–∞—è —è—á–µ–π–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (FAISS, sentence-transformers, –∏ —Ç.–¥.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# –ê–í–¢–û–£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô (–≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–µ—Ä–≤–æ–π!)\n",
        "# ============================================================\n",
        "# –ï—Å–ª–∏ –ø–∞–∫–µ—Ç—ã —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã ‚Äî —è—á–µ–π–∫–∞ –ø—Ä–æ–π–¥—ë—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ.\n",
        "# –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞–µ—Ç –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç –≤—Å—ë –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ.\n",
        "\n",
        "import sys, subprocess\n",
        "\n",
        "def _ensure_packages():\n",
        "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã\"\"\"\n",
        "    required = {\n",
        "        \"yaml\": \"pyyaml\",\n",
        "        \"httpx\": \"httpx\",\n",
        "        \"faiss\": \"faiss-cpu\",\n",
        "        \"langchain_core\": \"langchain-core\",\n",
        "        \"langchain_community\": \"langchain-community\",\n",
        "        \"langchain\": \"langchain\",\n",
        "        \"langchain_gigachat\": \"langchain-gigachat\",\n",
        "        \"sentence_transformers\": \"sentence-transformers\",\n",
        "        \"tabulate\": \"tabulate\",\n",
        "        \"pandas\": \"pandas\",\n",
        "    }\n",
        "    missing = []\n",
        "    for module, pip_name in required.items():\n",
        "        try:\n",
        "            __import__(module)\n",
        "        except ImportError:\n",
        "            missing.append(pip_name)\n",
        "\n",
        "    # PyTorch ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ, —Å—Ç–∞–≤–∏–º CPU-–≤–µ—Ä—Å–∏—é\n",
        "    try:\n",
        "        import torch\n",
        "    except ImportError:\n",
        "        missing.append(\"torch\")\n",
        "\n",
        "    if not missing:\n",
        "        print(\"‚úÖ –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")\n",
        "        return\n",
        "\n",
        "    print(f\"üì¶ –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã: {', '.join(missing)}\")\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"]\n",
        "\n",
        "    # Torch CPU –æ—Ç–¥–µ–ª—å–Ω–æ (—ç–∫–æ–Ω–æ–º–∏–º ~2 –ì–ë vs CUDA-–≤–µ—Ä—Å–∏—è)\n",
        "    if \"torch\" in missing:\n",
        "        print(\"   ‚ö° –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch (CPU)...\")\n",
        "        subprocess.check_call(\n",
        "            cmd + [\"torch\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"]\n",
        "        )\n",
        "        missing.remove(\"torch\")\n",
        "\n",
        "    if missing:\n",
        "        print(f\"   üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞: {', '.join(missing)}...\")\n",
        "        subprocess.check_call(cmd + missing)\n",
        "\n",
        "    print(\"‚úÖ –í—Å–µ –ø–∞–∫–µ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!\")\n",
        "\n",
        "_ensure_packages()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø (–≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–¥–∏–Ω —Ä–∞–∑)\n",
        "# ============================================================\n",
        "\n",
        "import os, json, re, yaml, time, httpx\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Optional, Any\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
        "with open(\"config.yml\", 'r', encoding='utf-8') as f:\n",
        "    CONFIG = yaml.safe_load(f)\n",
        "\n",
        "CUBE_URL = CONFIG[\"cube\"][\"api_url\"]\n",
        "CUBE_TOKEN = CONFIG[\"cube\"].get(\"api_token\", \"\")\n",
        "FAISS_PATH = CONFIG[\"faiss\"][\"index_path\"]\n",
        "EMBEDDING_MODEL = CONFIG[\"faiss\"][\"embedding_model\"]\n",
        "SEARCH_K = CONFIG[\"faiss\"].get(\"search_k\", 20)\n",
        "MAX_ROWS = CONFIG[\"agent\"].get(\"max_rows_display\", 15)\n",
        "\n",
        "print(\"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "print(f\"   Cube URL: {CUBE_URL}\")\n",
        "print(f\"   FAISS path: {FAISS_PATH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# –ó–ê–ì–†–£–ó–ö–ê –ö–û–ú–ü–û–ù–ï–ù–¢–û–í\n",
        "# ============================================================\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_gigachat import GigaChat\n",
        "\n",
        "# 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ FAISS\n",
        "print(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "print(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ FAISS-–∏–Ω–¥–µ–∫—Å–∞...\")\n",
        "vector_store = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# 2. –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ members\n",
        "with open(os.path.join(FAISS_PATH, \"members.json\"), 'r', encoding='utf-8') as f:\n",
        "    ALL_MEMBERS = json.load(f)\n",
        "\n",
        "# –ö–∞—Ä—Ç–∞ name ‚Üí title –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π\n",
        "TITLE_MAP = {}\n",
        "for m in ALL_MEMBERS:\n",
        "    TITLE_MAP[m[\"name\"]] = m[\"title\"]\n",
        "    short = m[\"name\"].split(\".\")[-1]\n",
        "    if short not in TITLE_MAP:\n",
        "        TITLE_MAP[short] = m[\"title\"]\n",
        "\n",
        "# 3. GigaChat ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Ä–µ–∂–∏–º–æ–≤ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è\n",
        "gc = CONFIG[\"gigachat\"]\n",
        "if gc.get(\"base_url\"):\n",
        "    # –†–µ–∂–∏–º 2: —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ (–∑–∞–∫—Ä—ã—Ç—ã–π –∫–æ–Ω—Ç—É—Ä)\n",
        "    token_env = gc.get(\"access_token_env\", \"JPY_API_TOKEN\")\n",
        "    llm = GigaChat(\n",
        "        base_url=gc[\"base_url\"],\n",
        "        access_token=os.getenv(token_env, \"\"),\n",
        "        model=gc.get(\"model\", \"GigaChat\")\n",
        "    )\n",
        "elif gc.get(\"credentials\"):\n",
        "    # –†–µ–∂–∏–º 1: —á–µ—Ä–µ–∑ credentials (SberCloud)\n",
        "    llm = GigaChat(\n",
        "        credentials=gc[\"credentials\"],\n",
        "        model=gc.get(\"model\", \"GigaChat\"),\n",
        "        verify_ssl_certs=gc.get(\"verify_ssl\", False),\n",
        "        timeout=gc.get(\"timeout\", 60)\n",
        "    )\n",
        "else:\n",
        "    raise RuntimeError(\"–ó–∞–ø–æ–ª–Ω–∏—Ç–µ gigachat.credentials –∏–ª–∏ gigachat.base_url –≤ config.yml\")\n",
        "\n",
        "# 4. Semantic config (glossary + examples) ‚Äî –ø—É—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞\n",
        "sem = CONFIG.get(\"semantic\", {})\n",
        "glossary_path = Path(sem.get(\"glossary_path\", \"config/glossary.yml\"))\n",
        "examples_path = Path(sem.get(\"examples_path\", \"config/examples.yml\"))\n",
        "\n",
        "GLOSSARY = {}\n",
        "if glossary_path.exists():\n",
        "    with open(glossary_path, 'r', encoding='utf-8') as f:\n",
        "        GLOSSARY = yaml.safe_load(f) or {}\n",
        "\n",
        "EXAMPLES = []\n",
        "if examples_path.exists():\n",
        "    with open(examples_path, 'r', encoding='utf-8') as f:\n",
        "        EXAMPLES = yaml.safe_load(f) or []\n",
        "\n",
        "print(f\"‚úÖ –í—Å—ë –∑–∞–≥—Ä—É–∂–µ–Ω–æ!\")\n",
        "print(f\"   FAISS members: {len(ALL_MEMBERS)}\")\n",
        "print(f\"   Glossary terms: {len(GLOSSARY)}\")\n",
        "print(f\"   Examples: {len(EXAMPLES)}\")\n",
        "print(f\"   LLM: GigaChat ({gc.get('model', 'GigaChat')})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# –Ø–î–†–û –ê–ì–ï–ù–¢–ê\n",
        "# ============================================================\n",
        "\n",
        "def search_relevant(question: str, k: int = SEARCH_K) -> List[Dict]:\n",
        "    \"\"\"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ FAISS\"\"\"\n",
        "    results = vector_store.similarity_search_with_score(question, k=k)\n",
        "    return [\n",
        "        {\n",
        "            \"name\": doc.metadata[\"name\"],\n",
        "            \"title\": doc.metadata[\"title\"],\n",
        "            \"type\": doc.metadata[\"type\"],\n",
        "            \"cube_name\": doc.metadata[\"cube_name\"],\n",
        "            \"member_type\": doc.metadata[\"member_type\"],\n",
        "            \"agg_type\": doc.metadata.get(\"agg_type\", \"\"),\n",
        "            \"description\": doc.metadata.get(\"description\", \"\"),\n",
        "            \"score\": float(score)\n",
        "        }\n",
        "        for doc, score in results\n",
        "    ]\n",
        "\n",
        "\n",
        "def find_relevant_examples(question: str, limit: int = 5) -> List[Dict]:\n",
        "    \"\"\"–ù–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ few-shot –ø—Ä–∏–º–µ—Ä—ã\"\"\"\n",
        "    q_lower = question.lower()\n",
        "    q_words = set(q_lower.split())\n",
        "    scored = []\n",
        "    for ex in EXAMPLES:\n",
        "        ex_words = set(ex.get(\"question\", \"\").lower().split())\n",
        "        overlap = len(q_words & ex_words)\n",
        "        tag_score = sum(2 for t in ex.get(\"tags\", []) if t in q_lower)\n",
        "        score = overlap * 0.5 + tag_score\n",
        "        if score > 0:\n",
        "            scored.append((score, ex))\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [ex for _, ex in scored[:limit]]\n",
        "\n",
        "\n",
        "def build_prompt(question: str, relevant: List[Dict]) -> str:\n",
        "    \"\"\"–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM\"\"\"\n",
        "    # –ú–µ—Ä—ã\n",
        "    measures = []\n",
        "    for m in relevant:\n",
        "        if m[\"member_type\"] == \"measure\":\n",
        "            line = f\"- {m['name']}: {m['title']}\"\n",
        "            if m[\"agg_type\"]:\n",
        "                line += f\" ({m['agg_type']})\"\n",
        "            if m[\"description\"]:\n",
        "                line += f\" ‚Äî {m['description'][:100]}\"\n",
        "            measures.append(line)\n",
        "    \n",
        "    # –ò–∑–º–µ—Ä–µ–Ω–∏—è\n",
        "    dimensions = []\n",
        "    for m in relevant:\n",
        "        if m[\"member_type\"] == \"dimension\":\n",
        "            line = f\"- {m['name']}: {m['title']}\"\n",
        "            if m[\"description\"]:\n",
        "                line += f\" ‚Äî {m['description'][:100]}\"\n",
        "            dimensions.append(line)\n",
        "    \n",
        "    # –ü—Ä–∏–º–µ—Ä—ã\n",
        "    examples = find_relevant_examples(question)\n",
        "    examples_text = \"\"\n",
        "    for i, ex in enumerate(examples, 1):\n",
        "        q_str = json.dumps(ex.get(\"query\", {}), ensure_ascii=False)\n",
        "        examples_text += f\"Q{i}: {ex['question']}\\nA{i}: {q_str}\\n\\n\"\n",
        "    \n",
        "    return f\"\"\"–¢—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Cube.js. –ü—Ä–µ–æ–±—Ä–∞–∑—É–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –≤ –≤–∞–ª–∏–¥–Ω—ã–π JSON-–∑–∞–ø—Ä–æ—Å.\n",
        "\n",
        "## –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ä—ã:\n",
        "{chr(10).join(measures) if measures else '–ù–µ—Ç –º–µ—Ä'}\n",
        "\n",
        "## –î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è:\n",
        "{chr(10).join(dimensions) if dimensions else '–ù–µ—Ç –∏–∑–º–µ—Ä–µ–Ω–∏–π'}\n",
        "\n",
        "## –§–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞:\n",
        "{{\n",
        "  \"measures\": [\"cube.measure\"],\n",
        "  \"dimensions\": [\"cube.dimension\"],\n",
        "  \"filters\": [\n",
        "    {{\"member\": \"cube.field\", \"operator\": \"equals|contains|gt|lt|set|notSet\", \"values\": [\"value\"]}}\n",
        "  ],\n",
        "  \"order\": {{\"cube.measure\": \"desc\"}},\n",
        "  \"limit\": 100\n",
        "}}\n",
        "\n",
        "### –û–ø–µ—Ä–∞—Ç–æ—Ä—ã: equals, contains, gt, lt, set (IS NOT NULL), notSet (IS NULL)\n",
        "–î–ª—è set/notSet –ù–ï –Ω—É–∂–µ–Ω \"values\".\n",
        "\n",
        "## –ü—Ä–∏–º–µ—Ä—ã:\n",
        "{examples_text if examples_text else '–ù–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤'}\n",
        "\n",
        "## –ü—Ä–∞–≤–∏–ª–∞:\n",
        "1. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–ß–ù–´–ï –∏–º–µ–Ω–∞ –ø–æ–ª–µ–π –∏–∑ —Å–ø–∏—Å–∫–æ–≤ –≤—ã—à–µ\n",
        "2. –î–ª—è –∏–º—ë–Ω –ª—é–¥–µ–π –∏—Å–ø–æ–ª—å–∑—É–π –æ–ø–µ—Ä–∞—Ç–æ—Ä \"contains\"\n",
        "3. –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–π —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–µ—Ä—É\n",
        "4. –í–°–ï–ì–î–ê –≤–∫–ª—é—á–∞–π dimensions –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Ñ–∏–ª—å—Ç—Ä—É–µ—à—å –≤ –≤—ã–≤–æ–¥\n",
        "5. \"–ø–æ–∫–∞–∂–∏/—Å–ø–∏—Å–æ–∫\" = –≤–∫–ª—é—á–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–µ dimensions (key, name, etc)\n",
        "6. \"—Å–∫–æ–ª—å–∫–æ\" = –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ count –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–æ—á–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ\n",
        "7. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π\n",
        "\n",
        "## –í–æ–ø—Ä–æ—Å:\n",
        "{question}\n",
        "\n",
        "## JSON Query:\"\"\"\n",
        "\n",
        "\n",
        "def parse_llm_response(content: str) -> Dict:\n",
        "    \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM\"\"\"\n",
        "    content = content.strip()\n",
        "    # –£–±—Ä–∞—Ç—å markdown\n",
        "    if content.startswith(\"```\"):\n",
        "        lines = content.split(\"\\n\")\n",
        "        content = \"\\n\".join(lines[1:])\n",
        "        if content.endswith(\"```\"):\n",
        "            content = content[:-3]\n",
        "    # –¢–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏–µ –∫–∞–≤—ã—á–∫–∏\n",
        "    for old, new in [('\\u201c', '\"'), ('\\u201d', '\"'), ('\\u00ab', '\"'), ('\\u00bb', '\"'),\n",
        "                     ('\\u2018', \"'\"), ('\\u2019', \"'\")]:\n",
        "        content = content.replace(old, new)\n",
        "    # –ù–∞–π—Ç–∏ JSON\n",
        "    match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
        "    if match:\n",
        "        cleaned = match.group()\n",
        "        cleaned = re.sub(r',\\s*}', '}', cleaned)\n",
        "        cleaned = re.sub(r',\\s*]', ']', cleaned)\n",
        "        return json.loads(cleaned)\n",
        "    return json.loads(content)\n",
        "\n",
        "\n",
        "def call_cube(query: Dict) -> Dict:\n",
        "    \"\"\"–í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ Cube REST API\"\"\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    if CUBE_TOKEN:\n",
        "        headers[\"Authorization\"] = f\"Bearer {CUBE_TOKEN}\"\n",
        "    \n",
        "    client = httpx.Client(timeout=30.0)\n",
        "    url = f\"{CUBE_URL}/load\"\n",
        "    resp = client.post(url, json={\"query\": query}, headers=headers)\n",
        "    return resp.json()\n",
        "\n",
        "\n",
        "def format_result(result: Dict) -> str:\n",
        "    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ä—É—Å—Å–∫–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏\"\"\"\n",
        "    if \"error\" in result:\n",
        "        return f\"‚ùå –û—à–∏–±–∫–∞: {result['error']}\"\n",
        "    data = result.get(\"data\", [])\n",
        "    if not data:\n",
        "        return \"üìä –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\"\n",
        "    \n",
        "    lines = [f\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ({len(data)} —Å—Ç—Ä–æ–∫):\"]\n",
        "    for i, row in enumerate(data[:MAX_ROWS], 1):\n",
        "        parts = []\n",
        "        for k, v in row.items():\n",
        "            name = TITLE_MAP.get(k, k.split('.')[-1])\n",
        "            if isinstance(v, float):\n",
        "                v = round(v, 2)\n",
        "            if v is None:\n",
        "                v = \"‚Äî\"\n",
        "            parts.append(f\"{name}: {v}\")\n",
        "        lines.append(f\"  {i}. {', '.join(parts)}\")\n",
        "    \n",
        "    if len(data) > MAX_ROWS:\n",
        "        lines.append(f\"  ... –∏ –µ—â—ë {len(data) - MAX_ROWS} —Å—Ç—Ä–æ–∫\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "print(\"‚úÖ –Ø–¥—Ä–æ –∞–≥–µ–Ω—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–æ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# –§–£–ù–ö–¶–ò–Ø –ó–ê–ü–†–û–°–ê (–æ—Å–Ω–æ–≤–Ω–∞—è)\n",
        "# ============================================================\n",
        "\n",
        "def ask(question: str, verbose: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ.\n",
        "    \n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "        question: –í–æ–ø—Ä–æ—Å (–Ω–∞ —Ä—É—Å—Å–∫–æ–º)\n",
        "        verbose: –ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª–∏ (–∑–∞–ø—Ä–æ—Å, –ø—Ä–æ–º–ø—Ç –∏ —Ç.–¥.)\n",
        "    \n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    \n",
        "    # 1. –ü–æ–∏—Å–∫ –ø–æ FAISS\n",
        "    relevant = search_relevant(question)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"üîé –ù–∞–π–¥–µ–Ω–æ members:\")\n",
        "        for m in relevant[:5]:\n",
        "            print(f\"   {m['score']:.1f} | {m['name']:40} | {m['title']}\")\n",
        "        print()\n",
        "    \n",
        "    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
        "    if not relevant or relevant[0][\"score\"] > 18.0:\n",
        "        return \"ü§î –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–∞–Ω–Ω—ã–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.\"\n",
        "    \n",
        "    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Cube-–∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ LLM\n",
        "    prompt = build_prompt(question, relevant)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üìù –ü—Ä–æ–º–ø—Ç: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "    \n",
        "    response = llm.invoke(prompt)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"ü§ñ LLM –æ—Ç–≤–µ—Ç: {response.content[:200]}\")\n",
        "        print()\n",
        "    \n",
        "    try:\n",
        "        cube_query = parse_llm_response(response.content)\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ LLM: {e}\\n\\n–û—Ç–≤–µ—Ç: {response.content[:300]}\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"üìã Cube Query:\")\n",
        "        print(json.dumps(cube_query, ensure_ascii=False, indent=2))\n",
        "        print()\n",
        "    \n",
        "    # 4. –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "    if not cube_query.get(\"measures\"):\n",
        "        return \"ü§î –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫—É. –£—Ç–æ—á–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.\"\n",
        "    \n",
        "    # 5. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ\n",
        "    try:\n",
        "        result = call_cube(cube_query)\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå –û—à–∏–±–∫–∞ Cube: {e}\"\n",
        "    \n",
        "    elapsed = int((time.time() - start) * 1000)\n",
        "    answer = format_result(result)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed}ms\")\n",
        "    \n",
        "    return answer\n",
        "\n",
        "\n",
        "print(\"‚úÖ –ì–æ—Ç–æ–≤–æ! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é ask() –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤.\")\n",
        "print()\n",
        "print('–ü—Ä–∏–º–µ—Ä—ã:')\n",
        "print('  ask(\"–°–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á –ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º?\")')\n",
        "print('  ask(\"–ü–æ–∫–∞–∂–∏ –∑–∞–¥–∞—á–∏ –ø–æ –ø—Ä–æ–µ–∫—Ç—É AI\", verbose=True)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# –ó–ê–î–ê–í–ê–ô–¢–ï –í–û–ü–†–û–°–´ –ó–î–ï–°–¨\n",
        "# ============================================================\n",
        "\n",
        "print(ask(\"–°–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á –ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º?\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ï—â—ë –≤–æ–ø—Ä–æ—Å (–¥—É–±–ª–∏—Ä—É–π—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É –¥–ª—è –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤)\n",
        "print(ask(\"–ü–æ–∫–∞–∂–∏ –∑–∞–¥–∞—á–∏ –ø–æ –ø—Ä–æ–µ–∫—Ç—É AI\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# –ü–æ–¥—Ä–æ–±–Ω—ã–π —Ä–µ–∂–∏–º ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—Å–µ —à–∞–≥–∏\n",
        "print(ask(\"–°–∫–æ–ª—å–∫–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–º Lisa\", verbose=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}