"""
=================================================================
–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –í DUCKDB
=================================================================
–°–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞—ë—Ç DuckDB-–±–∞–∑—É –∏–∑ Parquet/CSV —Ñ–∞–π–ª–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–∞–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —à–∞–≥: GP(Spark) ‚Üí Parquet ‚Üí DuckDB ‚Üí Cube.

–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–≤ PySpark):
    df = spark.sql("SELECT * FROM schema.my_table")
    df.write.parquet("/path/to/export/my_table.parquet")
    # –∏–ª–∏
    df.toPandas().to_csv("/path/to/export/my_table.csv", index=False)

–ó–∞–ø—É—Å–∫:
    python 00_load_duckdb.py                          # –∏–∑ ./data/
    python 00_load_duckdb.py --data-dir /path/to/export
    python 00_load_duckdb.py --data-dir ./data --db ./data.duckdb --schema main
=================================================================
"""

import os
import sys
import argparse
from pathlib import Path


def ensure_duckdb():
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å duckdb –µ—Å–ª–∏ –Ω–µ—Ç"""
    try:
        import duckdb
        return duckdb
    except ImportError:
        import subprocess
        print("üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ duckdb...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "--quiet", "duckdb"])
        import duckdb
        return duckdb


def load_files_to_duckdb(data_dir, db_path, schema="main"):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ Parquet/CSV —Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ DuckDB"""
    duckdb = ensure_duckdb()
    
    data_dir = Path(data_dir)
    if not data_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
        print(f"   –°–æ–∑–¥–∞–π—Ç–µ –µ—ë –∏ –ø–æ–ª–æ–∂–∏—Ç–µ —Ç—É–¥–∞ Parquet/CSV —Ñ–∞–π–ª—ã:")
        print(f"   mkdir -p {data_dir}")
        sys.exit(1)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã
    parquet_files = sorted(data_dir.glob("*.parquet"))
    csv_files = sorted(data_dir.glob("*.csv"))
    
    # –¢–∞–∫–∂–µ –∏—â–µ–º Parquet-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (Spark —Å–æ–∑–¥–∞—ë—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å part-—Ñ–∞–π–ª–∞–º–∏)
    parquet_dirs = []
    for d in sorted(data_dir.iterdir()):
        if d.is_dir() and list(d.glob("*.parquet")):
            parquet_dirs.append(d)
    
    total = len(parquet_files) + len(csv_files) + len(parquet_dirs)
    if total == 0:
        print(f"‚ùå –í {data_dir} –Ω–µ—Ç Parquet –∏–ª–∏ CSV —Ñ–∞–π–ª–æ–≤")
        print()
        print("–ö–∞–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ GreenPlum (PySpark):")
        print("=" * 50)
        print("""
# –í PySpark-–Ω–æ—É—Ç–±—É–∫–µ –∏–ª–∏ —Å–∫—Ä–∏–ø—Ç–µ:

from pyspark.sql import SparkSession
spark = SparkSession.builder.enableHiveSupport().getOrCreate()

# –°–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏
tables = [
    "schema.users",
    "schema.roles",
    "schema.groups",
    # ... –¥–æ–±–∞–≤—å—Ç–µ –Ω—É–∂–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
]

export_dir = "/path/to/export"

for table in tables:
    name = table.split(".")[-1]
    print(f"–≠–∫—Å–ø–æ—Ä—Ç {table}...")
    df = spark.sql(f"SELECT * FROM {table}")
    
    # –í–∞—Ä–∏–∞–Ω—Ç 1: Parquet (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è ‚Äî –±—ã—Å—Ç—Ä–µ–µ, –∫–æ–º–ø–∞–∫—Ç–Ω–µ–µ)
    df.coalesce(1).write.mode("overwrite").parquet(f"{export_dir}/{name}.parquet")
    
    # –í–∞—Ä–∏–∞–Ω—Ç 2: CSV (–µ—Å–ª–∏ Parquet –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
    # df.toPandas().to_csv(f"{export_dir}/{name}.csv", index=False)

print("–ì–æ—Ç–æ–≤–æ! –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É data/ –ø–∞–∫–µ—Ç–∞.")
""")
        sys.exit(1)
    
    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ: {len(parquet_files)} parquet-—Ñ–∞–π–ª–æ–≤, "
          f"{len(csv_files)} csv-—Ñ–∞–π–ª–æ–≤, {len(parquet_dirs)} parquet-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
    print()
    
    # –°–æ–∑–¥–∞—ë–º / –æ—Ç–∫—Ä—ã–≤–∞–µ–º DuckDB
    conn = duckdb.connect(str(db_path))
    
    # –°–æ–∑–¥–∞—ë–º —Å—Ö–µ–º—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if schema != "main":
        conn.execute(f"CREATE SCHEMA IF NOT EXISTS {schema}")
    
    loaded = 0
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ Parquet-—Ñ–∞–π–ª—ã
    for pf in parquet_files:
        table_name = pf.stem  # users.parquet ‚Üí users
        fqn = f"{schema}.{table_name}" if schema != "main" else table_name
        print(f"  üì• {pf.name} ‚Üí {fqn}")
        try:
            conn.execute(f'CREATE OR REPLACE TABLE {fqn} AS SELECT * FROM read_parquet(\'{pf}\')')
            count = conn.execute(f"SELECT COUNT(*) FROM {fqn}").fetchone()[0]
            cols = conn.execute(f"SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '{table_name}'").fetchone()[0]
            print(f"     ‚úÖ {count} —Å—Ç—Ä–æ–∫, {cols} –∫–æ–ª–æ–Ω–æ–∫")
            loaded += 1
        except Exception as e:
            print(f"     ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º Parquet-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (Spark output)
    for pd in parquet_dirs:
        table_name = pd.name
        fqn = f"{schema}.{table_name}" if schema != "main" else table_name
        glob_path = str(pd / "*.parquet")
        print(f"  üì• {pd.name}/ (Spark parquet) ‚Üí {fqn}")
        try:
            conn.execute(f"CREATE OR REPLACE TABLE {fqn} AS SELECT * FROM read_parquet('{glob_path}')")
            count = conn.execute(f"SELECT COUNT(*) FROM {fqn}").fetchone()[0]
            cols = conn.execute(f"SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '{table_name}'").fetchone()[0]
            print(f"     ‚úÖ {count} —Å—Ç—Ä–æ–∫, {cols} –∫–æ–ª–æ–Ω–æ–∫")
            loaded += 1
        except Exception as e:
            print(f"     ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
    for cf in csv_files:
        table_name = cf.stem
        fqn = f"{schema}.{table_name}" if schema != "main" else table_name
        print(f"  üì• {cf.name} ‚Üí {fqn}")
        try:
            conn.execute(f"CREATE OR REPLACE TABLE {fqn} AS SELECT * FROM read_csv_auto('{cf}')")
            count = conn.execute(f"SELECT COUNT(*) FROM {fqn}").fetchone()[0]
            cols = conn.execute(f"SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '{table_name}'").fetchone()[0]
            print(f"     ‚úÖ {count} —Å—Ç—Ä–æ–∫, {cols} –∫–æ–ª–æ–Ω–æ–∫")
            loaded += 1
        except Exception as e:
            print(f"     ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    # –ò—Ç–æ–≥
    if schema != "main":
        all_tables = conn.execute(
            "SELECT table_name FROM information_schema.tables "
            "WHERE table_schema = ? AND table_type = 'BASE TABLE' ORDER BY table_name",
            [schema]
        ).fetchall()
    else:
        all_tables = conn.execute(
            "SELECT table_name FROM information_schema.tables "
            "WHERE table_type = 'BASE TABLE' ORDER BY table_name"
        ).fetchall()
    
    conn.close()
    
    print()
    print("=" * 60)
    print(f"  ‚úÖ –ì–û–¢–û–í–û! –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {loaded}")
    print("=" * 60)
    print(f"""
DuckDB-—Ñ–∞–π–ª: {db_path} ({os.path.getsize(db_path) / 1024 / 1024:.1f} MB)
–°—Ö–µ–º–∞: {schema}
–¢–∞–±–ª–∏—Ü—ã: {', '.join(t[0] for t in all_tables)}

–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
  1. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ config.yml –Ω–∞—Å—Ç—Ä–æ–µ–Ω:
     database:
       driver: "duckdb"
       path: "{db_path}"
       schema: "{schema}"

  2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ Cube –¥–ª—è DuckDB (—Å–º. cube.env.example)

  3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python 01_data_loader.py
     (–∏–ª–∏: python 01_data_loader.py --source duckdb)
""")


def main():
    parser = argparse.ArgumentParser(
        description="–ó–∞–≥—Ä—É–∑–∫–∞ Parquet/CSV –≤ DuckDB –¥–ª—è Cube",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  python 00_load_duckdb.py --data-dir ./data
  python 00_load_duckdb.py --data-dir /export/gp_tables --db ./analytics.duckdb
  python 00_load_duckdb.py --data-dir ./data --schema dbo
        """
    )
    parser.add_argument("--data-dir", default="./data",
                        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å Parquet/CSV —Ñ–∞–π–ª–∞–º–∏ (default: ./data)")
    parser.add_argument("--db", default="./data.duckdb",
                        help="–ü—É—Ç—å –∫ DuckDB-—Ñ–∞–π–ª—É (default: ./data.duckdb)")
    parser.add_argument("--schema", default="main",
                        help="–°—Ö–µ–º–∞ –≤ DuckDB (default: main)")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("  –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –í DUCKDB")
    print("  Parquet/CSV ‚Üí DuckDB ‚Üí Cube")
    print("=" * 60)
    print()
    
    load_files_to_duckdb(args.data_dir, args.db, args.schema)


if __name__ == "__main__":
    main()
