"""
=================================================================
–ó–ê–ì–†–£–ó–ß–ò–ö –î–ê–ù–ù–´–• –í CUBE
=================================================================
–°–∫—Ä–∏–ø—Ç —á–∏—Ç–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è
–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —á–µ—Ä–µ–∑ GigaChat –∏ —Å–æ–∑–¥–∞—ë—Ç YAML-–º–æ–¥–µ–ª–∏ –¥–ª—è Cube.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 3 —Ä–µ–∂–∏–º–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö (database.driver –≤ config.yml):
  - postgresql / greenplum  ‚Äî –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
  - duckdb                  ‚Äî –ª–æ–∫–∞–ª—å–Ω—ã–π DuckDB-—Ñ–∞–π–ª
  - cube                    ‚Äî —á—Ç–µ–Ω–∏–µ –∏–∑ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ Cube API (–±–µ–∑ –ë–î)

–ó–∞–ø—É—Å–∫: python 01_data_loader.py
        python 01_data_loader.py --source cube    (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ–∂–∏–º)
        python 01_data_loader.py --source duckdb
=================================================================
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path
from datetime import datetime

# ============================================================
# –ê–≤—Ç–æ—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
# ============================================================

def _ensure_packages():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã (–±–∞–∑–æ–≤—ã–µ)"""
    required = {
        "yaml": "pyyaml",
        "langchain_gigachat": "langchain-gigachat",
    }
    missing = []
    for module, pip_name in required.items():
        try:
            __import__(module)
        except ImportError:
            missing.append(pip_name)
    if missing:
        print(f"üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–∞–∫–µ—Ç–æ–≤: {', '.join(missing)}")
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "--quiet"] + missing
        )
        print("‚úÖ –ü–∞–∫–µ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

_ensure_packages()

import yaml

# psycopg2 / duckdb –ø–æ–¥–≥—Ä—É–∂–∞—é—Ç—Å—è –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ create_data_source()
try:
    import psycopg2
except ImportError:
    psycopg2 = None  # –ù–µ –Ω—É–∂–µ–Ω –¥–ª—è duckdb/cube —Ä–µ–∂–∏–º–æ–≤

# ============================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
# ============================================================

def load_config(config_path="config.yml"):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML-—Ñ–∞–π–ª–∞"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


# ============================================================
# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat
# ============================================================

def create_gigachat(config):
    """–°–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç GigaChat. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 2 —Ä–µ–∂–∏–º–∞:
       1. credentials ‚Äî –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø (SberCloud)
       2. base_url + access_token ‚Äî —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ (–∑–∞–∫—Ä—ã—Ç—ã–π –∫–æ–Ω—Ç—É—Ä)
    """
    from langchain_gigachat import GigaChat
    
    gc = config["gigachat"]
    model = gc.get("model", "GigaChat")
    
    # –†–µ–∂–∏–º 2: —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ (base_url + access_token –∏–∑ env)
    if gc.get("base_url"):
        token_env = gc.get("access_token_env", "JPY_API_TOKEN")
        token = os.getenv(token_env, "")
        return GigaChat(
            base_url=gc["base_url"],
            access_token=token,
            model=model
        )
    
    # –†–µ–∂–∏–º 1: —á–µ—Ä–µ–∑ credentials
    if gc.get("credentials"):
        return GigaChat(
            credentials=gc["credentials"],
            model=model,
            verify_ssl_certs=gc.get("verify_ssl", False),
            timeout=gc.get("timeout", 60)
        )
    
    print("‚ùå –û—à–∏–±–∫–∞: –ó–∞–ø–æ–ª–Ω–∏—Ç–µ gigachat.credentials –∏–ª–∏ gigachat.base_url –≤ config.yml")
    sys.exit(1)


# ============================================================
# –ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ë–î
# ============================================================

def get_db_connection(config):
    """–ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ PostgreSQL / GreenPlum"""
    if psycopg2 is None:
        print("‚ùå psycopg2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install psycopg2-binary")
        print("   –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --source duckdb / --source cube")
        sys.exit(1)
    db = config["database"]
    return psycopg2.connect(
        host=db["host"],
        port=db["port"],
        dbname=db["name"],
        user=db["user"],
        password=db["password"]
    )


def get_schema(config):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è —Å—Ö–µ–º—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
    return config.get("database", {}).get("schema", "public")


def get_tables(conn, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü"""
    cur = conn.cursor()
    cur.execute("""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = %s 
          AND table_type = 'BASE TABLE'
        ORDER BY table_name
    """, (schema,))
    tables = [row[0] for row in cur.fetchall()]
    cur.close()
    return tables


def get_columns(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ —Ç–∞–±–ª–∏—Ü—ã —Å —Ç–∏–ø–∞–º–∏"""
    cur = conn.cursor()
    cur.execute("""
        SELECT column_name, data_type, is_nullable, column_default,
               character_maximum_length
        FROM information_schema.columns
        WHERE table_schema = %s AND table_name = %s
        ORDER BY ordinal_position
    """, (schema, table_name))
    columns = []
    for row in cur.fetchall():
        columns.append({
            "name": row[0],
            "data_type": row[1],
            "nullable": row[2] == "YES",
            "default": row[3],
            "max_length": row[4]
        })
    cur.close()
    return columns


def get_foreign_keys(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏ —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    cur.execute("""
        SELECT
            kcu.column_name,
            ccu.table_name AS foreign_table,
            ccu.column_name AS foreign_column
        FROM information_schema.table_constraints AS tc
        JOIN information_schema.key_column_usage AS kcu
            ON tc.constraint_name = kcu.constraint_name
        JOIN information_schema.constraint_column_usage AS ccu
            ON ccu.constraint_name = tc.constraint_name
        WHERE tc.constraint_type = 'FOREIGN KEY'
          AND tc.table_name = %s
          AND tc.table_schema = %s
    """, (table_name, schema))
    fks = []
    for row in cur.fetchall():
        fks.append({
            "column": row[0],
            "foreign_table": row[1],
            "foreign_column": row[2]
        })
    cur.close()
    return fks


def get_primary_key(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å primary key —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    cur.execute("""
        SELECT kcu.column_name
        FROM information_schema.table_constraints tc
        JOIN information_schema.key_column_usage kcu
            ON tc.constraint_name = kcu.constraint_name
        WHERE tc.constraint_type = 'PRIMARY KEY'
          AND tc.table_name = %s
          AND tc.table_schema = %s
    """, (table_name, schema))
    result = cur.fetchone()
    cur.close()
    return result[0] if result else "id"


def get_sample_data(conn, table_name, schema="public", limit=5):
    """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
    cur = conn.cursor()
    try:
        cur.execute(f'SELECT * FROM {schema}."{table_name}" LIMIT %s', (limit,))
        columns = [desc[0] for desc in cur.description]
        rows = cur.fetchall()
        cur.close()
        return columns, rows
    except Exception:
        cur.close()
        return [], []


def get_row_count(conn, table_name, schema="public"):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫"""
    cur = conn.cursor()
    cur.execute(f'SELECT COUNT(*) FROM {schema}."{table_name}"')
    count = cur.fetchone()[0]
    cur.close()
    return count


# ============================================================
# –ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑ DuckDB
# ============================================================

class DuckDBSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ‚Äî –ª–æ–∫–∞–ª—å–Ω—ã–π DuckDB-—Ñ–∞–π–ª.
    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ç–æ—Ç –∂–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á—Ç–æ –∏ psycopg2-—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã—à–µ.
    """

    def __init__(self, config):
        import duckdb
        db_path = config["database"].get("path", "./data.duckdb")
        self.schema = config["database"].get("schema", "main")
        self.conn = duckdb.connect(db_path, read_only=True)
        print(f"‚úÖ DuckDB: {db_path} (schema={self.schema})")

    def get_tables(self):
        rows = self.conn.execute(
            "SELECT table_name FROM information_schema.tables "
            "WHERE table_schema = ? AND table_type = 'BASE TABLE' "
            "ORDER BY table_name",
            [self.schema]
        ).fetchall()
        return [r[0] for r in rows]

    def get_columns(self, table_name):
        rows = self.conn.execute(
            "SELECT column_name, data_type, is_nullable, column_default, "
            "character_maximum_length "
            "FROM information_schema.columns "
            "WHERE table_schema = ? AND table_name = ? "
            "ORDER BY ordinal_position",
            [self.schema, table_name]
        ).fetchall()
        return [{
            "name": r[0],
            "data_type": r[1],
            "nullable": r[2] == "YES",
            "default": r[3],
            "max_length": r[4]
        } for r in rows]

    def get_foreign_keys(self, table_name):
        # DuckDB –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç FK, –Ω–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω—è–µ—Ç information_schema
        try:
            rows = self.conn.execute(
                "SELECT kcu.column_name, ccu.table_name, ccu.column_name "
                "FROM information_schema.table_constraints tc "
                "JOIN information_schema.key_column_usage kcu "
                "  ON tc.constraint_name = kcu.constraint_name "
                "JOIN information_schema.constraint_column_usage ccu "
                "  ON ccu.constraint_name = tc.constraint_name "
                "WHERE tc.constraint_type = 'FOREIGN KEY' "
                "  AND tc.table_name = ? AND tc.table_schema = ?",
                [table_name, self.schema]
            ).fetchall()
            return [{"column": r[0], "foreign_table": r[1], "foreign_column": r[2]}
                    for r in rows]
        except Exception:
            return []

    def get_primary_key(self, table_name):
        try:
            rows = self.conn.execute(
                "SELECT kcu.column_name "
                "FROM information_schema.table_constraints tc "
                "JOIN information_schema.key_column_usage kcu "
                "  ON tc.constraint_name = kcu.constraint_name "
                "WHERE tc.constraint_type = 'PRIMARY KEY' "
                "  AND tc.table_name = ? AND tc.table_schema = ?",
                [table_name, self.schema]
            ).fetchall()
            return rows[0][0] if rows else "id"
        except Exception:
            return "id"

    def get_sample_data(self, table_name, limit=5):
        try:
            result = self.conn.execute(
                f'SELECT * FROM {self.schema}."{table_name}" LIMIT ?', [limit]
            )
            columns = [desc[0] for desc in result.description]
            rows = result.fetchall()
            return columns, rows
        except Exception:
            return [], []

    def get_row_count(self, table_name):
        result = self.conn.execute(
            f'SELECT COUNT(*) FROM {self.schema}."{table_name}"'
        )
        return result.fetchone()[0]

    def close(self):
        self.conn.close()


# ============================================================
# –ß—Ç–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ Cube API
# ============================================================

class CubeAPISource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ‚Äî –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ Cube REST API /meta.
    –ù–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –≤–æ–æ–±—â–µ.
    Cube —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç ‚Üí —á–∏—Ç–∞–µ–º –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
    """

    def __init__(self, config):
        import httpx
        self.cube_url = config["cube"]["api_url"]
        self.schema = config["database"].get("schema", "public")
        headers = {}
        token = config["cube"].get("api_token", "")
        if token:
            headers["Authorization"] = f"Bearer {token}"

        resp = httpx.get(f"{self.cube_url}/meta", headers=headers, timeout=15.0)
        resp.raise_for_status()
        self.meta = resp.json()
        self.cubes = {c["name"]: c for c in self.meta.get("cubes", [])}
        print(f"‚úÖ Cube API: {len(self.cubes)} –∫—É–±–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {self.cube_url}")

    def get_tables(self):
        return sorted(self.cubes.keys())

    def get_columns(self, table_name):
        cube = self.cubes.get(table_name, {})
        columns = []
        for dim in cube.get("dimensions", []):
            short_name = dim["name"].split(".")[-1]
            columns.append({
                "name": short_name,
                "data_type": dim.get("type", "string"),
                "nullable": True,
                "default": None,
                "max_length": None
            })
        return columns

    def get_foreign_keys(self, table_name):
        # Cube API –Ω–µ –æ—Ç–¥–∞—ë—Ç FK ‚Äî joins –æ–±–Ω–∞—Ä—É–∂–∏–º –ø–æ –∏–º–µ–Ω–∞–º –∫–æ–ª–æ–Ω–æ–∫
        return []

    def get_primary_key(self, table_name):
        cube = self.cubes.get(table_name, {})
        for dim in cube.get("dimensions", []):
            if dim.get("primaryKey"):
                return dim["name"].split(".")[-1]
        return "id"

    def get_sample_data(self, table_name, limit=5):
        # –ù–µ –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å sample data —á–µ—Ä–µ–∑ Cube API
        return [], []

    def get_row_count(self, table_name):
        # –ü—Ä–æ–±—É–µ–º count –∏–∑ Cube
        import httpx
        cube = self.cubes.get(table_name, {})
        count_measure = None
        for m in cube.get("measures", []):
            if m.get("type") == "count":
                count_measure = m["name"]
                break
        if not count_measure:
            return 0
        try:
            headers = {"Content-Type": "application/json"}
            resp = httpx.post(
                f"{self.cube_url}/load",
                json={"query": {"measures": [count_measure], "limit": 1}},
                headers=headers,
                timeout=15.0
            )
            data = resp.json().get("data", [])
            if data:
                return list(data[0].values())[0]
        except Exception:
            pass
        return 0

    def close(self):
        pass


# ============================================================
# –§–∞–±—Ä–∏–∫–∞: –≤—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ============================================================

def create_data_source(config, override_source=None):
    """–°–æ–∑–¥–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ config.yml –∏–ª–∏ --source –∞—Ä–≥—É–º–µ–Ω—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –º–µ—Ç–æ–¥–∞–º–∏: get_tables, get_columns, get_foreign_keys,
    get_primary_key, get_sample_data, get_row_count, close.
    """
    driver = override_source or config.get("database", {}).get("driver", "postgresql")
    driver = driver.lower().strip()

    if driver == "duckdb":
        return DuckDBSource(config), driver
    elif driver == "cube":
        return CubeAPISource(config), driver
    elif driver in ("postgresql", "greenplum", "postgres"):
        # –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ psycopg2-—Ñ—É–Ω–∫—Ü–∏—è–º–∏ –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        conn = get_db_connection(config)
        schema = get_schema(config)
        return _PsycopgSource(conn, schema), driver
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π driver: {driver}")
        print("   –î–æ–ø—É—Å—Ç–∏–º—ã–µ: postgresql, greenplum, duckdb, cube")
        sys.exit(1)


class _PsycopgSource:
    """–û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ psycopg2 –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."""
    def __init__(self, conn, schema):
        self.conn = conn
        self.schema = schema

    def get_tables(self):
        return get_tables(self.conn, self.schema)

    def get_columns(self, table_name):
        return get_columns(self.conn, table_name, self.schema)

    def get_foreign_keys(self, table_name):
        return get_foreign_keys(self.conn, table_name, self.schema)

    def get_primary_key(self, table_name):
        return get_primary_key(self.conn, table_name, self.schema)

    def get_sample_data(self, table_name, limit=5):
        return get_sample_data(self.conn, table_name, self.schema, limit)

    def get_row_count(self, table_name):
        return get_row_count(self.conn, table_name, self.schema)

    def close(self):
        self.conn.close()


# ============================================================
# –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏
# ============================================================

def detect_implicit_relationships(table_name, columns, all_tables, explicit_fks):
    """
    –ù–∞–π—Ç–∏ –Ω–µ—è–≤–Ω—ã–µ —Å–≤—è–∑–∏ –ø–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏—é –æ–± –∏–º–µ–Ω–∞—Ö (*_id ‚Üí —Ç–∞–±–ª–∏—Ü–∞).
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
      - –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: project_id ‚Üí projects
      - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ: priority_id ‚Üí priorities, status_id ‚Üí statuses
      - –ü—Ä–µ—Ñ–∏–∫—Å—ã –¥–æ–º–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü: status_id ‚Üí issue_statuses, type_id ‚Üí issue_types
      - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–∞–ø–ø–∏–Ω–≥–∏: assignee_id ‚Üí users, reporter_id ‚Üí users, author_id ‚Üí users
    """
    explicit_cols = {fk["column"] for fk in explicit_fks}
    implicit = []

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–∞–ø–ø–∏–Ω–≥–∏: –∫–æ–ª–æ–Ω–∫–∞ ‚Üí —Ü–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
    SEMANTIC_MAP = {
        "assignee": "users",
        "reporter": "users",
        "author": "users",
        "creator": "users",
        "owner": "users",
        "updated_by": "users",
        "created_by": "users",
        "lead": "users",
        "manager": "users",
        "parent": None,  # self-join –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
    }

    for col in columns:
        col_name = col["name"]
        if not col_name.endswith("_id") or col_name in explicit_cols:
            continue
        if col_name == "id":
            continue

        base = col_name[:-3]  # "project_id" ‚Üí "project"

        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥
        if base in SEMANTIC_MAP:
            target = SEMANTIC_MAP[base]
            if target and target in all_tables:
                implicit.append({
                    "column": col_name,
                    "foreign_table": target,
                    "foreign_column": "id",
                    "source": "implicit"
                })
                continue
            elif target is None and table_name in all_tables:
                # self-join (parent_id ‚Üí —Ç–∞ –∂–µ —Ç–∞–±–ª–∏—Ü–∞)
                implicit.append({
                    "column": col_name,
                    "foreign_table": table_name,
                    "foreign_column": "id",
                    "source": "implicit"
                })
                continue

        # 2. –ü—Ä—è–º—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –ø–æ –∏–º–µ–Ω–∏
        candidates = [
            base + "s",       # project ‚Üí projects
            base + "es",      # status ‚Üí statuses
            base,             # sprint ‚Üí sprint
        ]
        if base.endswith("y"):
            candidates.append(base[:-1] + "ies")  # priority ‚Üí priorities, category ‚Üí categories
        if base.endswith("s"):
            candidates.append(base)

        matched_table = None
        for candidate in candidates:
            if candidate in all_tables and candidate != table_name:
                matched_table = candidate
                break

        # 3. –ï—Å–ª–∏ –ø—Ä—è–º–æ–µ –Ω–µ –Ω–∞—à–ª–æ ‚Äî –ø—Ä–æ–±—É–µ–º —Å –¥–æ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏
        if not matched_table:
            # –ü–æ–ª—É—á–∞–µ–º ¬´–¥–æ–º–µ–Ω¬ª –∏–∑ –∏–º–µ–Ω–∏ —Ç–µ–∫—É—â–µ–π —Ç–∞–±–ª–∏—Ü—ã (issue_comments ‚Üí issue)
            # –∏ –ø—Ä–æ–±—É–µ–º prefix_base (issue_status, issue_priority, etc.)
            prefixes_to_try = set()
            # –ò–∑ –∏–º–µ–Ω–∏ —Ç–∞–±–ª–∏—Ü—ã: issues ‚Üí issue, issue_comments ‚Üí issue
            parts = table_name.split("_")
            if parts:
                singular = parts[0].rstrip("s")
                prefixes_to_try.add(singular)       # "issue"
                prefixes_to_try.add(parts[0])       # "issues"

            # –¢–∞–∫–∂–µ –æ–±—â–∏–µ –¥–æ–º–µ–Ω–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
            prefixes_to_try.update(["issue", "project", "workflow", "notification", "permission"])

            for prefix in prefixes_to_try:
                prefix_candidates = [
                    f"{prefix}_{base}s",       # issue_statuses
                    f"{prefix}_{base}es",      # issue_statuses
                    f"{prefix}_{base}",        # issue_type
                ]
                if base.endswith("y"):
                    prefix_candidates.append(f"{prefix}_{base[:-1]}ies")  # issue_priorities
                
                for candidate in prefix_candidates:
                    if candidate in all_tables and candidate != table_name:
                        matched_table = candidate
                        break
                if matched_table:
                    break

        # 4. –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äî –ø–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö base –≤ –∏–º–µ–Ω–∏
        if not matched_table:
            for t in all_tables:
                if t != table_name and base in t and t.endswith("s"):
                    matched_table = t
                    break

        if matched_table:
            implicit.append({
                "column": col_name,
                "foreign_table": matched_table,
                "foreign_column": "id",
                "source": "implicit"
            })

    return implicit


def build_all_relationships(table_name, columns, all_tables, explicit_fks):
    """
    –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —è–≤–Ω—ã–µ FK –∏ –Ω–µ—è–≤–Ω—ã–µ —Å–≤—è–∑–∏.
    –ü—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–∫–∞—Ö –Ω–∞ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–ª–∏–∞—Å—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ join-–∑–∞–ø–∏—Å–µ–π:
      [{"column": "assignee_id", "foreign_table": "users", "alias": "users_assignee",
        "foreign_column": "id", "relationship": "many_to_one"}]
    """
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏
    all_rels = []
    for fk in explicit_fks:
        all_rels.append({**fk, "source": "explicit"})

    implicit = detect_implicit_relationships(table_name, columns, all_tables, explicit_fks)
    all_rels.extend(implicit)

    if not all_rels:
        return []

    # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –∫–∞–∂–¥–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç –∫–∞–∫ —Ü–µ–ª—å
    target_count = {}
    for rel in all_rels:
        t = rel["foreign_table"]
        target_count[t] = target_count.get(t, 0) + 1

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å –∞–ª–∏–∞—Å–∞–º–∏
    joins = []
    for rel in all_rels:
        target = rel["foreign_table"]
        col = rel["column"]
        base = col[:-3] if col.endswith("_id") else col  # assignee_id ‚Üí assignee

        # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è >1 —Ä–∞–∑ ‚Äî –∞–ª–∏–∞—Å –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω
        if target_count[target] > 1:
            alias = f"{target}_{base}"  # users_assignee, users_reporter
        else:
            alias = target

        joins.append({
            "column": col,
            "foreign_table": target,
            "alias": alias,
            "foreign_column": rel.get("foreign_column", "id"),
            "relationship": "many_to_one",
            "source": rel.get("source", "explicit")
        })

    return joins


def _parse_json_safe(text):
    """–†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
    import json as _json
    import re

    text = text.strip()
    # –£–±–∏—Ä–∞–µ–º markdown
    if text.startswith("```"):
        lines = text.split("\n")
        text = "\n".join(lines[1:])
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()
    # –¢–∏–ø–æ–≥—Ä–∞—Ñ—Å–∫–∏–µ –∫–∞–≤—ã—á–∫–∏
    for old, new in [('\u201c', '"'), ('\u201d', '"'), ('\u00ab', '"'), ('\u00bb', '"'),
                     ('\u2018', "'"), ('\u2019', "'")]:
        text = text.replace(old, new)
    # –ò—â–µ–º JSON-–±–ª–æ–∫
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        cleaned = match.group()
        cleaned = re.sub(r',\s*}', '}', cleaned)
        cleaned = re.sub(r',\s*]', ']', cleaned)
        return _json.loads(cleaned)
    return _json.loads(text)


def suggest_joins_via_llm(llm, table_name, columns, detected_joins, all_tables):
    """
    –ü–æ–ø—Ä–æ—Å–∏—Ç—å GigaChat –¥–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –¥–∂–æ–π–Ω–æ–≤
    –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤—è–∑–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
    """
    if not detected_joins and not columns:
        return {}

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π
    join_lines = []
    for j in detected_joins:
        join_lines.append(f"{j['column']} ‚Üí {j['foreign_table']} (–∞–ª–∏–∞—Å: {j['alias']})")
    detected_text = "–°–≤—è–∑–∏:\n" + "\n".join(f"  - {l}" for l in join_lines)

    # –ö–æ–ª–æ–Ω–∫–∏ —Å _id –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–ª–∏ –≤ detected
    detected_cols = {j["column"] for j in detected_joins}
    unmatched_id_cols = [
        c["name"] for c in columns
        if c["name"].endswith("_id") and c["name"] != "id" and c["name"] not in detected_cols
    ]

    unmatched_text = ""
    if unmatched_id_cols:
        unmatched_text = f"\n–ù–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(unmatched_id_cols)}"

    prompt = f"""–î–ª—è –∫–∞–∂–¥–æ–π —Å–≤—è–∑–∏ —Ç–∞–±–ª–∏—Ü—ã {table_name} –¥–∞–π title (1-2 —Å–ª–æ–≤–∞) –∏ description (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ) –Ω–∞ —Ä—É—Å—Å–∫–æ–º.

{detected_text}{unmatched_text}

–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{"joins": [{{"column": "col_id", "title": "–ù–∞–∑–≤–∞–Ω–∏–µ", "description": "–û–ø–∏—Å–∞–Ω–∏–µ"}}], "extra_joins": []}}"""

    try:
        response = llm.invoke(prompt)
        return _parse_json_safe(response.content)
    except Exception as e:
        print(f"  ‚ö†Ô∏è GigaChat –Ω–µ —Å–º–æ–≥ –æ–ø–∏—Å–∞—Ç—å —Å–≤—è–∑–∏ {table_name}: {e}")
        return {}


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ GigaChat
# ============================================================

def generate_descriptions(llm, table_name, columns, fks, sample_columns, sample_rows, row_count):
    """
    –ü–æ–ø—Ä–æ—Å–∏—Ç—å GigaChat —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã –∏ –∫–æ–ª–æ–Ω–æ–∫.
    –ü—Ä–∏ –Ω–µ—É–¥–∞—á–µ —Å –±–æ–ª—å—à–æ–π —Ç–∞–±–ª–∏—Ü–µ–π ‚Äî —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ 2 –∑–∞–ø—Ä–æ—Å–∞.
    """
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö (–Ω–µ –±–æ–ª–µ–µ 3 —Å—Ç—Ä–æ–∫, —É–∫–æ—Ä–æ—á–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
    sample_text = ""
    if sample_rows:
        sample_text = "\n–ü—Ä–∏–º–µ—Ä—ã:\n"
        for row in sample_rows[:2]:
            parts = []
            for col, val in zip(sample_columns, row):
                val_str = str(val)[:50] if val is not None else "NULL"
                parts.append(f"{col}={val_str}")
            sample_text += "  " + ", ".join(parts[:8]) + "\n"

    columns_text = "\n".join(
        f"  - {c['name']} ({c['data_type']})"
        for c in columns
    )

    prompt = f"""–û–ø–∏—à–∏ —Ç–∞–±–ª–∏—Ü—É {table_name} ({row_count} —Å—Ç—Ä–æ–∫) –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –ö–æ–ª–æ–Ω–∫–∏:
{columns_text}
{sample_text}
–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ JSON:
{{"table_title": "–†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (2-3 —Å–ª–æ–≤–∞)", "table_description": "–û–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)", "columns": {{"col_name": {{"title": "–ù–∞–∑–≤–∞–Ω–∏–µ", "description": "–ß—Ç–æ —Ö—Ä–∞–Ω–∏—Ç"}}}}}}"""

    # –ü–æ–ø—ã—Ç–∫–∞ 1
    try:
        response = llm.invoke(prompt)
        return _parse_json_safe(response.content)
    except Exception:
        pass

    # –ü–æ–ø—ã—Ç–∫–∞ 2: —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü)
    short_cols = ", ".join(c["name"] for c in columns)
    prompt2 = f"""–¢–∞–±–ª–∏—Ü–∞ {table_name}. –ö–æ–ª–æ–Ω–∫–∏: {short_cols}.
–î–∞–π table_title (2 —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º) –∏ table_description (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ).
–î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏ –¥–∞–π title (1-2 —Å–ª–æ–≤–∞) –∏ description (–∫—Ä–∞—Ç–∫–æ).
–û—Ç–≤–µ—Ç: JSON {{"table_title":"...", "table_description":"...", "columns":{{"col":{{"title":"...", "description":"..."}}}}}}"""

    try:
        response = llm.invoke(prompt2)
        return _parse_json_safe(response.content)
    except Exception as e:
        print(f"  ‚ö†Ô∏è GigaChat –Ω–µ —Å–º–æ–≥ –æ–ø–∏—Å–∞—Ç—å {table_name}: {e}")

    # Fallback ‚Äî –±–∞–∑–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
    result = {
        "table_title": table_name.replace("_", " ").title(),
        "table_description": f"–¢–∞–±–ª–∏—Ü–∞ {table_name}",
        "columns": {}
    }
    for c in columns:
        result["columns"][c["name"]] = {
            "title": c["name"].replace("_", " ").replace("id", "").strip().title() or c["name"],
            "description": f"{c['name']} ({c['data_type']})"
        }
    return result


# ============================================================
# –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ PostgreSQL ‚Üí Cube.js
# ============================================================

def pg_type_to_cube(pg_type, column_name):
    """–°–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–∏–ø PostgreSQL —Å —Ç–∏–ø–æ–º Cube.js"""
    pg_type = pg_type.lower()
    
    # –í—Ä–µ–º—è
    if any(t in pg_type for t in ["timestamp", "date", "time"]):
        return "time"
    
    # –ß–∏—Å–ª–∞
    if any(t in pg_type for t in ["integer", "int", "bigint", "smallint", "serial",
                                    "numeric", "decimal", "real", "double", "float"]):
        return "number"
    
    # –ë—É–ª–µ–≤—ã
    if "boolean" in pg_type:
        return "boolean"
    
    # –°—Ç—Ä–æ–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    return "string"


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Cube YAML
# ============================================================

def generate_cube_yaml(table_name, columns, enriched_joins, pk, descriptions, schema="public"):
    """
    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å YAML-–º–æ–¥–µ–ª—å Cube –¥–ª—è –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã.
    enriched_joins ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç build_all_relationships + –æ–ø–∏—Å–∞–Ω–∏—è –æ—Ç LLM.
    """
    
    desc = descriptions
    cube_name = table_name
    
    cube = {
        "name": cube_name,
        "sql_table": f"{schema}.{table_name}",
        "title": desc.get("table_title", table_name),
        "description": desc.get("table_description", ""),
    }
    
    # --- Joins (–æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–µ: FK + implicit + LLM) ---
    joins = []
    join_columns = set()  # –∫–æ–ª–æ–Ω–∫–∏, –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ –≤ join
    
    for j in enriched_joins:
        alias = j["alias"]
        col = j["column"]
        foreign_col = j.get("foreign_column", "id")
        join_columns.add(col)
        
        join_entry = {
            "name": alias,
            "sql": "{CUBE}." + col + " = {" + alias + "}." + foreign_col,
            "relationship": j.get("relationship", "many_to_one"),
        }
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if j.get("title"):
            join_entry["title"] = j["title"]
        if j.get("description"):
            join_entry["description"] = j["description"]
        
        joins.append(join_entry)
    
    if joins:
        cube["joins"] = joins
    
    # --- Dimensions ---
    dimensions = []
    col_descs = desc.get("columns", {})
    
    for c in columns:
        col_name = c["name"]
        cube_type = pg_type_to_cube(c["data_type"], col_name)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º FK-–∫–æ–ª–æ–Ω–∫–∏ (–æ–Ω–∏ —É—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ join)
        if col_name in join_columns and col_name != pk:
            continue
        
        dim = {
            "name": col_name,
            "sql": col_name,
            "type": cube_type,
        }
        
        if col_name == pk:
            dim["primary_key"] = True
        
        col_desc = col_descs.get(col_name, {})
        if col_desc.get("title"):
            dim["title"] = col_desc["title"]
        if col_desc.get("description"):
            dim["description"] = col_desc["description"]
        
        dimensions.append(dim)
    
    cube["dimensions"] = dimensions
    
    # --- Measures ---
    measures = [
        {
            "name": "count",
            "type": "count",
            "title": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            "description": f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ {desc.get('table_title', table_name)}"
        }
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º sum/avg –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–Ω–µ ID –∏ –Ω–µ FK)
    for c in columns:
        col_name = c["name"]
        if col_name.endswith("_id") or col_name == pk:
            continue
        if pg_type_to_cube(c["data_type"], col_name) == "number":
            col_title = col_descs.get(col_name, {}).get("title", col_name)
            measures.append({
                "name": f"total_{col_name}",
                "sql": col_name,
                "type": "sum",
                "title": f"–°—É–º–º–∞ {col_title}",
                "description": f"–°—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ª—è {col_name}"
            })
            measures.append({
                "name": f"avg_{col_name}",
                "sql": col_name,
                "type": "avg",
                "title": f"–°—Ä–µ–¥–Ω–µ–µ {col_title}",
                "description": f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è {col_name}"
            })
    
    cube["measures"] = measures
    
    return {"cubes": [cube]}


# ============================================================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è semantic-–∫–æ–Ω—Ñ–∏–≥–æ–≤ (glossary, examples)
# ============================================================

def generate_glossary(all_tables_info):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–π glossary.yml"""
    glossary = {}
    
    for info in all_tables_info:
        table = info["table_name"]
        desc = info["descriptions"]
        cube_name = table
        
        # –¢–µ—Ä–º–∏–Ω –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        title = desc.get("table_title", table)
        glossary[table] = {
            "aliases": [title.lower(), table, table.replace("_", " ")],
            "semantic_type": "entity",
            "fields": [f"{cube_name}.id"],
            "filter_operator": "equals",
            "description": desc.get("table_description", "")
        }
        
        # –¢–µ—Ä–º–∏–Ω count –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        glossary[f"{table}_count"] = {
            "aliases": [
                f"–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ {title.lower()}",
                f"—Å–∫–æ–ª—å–∫–æ {title.lower()}",
            ],
            "semantic_type": "metric",
            "measures": [f"{cube_name}.count"],
            "description": f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π {title}"
        }
    
    return glossary


def generate_examples(all_tables_info):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–π examples.yml"""
    examples = []
    
    for info in all_tables_info:
        table = info["table_name"]
        desc = info["descriptions"]
        cube_name = table
        title = desc.get("table_title", table)
        
        # –ü—Ä–∏–º–µ—Ä: "—Å–∫–æ–ª—å–∫–æ <—Å—É—â–Ω–æ—Å—Ç–µ–π>"
        examples.append({
            "question": f"—Å–∫–æ–ª—å–∫–æ {title.lower()}",
            "intent": "analytics",
            "query": {
                "measures": [f"{cube_name}.count"],
                "limit": 100
            },
            "tags": ["count", table]
        })
        
        # –ü—Ä–∏–º–µ—Ä: "—Å–ø–∏—Å–æ–∫ <—Å—É—â–Ω–æ—Å—Ç–µ–π>"
        dims = []
        for c in info["columns"][:5]:
            if c["name"] != "id" and not c["name"].endswith("_id"):
                dims.append(f"{cube_name}.{c['name']}")
        
        if dims:
            examples.append({
                "question": f"—Å–ø–∏—Å–æ–∫ {title.lower()}",
                "intent": "analytics",
                "query": {
                    "measures": [f"{cube_name}.count"],
                    "dimensions": dims[:4],
                    "limit": 100
                },
                "tags": ["list", table]
            })
    
    return examples


# ============================================================
# MAIN
# ============================================================

def main():
    # –†–∞–∑–±–æ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = argparse.ArgumentParser(description="–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –≤ Cube")
    parser.add_argument("--source", choices=["postgresql", "greenplum", "duckdb", "cube"],
                        help="–ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å database.driver –∏–∑ config.yml")
    args = parser.parse_args()

    print("=" * 60)
    print("  –ó–ê–ì–†–£–ó–ß–ò–ö –î–ê–ù–ù–´–• –í CUBE")
    print("  –ò—Å—Ç–æ—á–Ω–∏–∫ ‚Üí –û–ø–∏—Å–∞–Ω–∏—è GigaChat ‚Üí YAML-–º–æ–¥–µ–ª–∏ Cube")
    print("=" * 60)
    print()
    
    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥
    config = load_config()
    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # 2. –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É –¥–∞–Ω–Ω—ã—Ö
    source, driver_name = create_data_source(config, args.source)
    schema = config.get("database", {}).get("schema", "public")
    print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {driver_name}, –°—Ö–µ–º–∞: {schema}")
    
    # 3. –ü–æ–¥–∫–ª—é—á–∏—Ç—å GigaChat
    print("üîÑ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat...")
    llm = create_gigachat(config)
    print("‚úÖ GigaChat –≥–æ—Ç–æ–≤")
    
    # 4. –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü
    tables = source.get_tables()
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
    for t in tables:
        print(f"   - {t}")
    print()
    
    # 5. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—É—é —Ç–∞–±–ª–∏—Ü—É
    model_path = Path(config["cube"]["model_path"])
    model_path.mkdir(parents=True, exist_ok=True)
    
    all_tables_set = set(tables)
    all_tables_info = []
    
    for i, table in enumerate(tables, 1):
        print(f"[{i}/{len(tables)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: {table}")
        
        # –ß–∏—Ç–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —á–µ—Ä–µ–∑ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        columns = source.get_columns(table)
        fks = source.get_foreign_keys(table)
        pk = source.get_primary_key(table)
        row_count = source.get_row_count(table)
        sample_cols, sample_rows = source.get_sample_data(table, 5)
        
        print(f"   –ö–æ–ª–æ–Ω–æ–∫: {len(columns)}, FK: {len(fks)}, –°—Ç—Ä–æ–∫: {row_count}")
        
        # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏ (FK + implicit –ø–æ –∏–º–µ–Ω–∞–º)
        enriched_joins = build_all_relationships(table, columns, all_tables_set, fks)
        implicit_count = sum(1 for j in enriched_joins if j.get("source") == "implicit")
        if enriched_joins:
            print(f"   üîó –°–≤—è–∑–µ–π: {len(enriched_joins)} (FK: {len(fks)}, –ø–æ –∏–º–µ–Ω–∞–º: {implicit_count})")
            for j in enriched_joins:
                src = "FK" if j["source"] == "explicit" else "‚Üí"
                print(f"      {src} {j['column']} ‚Üí {j['foreign_table']} (as {j['alias']})")
        
        # –ü—Ä–æ—Å–∏–º GigaChat –æ–ø–∏—Å–∞—Ç—å —Å–≤—è–∑–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
        if enriched_joins:
            print(f"   ü§ñ GigaChat: –æ–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–µ–π...")
            join_suggestions = suggest_joins_via_llm(llm, table, columns, enriched_joins, all_tables_set)
            
            # –û–±–æ–≥–∞—â–∞–µ–º joins –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –æ—Ç LLM
            llm_joins_map = {}
            for lj in join_suggestions.get("joins", []):
                key = lj.get("column", "")
                llm_joins_map[key] = lj
            
            for j in enriched_joins:
                llm_info = llm_joins_map.get(j["column"], {})
                if llm_info.get("title"):
                    j["title"] = llm_info["title"]
                if llm_info.get("description"):
                    j["description"] = llm_info["description"]
                if llm_info.get("alias") and llm_info["alias"] != j["alias"]:
                    j["alias"] = llm_info["alias"]
            
            # –î–æ–±–∞–≤–ª—è–µ–º extra_joins –æ—Ç LLM
            for extra in join_suggestions.get("extra_joins", []):
                extra_table = extra.get("foreign_table", "")
                if extra_table in all_tables_set and extra_table != table:
                    col_names = {c["name"] for c in columns}
                    if extra.get("column") in col_names:
                        enriched_joins.append({
                            "column": extra["column"],
                            "foreign_table": extra_table,
                            "alias": extra.get("alias", extra_table),
                            "foreign_column": extra.get("foreign_column", "id"),
                            "relationship": "many_to_one",
                            "title": extra.get("title", ""),
                            "description": extra.get("description", ""),
                            "source": "llm"
                        })
                        print(f"      ‚ú® LLM –ø—Ä–µ–¥–ª–æ–∂–∏–ª: {extra['column']} ‚Üí {extra_table}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ GigaChat
        print(f"   ü§ñ GigaChat: –æ–ø–∏—Å–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –∏ –∫–æ–ª–æ–Ω–æ–∫...")
        descriptions = generate_descriptions(
            llm, table, columns, fks, sample_cols, sample_rows, row_count
        )
        print(f"   ‚úÖ –û–ø–∏—Å–∞–Ω–∏—è: {descriptions.get('table_title', '?')}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Cube YAML
        # –î–ª—è duckdb schema=main, –¥–ª—è cube ‚Äî –±–µ—Ä—ë–º –∏–∑ config
        cube_schema = schema if driver_name != "duckdb" else "main"
        cube_yaml = generate_cube_yaml(table, columns, enriched_joins, pk, descriptions, cube_schema)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        yaml_path = model_path / f"{table}.yml"
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(cube_yaml, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {yaml_path}")
        
        all_tables_info.append({
            "table_name": table,
            "columns": columns,
            "fks": fks,
            "enriched_joins": enriched_joins,
            "descriptions": descriptions
        })
        print()
    
    # 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º semantic-–∫–æ–Ω—Ñ–∏–≥–∏
    config_path = Path("config")
    config_path.mkdir(exist_ok=True)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è glossary.yml...")
    glossary = generate_glossary(all_tables_info)
    with open(config_path / "glossary.yml", 'w', encoding='utf-8') as f:
        yaml.dump(glossary, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è examples.yml...")
    examples = generate_examples(all_tables_info)
    with open(config_path / "examples.yml", 'w', encoding='utf-8') as f:
        yaml.dump(examples, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è semantic_layer.yml...")
    layer_config = {
        "cube": {
            "base_url": config["cube"]["api_url"],
            "enabled": True,
            "preferred_cubes": [t["table_name"] for t in all_tables_info]
        },
        "intents": {
            "analytics": {
                "description": "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ Cube",
                "keywords": ["—Å–∫–æ–ª—å–∫–æ", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–ø–æ–∫–∞–∂–∏", "—Å–ø–∏—Å–æ–∫",
                             "—Å—Ä–µ–¥–Ω–∏–π", "—Å—É–º–º–∞", "—Ç–æ–ø", "–≤—Å–µ–≥–æ", "–ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º"],
                "priority": 1
            }
        },
        "query_generation": {
            "default_limit": 100,
            "max_limit": 10000
        }
    }
    with open(config_path / "semantic_layer.yml", 'w', encoding='utf-8') as f:
        yaml.dump(layer_config, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    source.close()
    
    # –°–≤–æ–¥–∫–∞ –ø–æ —Å–≤—è–∑—è–º
    total_joins = sum(len(info.get("enriched_joins", [])) for info in all_tables_info)
    fk_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "explicit")
        for info in all_tables_info
    )
    implicit_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "implicit")
        for info in all_tables_info
    )
    llm_joins = sum(
        sum(1 for j in info.get("enriched_joins", []) if j.get("source") == "llm")
        for info in all_tables_info
    )
    
    print()
    print("=" * 60)
    print("  ‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 60)
    print(f"""
–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: {driver_name}
–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:
  üìÅ {model_path}/          ‚Äî YAML-–º–æ–¥–µ–ª–∏ Cube ({len(tables)} —Ñ–∞–π–ª–æ–≤)
  üìÅ config/glossary.yml    ‚Äî –ë–∏–∑–Ω–µ—Å-–≥–ª–æ—Å—Å–∞—Ä–∏–π
  üìÅ config/examples.yml    ‚Äî –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
  üìÅ config/semantic_layer.yml ‚Äî –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è

–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ (joins):
  üîó –í—Å–µ–≥–æ: {total_joins}
     - –ò–∑ FK constraints: {fk_joins}
     - –ü–æ –∏–º–µ–Ω–∞–º –∫–æ–ª–æ–Ω–æ–∫: {implicit_joins}
     - –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–æ GigaChat: {llm_joins}

–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
  1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –∏–∑ {model_path}/ –≤ –ø–∞–ø–∫—É model/cubes/ –≤–∞—à–µ–≥–æ Cube-–ø—Ä–æ–µ–∫—Ç–∞
  2. –ü–†–û–í–ï–†–¨–¢–ï –°–í–Ø–ó–ò: –æ—Ç–∫—Ä–æ–π—Ç–µ YAML-—Ñ–∞–π–ª—ã –∏ —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ joins –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
  3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Cube: npx cubejs-server
  4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: curl http://localhost:4000/cubejs-api/v1/meta
  5. –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ glossary.yml –∏ examples.yml –ø–æ–¥ –≤–∞—à–∏ –∑–∞–¥–∞—á–∏
  6. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 02_build_faiss.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
  7. –û—Ç–∫—Ä–æ–π—Ç–µ 03_agent.ipynb –≤ JupyterLab –∏ –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã
""")


if __name__ == "__main__":
    main()
